---
title: "Points de référence biologiques et éléments de base pour l’établissement des objectifs de gestion pour les regroupements de saumon rouge (*Oncorhynchus nerka*) des rivières Skeena et Nass"
author: |
  Gottfried. P. Pestal^1^ and Charmaine Carr-Harris^2^
author_list: "Pestal, G.P. and C. Carr-Harris"
address: |
  ^1^SOLV Consulting Ltd.\
     Vancouver, B.C.\
  ^2^Fisheries and Oceans Canada\
     Prince Rupert, B.C.\
month: "Month"
year: 2024
report_number: nnn
region: "Pacific Region"
isbn: "Insert ISBN"
cat_no: "Insert Cat No"
citation_other_language: "Pestal, G.P. and C. Carr-Harris 2024. Biological Benchmarks and Building Blocks for Aggregatelevel
Management Targets for Skeena and Nass Sockeye Salmon (Oncorhynchus nerka).
DFO Can. Sci. Advis. Sec. Res. Doc. 2024/nnn. v + 307 p."
abstract: |
    Dans le cadre des dispositions du Traité sur le saumon du Pacifique (TSP) renouvelé, le Canada a accepté de réaliser une analyse approfondie des objectifs d’échappée pour les saumons rouges (*Oncorhynchus nerka*) remontant les rivières Skeena et Nass au sein de 31 stocks dont le cycle biologique et la productivité observée varient. Nous avons mis à l’essai d’autres ajustements du modèle géniteurs-recrues, élaboré des lignes directrices pour la sélection de scénarios de productivité de rechange fondés sur les ajustements du modèle et calculé des points de référence biologiques pour les scénarios retenus. Nous avons aussi comparé d’autres approches pour combiner les estimations des points de référence biologiques au niveau du stock en points de référence au niveau du regroupement. Une grande proportion des saumons rouges qui remontent la rivière Skeena vient du projet de mise en valeur à faible intensité dans le lac Babine qui consiste en une série de frayères artificielles et de tronçons gérés sur deux affluents du lac Babine (le ruisseau Pinkut et la rivière Fulton). Dans le cadre de cet examen, nous avons résumé les tendances relevées dans les données de production du projet de mise en valeur dans le lac Babine et constaté que, bien que les densités de charge des systèmes concernés soient demeurées relativement constantes au fil du temps, la productivité globale des saumons rouges de la rivière Skeena issus de la mise en valeur a diminué dans les 20 dernières années.

header: "" # or "" to omit
output:
 csasdown::resdoc_pdf:
   french: true
   copy_sty: true
   line_nums: false
   line_nums_mod: 1
   lot_lof: false
# ------------
# End of options to set
knit: bookdown::render_book
link-citations: true
bibliography: bib/refs.bib
# Any extra LaTeX code for the header:
header-includes:
# - \usepackage{tikz}
 # rest is from page 4 of https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf
 - \usepackage{booktabs} 
 - \usepackage{longtable}
 - \usepackage{array}
 - \usepackage{multirow}
 - \usepackage{wrapfig}
 - \usepackage{float}
 - \usepackage{colortbl}
 - \usepackage{pdflscape}
 - \usepackage{tabu}
 - \usepackage{threeparttable}
 - \usepackage{threeparttablex}
 - \usepackage[normalem]{ulem}
 - \usepackage{makecell}
 - \usepackage{xcolor}
 - \newcommand{\Smax}{$S_\textrm{max}$} 
 - \newcommand{\Smsy}{$S_\textrm{MSY}$}
 - \newcommand{\Umsy}{$U_\textrm{MSY}$}

---

```{r setup, echo=FALSE, cache=FALSE, message=FALSE, results='hide', warning=FALSE}
library(knitr)
if (is_latex_output()) {
  knitr_figs_dir <- "knitr-figs-pdf/"
  knitr_cache_dir <- "knitr-cache-pdf/"
  fig_out_type <- "png"
} else {
  knitr_figs_dir <- "knitr-figs-docx/"
  knitr_cache_dir <- "knitr-cache-docx/"
  fig_out_type <- "png"
}
fig_asp <- 0.618
fig_width <- 9
fig_out_width <- "6in"
fig_dpi <- 180
fig_align <- "center"
fig_pos <- "htb"
opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  fig.path = knitr_figs_dir,
  cache.path = knitr_cache_dir,
  fig.asp = fig_asp,
  fig.width = fig_width,
  out.width = fig_out_width,
  echo = FALSE,
  #  autodep = TRUE,
  #  cache = TRUE,
  cache.comments = FALSE,
  dev = fig_out_type,
  dpi = fig_dpi,
  fig.align = fig_align,
  fig.pos = fig_pos
)
options(xtable.comment = FALSE)
options(kableExtra.latex.load_packages = FALSE)
```

```{r load-libraries, cache=FALSE}
# `french` and `prepub` variables are extracted from the YAML headers above and
#  are used throughout the document. To make the document all in french, change
#  the line in the YAML header above to `french: true`
meta <- rmarkdown::metadata$output
if (length(grep("pdf", names(meta)))) {
  french <- meta$`csasdown::resdoc_pdf`$french
  prepub <- meta$`csasdown::resdoc_pdf`$prepub
} else if (length(grep("word", names(meta)))) {
  french <- meta$`csasdown::resdoc_word`$french
  prepub <- meta$`csasdown::resdoc_word`$prepub
}
csl <- "csl/csas.csl"
if (french) {
  csl <- "csl/csas-french.csl"
  options(OutDec = ",")
}

# add other packages here:
library(dplyr)
library(ggplot2)
library(readr)
library(tibble)
library(csasdown)
library(tidyverse)
library(kableExtra)
library(rosettafish)


# custom functions
source("functions/FUNCTIONS_prepTable.R")



# data used throughout

stock.info <- read.csv("data/SR Data/StockInfo_Main.csv", 
                       stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM") %>%
                dplyr::rename(ERInd = ER_Indicator) %>% 
                arrange(StkSeq) %>%
                mutate(ContrVal = round(ContrVal,1))

stock.info$NumSurvLk[stock.info$LifeHistory != "Lake"] <- NA #
stock.info$Stock <- gsub("Motase ","Motase",stock.info$Stock)

data.notes.tab <- read.csv("data/SR Data/NotesonSRDataInputs.csv", 
                       stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM")
data.notes.tab$Stock <- gsub("Motase ","Motase",data.notes.tab$Stock)

alt.sr.data <-  read.csv("data/SR Data/SR_Data_AltVersions_MERGED.csv", 
                       stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM")


# BM CALC TESTS

smsy.eq <- read_csv("data/Reference Tables/Smsy_Calc_Equations.csv")
test1.table <- read.csv("data/GeneratedReportTables/BMCalc_Test1_SampleParValues.csv",
                        stringsAsFactors = FALSE) %>% 
                        mutate(Smsy = round(Smsy,1),Sgen = round(Sgen,1))


test1.table$SgenCalc <- gsub("Connorsetal2022","Connorsetal2023", test1.table$SgenCalc)


test2.vals <- read.csv("data/GeneratedReportTables/BMCalc_Test2_GridTest_CalcVersions_PercRanges.csv", stringsAsFactors = FALSE)


test3.table <- read.csv("data/GeneratedReportTables/Test3_SpeedTest.csv", stringsAsFactors = FALSE)


test3.table$Method<- gsub("Connorsetal2022","Connorsetal2023", test3.table$Method)

# Alt SR TESTS

alt.sr.test1 <- read_csv("data/ReportTable_AltSRTest_PercChange.csv")










```





---
csl: `r csl`    
---

<!--chapter:end:index.Rmd-->

**TO DO LIST**

- Report Number
- French abstract not showing up?
- French translation of references? PSC, BLMDP, NBSRR = "RRSRLN", NCCSDB = "NCCDSB", SSIR = "RRSRRS" etc?  (even in the French text they switch back and forth between NCCSDB and NCCDSB 
- Is "enhanced" really "mise en valeur" in a fisheries context? Maybe the French for "supplemented" would be better?
- table formatting (blank lines, number formatting!!!!!)
- SOme citations are as "et al." in the French callout, but many others are like this "Cox-Rogers et ses collaborateurs (2010)" -> How to fix this in csasdown/bibtex?
- In some equations (and all/most labels/tables/text) Spn is translated as "Gèn" or "G", but in Sec 2.2.1 with the Ricker equations it and Sec 2.2.3 with the prior equations
uses "S" in text and equations. Section 2.2.5 uses Srmd instead of Smsy or Grmd. I carried over the French callout text as-is in each section.
- Model labels: BR, AR1,TVP -> need to make sure consistently handled in text, figs,tables
- Sec 2.3 title "model selection and productivity scenarios" translated as "scenarios of model selection and scenarios of productivity"???? -> used what they provided
- **TABLE 8** - special symbols crashing the compile 
- "State vs. Status" and "statut ou l'état" It seems that in both languages there are complicted nuances around these terms. The French version of the WSP uses "POINTS DE RÉFÉRENCE DE L’ÉTAT" for status benchmarks, and so the res doc translation is right. It just surprised me. It seems like in French they use both depending om the kind of status you're talking about. See examples at https://www.linguee.fr/francais-anglais/traduction/statut+ou+%C3%A9tat.html
- **Table 12** "Étiquette" column label is crashing the compile, Even without it, some cell content also crashing the compile.


## REMERCIEMENTS {-}

Le travail présenté dans ce document s’inscrit dans une initiative plus vaste qui n’aurait pas été possible sans les contributions et les conseils constructifs d’un groupe diversifié. Les détenteurs de droits, les intervenants et les experts techniques qui ont participé à l’atelier de détermination de la portée à Prince Rupert en 2019 ont tracé la voie initiale. Un groupe de travail technique (annexe \@ref(app:TWG)) a coordonné les examens des données et le travail analytique. Randall Peterman (Ph. D.) et Milo Adkison (Ph. D.) ont agi à titre d’examinateurs indépendants pour l’ensemble de l’initiative et nous ont guidés dans les périodes difficiles. Murdoch McAllister (Ph. D.) et Wendell Challenger (Ph. D.) ont fourni un ensemble distinct d’analyses des géniteurs-recrues pour permettre de comparer les méthodes. Nous remercions tous les contributeurs qui nous ont aidés à nous rendre jusqu’ici, et nous avons hâte de poursuivre ce parcours avec eux.

## ACRONYMES ET SIGLES {-}


(ref:TableAcronyms) Formes longue et abrégée des termes techniques utilisés tout au long du document. 

```{r TableAcronyms, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Reference Tables/Acronyms.csv")# %>% select

table.in[,1:2]  %>%     
   #mutate_all(function(x){x = as.character(x)}) %>%
   #mutate_all(function(x){gsub("-", "", x)}) %>% 
   #mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
     #mutate_all(function(x){gsub("@", "\\\\@", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = TRUE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableAcronyms)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "10em") %>%
     kableExtra::column_spec(2, width = "24em") #%>%
   #  kableExtra::column_spec(3, width = "12em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage
# INTRODUCTION

## EXAMEN DES OBJECTIFS D’ÉCHAPPÉE {#Project}

### Renseignements de base {#Background}

Les populations de saumon rouge changent rapidement en raison des effets cumulatifs des agents de stress anthropiques, notamment la pression de la pêche, la dégradation de l’habitat et les changements climatiques. Pour les saumons rouges qui remontent les rivières Skeena et Nass (ci-après appelés « saumons rouges des rivières Skeena et Nass »), dont les remontes sont respectivement les deuxième et troisième plus grandes en Colombie-Britannique, on constate une baisse de la productivité, une variabilité croissante de la taille des remontes et une augmentation de la fréquence des remontes faibles depuis 2000. Ces dernières années, les faibles remontes des saumons rouges des rivières Skeena et Nass ont entraîné des réductions ou des fermetures des pêches commerciales canadiennes, et des restrictions des pêches autochtones ciblant le saumon rouge de la rivière Skeena ont été imposées certaines années. Les quatre remontes les plus faibles de saumon rouge de la rivière Nass ont été enregistrées de 2017 à 2021. Pour le saumon rouge de la rivière Skeena, les échappées les plus faibles depuis le glissement de terrain catastrophique dans la rivière Babine s’étant produit dans les années 1950 ont eu lieu en 2013, 2017 et 2019.


En vertu des dispositions du Traité sur le saumon du Pacifique (TSP) renouvelé, le Canada a accepté de réaliser une analyse approfondie des objectifs d’échappée pour les saumons rouges (Oncorhynchus nerka) remontant les rivières Skeena et Nass [@PST]. Les objectifs d’échappée pour les regroupements de saumon rouge des rivières Skeena et Nass servent à fixer la récolte annuelle autorisée (RAA) dans les pêches américaines et canadiennes ciblant ces deux regroupements de stocks. En plus des dispositions du TSP renouvelé, les objectifs d’échappée fondés sur des données biologiques pour les saumons rouges des rivières Skeena et Nass sont utilisés pour la gestion des pêches canadiennes, y compris la mise en œuvre du traité avec les Nisga’a [@NisgaaFinalAg], des pêches des Premières Nations à des fins alimentaires, sociales et rituelles (ASR), des débouchés économiques et des pêches récréatives dans les rivières Skeena et Nass.

Les remontes des regroupements de saumon rouge dans les bassins versants des rivières Skeena et Nass sont composées de nombreux stocks distincts, dont certains sont appauvris et considérés comme des stocks préoccupants, tandis que d’autres ont une faible abondance de géniteurs et peu ou pas d’estimations fiables [@SkeenaNassSkDataRep]. Les saumons rouges issus de deux affluents mis en valeur du lac Babine constituent une grande partie de la production du regroupement de saumon rouge de la rivière Skeena. Bien que les différents stocks sauvages, dont la taille des remontes varie entre des centaines et des dizaines de milliers de poissons, soient beaucoup moins abondants que les stocks du lac Babine issus de la mise en valeur, ils représentent la majeure partie de la diversité génétique des saumons rouges des rivières Skeena et Nass. Conformément à la Politique concernant le saumon sauvage [PSS, @WSP], le Canada s’efforce de maintenir la productivité future des saumons rouges remontant les rivières Skeena et Nass en préservant les populations sauvages génétiquement uniques qui contribuent aux remontes des regroupements. Depuis au moins les années 1960, avant le début des remontes de poissons issus du projet de mise en valeur dans le lac Babine [PMVLB; p. ex.  @Larkin1968SkeenaPopBio], on s’inquiète de la possibilité d’une surexploitation des stocks plus petits et moins productifs des rivières Skeena et Nass dans les pêches ciblant les grands stocks mixtes des lacs. Plus récemment, un examen indépendant des saumons de la rivière Skeena a souligné la nécessité d’envisager des compromis entre les mesures de gestion des pêches et les exigences de la PSS afin de conserver les stocks plus faibles déjà appauvris [@Waltersetal2008ISRP].

Les objectifs d’échappée actuels, qui sont fondés sur des estimations antérieures de l’abondance des géniteurs pour produire le rendement maximal durable (Srmd), sont de 900 000 saumons rouges pour le regroupement de la rivière Skeena et de 200 000 saumons rouges pour le regroupement de la rivière Nass [@ShepardWithlerSkeenaBM; @RickerSmithSkeenaBM; @BockingetalMeziadinBM; @CoxRogers2013SkeenaMemo]. Ces objectifs ne tiennent pas compte de la structure complexe des stocks formant chaque regroupement. De plus, la productivité des regroupements de saumon rouge des rivières Skeena et Nass a considérablement diminué ces dernières années et la composition des stocks dans et entre les deux regroupements a changé, surtout après la mise en place des installations de mise en valeur dans le lac Babine dans les années 1970 et une réduction plus récente de la proportion de saumons rouges remontant jusqu’au lac Meziadin, qui est le plus grand réseau pour les saumons rouges dans le bassin versant de la rivière Nass. Les objectifs d’échappée au niveau du regroupement fondés sur le rendement maximal durable (RMD), qui supposent une productivité moyenne à long terme et une composition stable des stocks, ne tiennent pas compte de ces changements et pourraient ne pas refléter les conditions actuelles ou futures des saumons rouges des rivières Skeena et Nass.

L’objectif d’échappée actuel pour le saumon rouge de la rivière Skeena ne tient pas compte de la contribution des poissons issus du PMVLB, qui ont représenté en moyenne 67 % des remontes de 1970 à 2020 avec une grande variabilité entre les années (fourchette de 33 à 83 %). Étant donné que le ratio moyen entre les saumons rouges issus de la mise en valeur et les saumons rouges sauvages de la rivière Skeena est d’environ 2 pour 1, l’objectif d’échappée provisoire présumé pour la composante sauvage du regroupement de saumon rouge de la rivière Skeena est d’environ 300 000 poissons, soit le tiers de l’objectif actuel d’échappée de 900 000 poissons.

Les saumons rouges des rivières Skeena et Nass, qui proviennent des eaux canadiennes, sont principalement récoltés dans les pêches commerciales du Canada et de l’Alaska, dans les pêches autochtones pratiquées dans les zones d’approche en milieu marin et dans l’ensemble des bassins versants de ces deux rivières. Un petit nombre de saumons rouges des rivières Skeena et Nass sont capturés dans les pêches récréatives pratiquées dans chaque rivière. De 1985 à 2021, le taux d’exploitation total moyen était de 54 % pour le saumon rouge de la rivière Skeena et de 64,6 % pour celui de la rivière Nass [@NBTC2020]. Les taux d’exploitation totaux des saumons rouges des rivières Skeena et Nass ont diminué au cours de cette période et la proportion de poissons provenant de ces rivières dans les prises totales variait selon le secteur.

Les objectifs d’échappée pour les regroupements de saumon rouge des rivières Skeena et Nass sont nécessaires pour mettre en œuvre les dispositions du chapitre 2 du TSP. Ce chapitre permet aux États-Unis de pêcher 2,45 % de la RAA de saumon de la rivière Skeena et de la rivière Nass avant la semaine statistique 31 des États-Unis pour la pêche à la senne coulissante dans le district 104, et 13,8 % pour la pêche au filet maillant dans le district 101. La RAA est calculée chaque année comme la remonte totale combinée au-dessus des objectifs d’échappée combinés pour les saumons rouges des rivières Skeena et Nass, à moins que la remonte soit inférieure à l’objectif d’échappée combiné de 1,1 million de poissons, auquel cas la RAA est définie comme la remonte totale moins l’échappée réelle.



### Processus d’examen des objectifs d’échappée {#EGProcess}

L’examen des objectifs d’échappée est guidé par le cadre de référence accepté bilatéralement qui précise les objectifs suivants :

1.	résumer et évaluer les renseignements biologiques pertinents pour guider l’élaboration d’objectifs d’échappée pour les regroupements de saumon rouge des rivières Skeena et Nass, y compris une évaluation des principales incertitudes et lacunes dans les données sur les populations de saumon rouge dans les bassins de ces rivières;
2.	évaluer d’autres objectifs d’échappée pour les regroupements de saumon rouge des rivières Skeena et Nass, en se penchant notamment sur l’état des stocks, la production et les répercussions des principales incertitudes.

Les membres canadiens du comité technique de la limite nord de la Commission du saumon du Pacifique ont été chargés de diriger un examen technique des données, des méthodes et des paramètres qui peuvent être utilisés pour élaborer des objectifs d’échappée fondés sur les données biologiques pour les saumons rouges des rivières Skeena et Nass. Un groupe de travail technique (GTT) a été créé pour mener ce travail. Il comptait des participants de Pêches et Océans Canada (MPO), des Premières Nations du nord de la côte, de la Fondation du saumon du Pacifique et d’organisations de consultation. Deux examinateurs indépendants, un par pays, ont également été désignés pour guider le travail technique et examiner l’avis scientifique en découlant. Le travail technique fait partie du processus plus vaste d’examen des objectifs d’échappée, et ce document est l’une des étapes de ce travail technique. L’annexe \@ref(app:TWG) dresse la liste des membres du GTT et des examinateurs indépendants.

Le GTT et les examinateurs indépendants ont franchi quatre étapes clés qui ont débouché sur le présent document de recherche.

* *Atelier de détermination de la portée* : Nous avons tenu un atelier avec les détenteurs de droits et les intervenants canadiens à l’automne 2019 pour donner un aperçu des travaux techniques prévus, solliciter des commentaires sur les priorités analytiques et compiler des suggestions pour le processus global d’examen des objectifs d’échappée.
* *Examen des données – Partie 1* : Structure des stocks : Nous avons regroupé l’information sur la structure des populations des regroupements de saumon rouge des rivières Skeena et Nass, établi une liste convenue de 31 stocks pour les analyses subséquentes et documenté la façon dont les stocks correspondent aux unités de conservation (UC) désignées en vertu de la PSS [@WSP]. L’une des principales constatations de l’examen de la structure des stocks était que certaines UC actuellement désignées étaient probablement erronées et nécessitaient un examen plus approfondi, mais cela ne fait pas partie de la portée du processus actuel.
* *Examen des données – Partie 2* : Données sur les géniteurs-recrues : Nous avons compilé, examiné et mis à jour les données accessibles afin de produire des ensembles de données convenus sur les géniteurs-recrues pour les stocks et les regroupements [@SkeenaNassSkDataRep]. Les 31 stocks ont été répartis en trois groupes en fonction de l’abondance relative et des données accessibles pour les analyses subséquentes. En nous fondant sur l’examen des données et des tests de sensibilité approfondis, nous avons déterminé les priorités pour l’analyse. Les principaux problèmes à étudier sont indiqués ci-dessous.

   1.	*Variation de la productivité* : La productivité des saumons rouges des rivières Skeena et Nass a diminué considérablement ces dernières années en raison de la variabilité croissante des remontes totales et de la productivité pour les regroupements et pour de nombreux stocks qui les composent. Un objectif d’échappée efficace doit tenir compte du fait que ces changements persisteront probablement à l’avenir. Le GTT et les deux examinateurs indépendants ont indiqué que la productivité variable dans le temps était l’un des facteurs les plus importants à prendre en compte dans le plan analytique.
   2.	*Points de référence de gestion au niveau du regroupement et au niveau du stock* : Les regroupements de saumon rouge des rivières Skeena et Nass sont tous deux composés de nombreux stocks plus petits présentant des caractéristiques et une dynamique de population uniques. L’un des principaux objectifs de l’examen des objectifs d’échappée des saumons rouges des rivières Skeena et Nass est de recommander un objectif d’échappée combiné pour ces regroupements qui tient compte de la diversité génétique au niveau du stock en plus de la productivité variable.
   3.	*Stocks mis en valeur et stocks sauvages* : La composante la plus importante des remontes combinées des regroupements des rivières Skeena et Nass provient des réseaux visés par le PMVLB, où l’échappée des géniteurs est relativement constante et gérée pour maintenir une production optimale d’alevins. L’objectif d’échappée actuel pour le regroupement de la rivière Skeena (900 000 géniteurs) est fondé sur les analyses des géniteurs-recrues qui ont été effectuées dans les années 1950 et 1960 [@ShepardWithlerSkeenaBM], soit avant le PMVLB. Un examen des objectifs de gestion pour le saumon rouge de la rivière Skeena doit tenir compte de l’importante contribution et de la capacité de reproduction des stocks mis en valeur.


Le présent document de recherche fournit un avis scientifique sur ces priorités d’analyse. Les compromis et les décisions éventuelles concernant les objectifs de gestion des regroupements et les stratégies de récolte connexes, qui tiendront compte de facteurs biologiques et autres (p. ex. objectifs socio-économiques), dépendront des décisions stratégiques et des objectifs précis des Premières Nations et des groupes d’intervenants qui seront définis dans un processus de gestion ultérieur. Dans ce document de recherche, nous présentons les points de référence biologiques possibles pour les stocks de saumon rouge des rivières Skeena et Nass en fonction des meilleures données accessibles et nous comparons des approches de rechange pour l’élaboration d’objectifs d’échappée fondés sur des données biologiques au niveau des regroupements de ces stocks. La section \@ref(AnalysisOverview) explique la raison d’être de la portée et de la structure du présent document de recherche.

Ce projet est étroitement lié à d’autres initiatives en cours.

* Les nouvelles dispositions relatives aux stocks de poissons définies dans la version révisée de la Loi sur les pêches [@NewFisheriesAct] exigent l’élaboration de points de référence limites (PRL) pour les grands stocks de poissons. Des zones de gestion des stocks ont été définies pour les saumons du Pacifique. Ces groupes d’UC ont été organisés en zones de gestion des stocks (ZGS), considérées comme de grands stocks de poissons qui sont gérés comme une unité pour parvenir à un état commun. En vertu du cadre proposé pour définir les ZGS, les saumons rouges des rivières Skeena et Nass sont définis comme faisant partie de ZGS distinctes [@smuref].
* L’un des principaux piliers de la PSS consiste à déterminer les UC et à évaluer leur état à l’aide d’un ensemble normalisé d’indicateurs, qui sont combinés en un état global intégré [@WSP; @HoltbyCiruna2007; @CURev2019; @Holtetal2009BM]. L’un des principaux objectifs de la PSS est de maintenir les UC au-dessus de leur point de référence inférieur afin de les protéger contre le risque d’extinction et de conserver leur diversité adaptative. Les lignes directrices sur la mise en œuvre des nouvelles dispositions de la Loi sur les pêches sont un produit livrable clé du plan de mise en œuvre de la PSS mis à jour [@WSPImplementationAddendum]. Les lignes directrices et les études de cas ont été examinées par des pairs au début du mois de mars 2022 [@LRPGuidelinesSAR]. Les travaux visant à élaborer des PRL pour les ZGS de saumon du Pacifique (MPO 2022b)se poursuivent, et nous résumons les recommandations pour les types d’analyses et d’outils analytiques qui seraient nécessaires pour faciliter l’élaboration de PRL pour les saumons rouges des rivières Skeena et Nass, mais n’incluons pas les PRL possibles dans le présent document de recherche.
* Un processus de consultation national canadien a commencé à l’automne 2022, dans le cadre duquel des détenteurs de droits et des intervenants examinent l’information technique et commentent les objectifs d’échappée actuels pour les saumons rouges des rivières Skeena et Nass. Les exemples inclus dans ce document de recherche visent à fournir une base technique solide pour établir l’ordre de priorité des travaux futurs à l’appui du processus de mobilisation.


### Objectifs du document de recherche {#PaperObj}

Le mandat du projet établi par le comité de la limite nord de la CSP (section \@ref(EGProcess)) exige l’élaboration et l’évaluation de points de référence possibles au niveau du stock et au niveau du regroupement. Des points de référence au niveau du regroupement sont nécessaires pour mettre en œuvre les dispositions de gestion internationales en vertu du TSP renouvelé [@PST], de même que des points de référence au niveau du stock pour atteindre les objectifs de conservation fixés dans la PSS [@WSP].


Les objectifs précis du document de recherche sont les suivants :

1.	Élaborer une approche pour l’évaluation et la sélection de paramètres d’ajustement du modèle géniteurs-recrues en utilisant d’autres ensembles de données et d’autres formes de modèles – notamment des modèles variant dans le temps –, et appliquer cette approche au niveau des stocks et des regroupements pour les saumons rouges de la Skeena et de la Nass.
2.	Élaborer une approche visant à déterminer d’autres scénarios de productivité plausibles (p. ex. la productivité moyenne à long terme par rapport à la productivité actuelle), ainsi que les ensembles de paramètres géniteurs-recrues correspondants.
3.	Établir des points de référence biologiques pour les stocks au moyen d’ensembles de données actuelles et de méthodes appropriées pour les stocks de saumon rouge sauvages et mis en valeur des rivières Skeena et Nass, ce qui comprend :
   a.	l’estimation et l’évaluation des points de référence biologiques candidats (p. ex. Srmd, Smax, Sgen, Urmd) à partir des ajustements du modèle basés sur les autres scénarios plausibles de productivité pour les stocks sauvages de saumon rouge de la Skeena et de la Nass;
   b.	l’examen de la capacité des chenaux et des tendances de productivité observées pour les stocks de saumon rouge de la Skeena qui ont été mis en valeur dans le cadre du projet d’aménagement du lac Babine.
4.	Faire une comparaison avec d’autres approches pour choisir des points de référence biologiques propres aux regroupements pour les saumons rouges des rivières Skeena et Nass, et évaluer les avantages et les inconvénients de chaque approche.
5.	Établir des priorités pour les travaux futurs afin d’appuyer l’élaboration d’objectifs d’échappée propres aux stocks et de points de référence propres aux regroupements.
6.	Relever et examiner les incertitudes relatives aux points de référence propres aux stocks par la comparaison des résultats générés à l’aide d’autres formes de modèles géniteurs-recrues et d’autres ensembles de données, et faire de même pour les incertitudes relatives aux points de référence propres aux regroupements.



## STRUCTURE DES STOCKS DE SAUMON ROUGE DES RIVIÈRES SKEENA ET NASS


### Types de cycle biologique, stocks et unités de conservation

Les remontes de saumon rouge des rivières Skeena et Nass sont les deuxième et troisième en importance au Canada, après celle du saumon rouge du fleuve Fraser. Ensemble, les regroupements de saumon rouge des rivières Skeena et Nass comptent des dizaines de populations génétiquement uniques qui remontent dans différents affluents des deux bassins versants et qui sont récoltées dans des pêches commerciales à grande échelle et de nombreuses pêches autochtones protégées par la Constitution dans les deux bassins versants [@Moore2015SellingFN]. L’importance de la diversité au niveau du stock, qui est protégée par la politique canadienne sur les pêches, est un élément clé de l’examen actuel des objectifs d’échappée des saumons rouges des rivières Skeena et Nass.

L’une des principales caractéristiques des remontes de saumon rouge des rivières Skeena et Nass est qu’un seul grand lac assure la majeure partie de la production de chaque regroupement (le lac Babine pour la rivière Skeena et le lac Meziadin pour la rivière Nass). Ces grandes populations lacustres sont elles-mêmes des regroupements de nombreuses populations reproductrices plus petites et, pour les deux, la composition des stocks a changé au fil du temps. Les nombreux autres stocks plus petits représentent la plus grande partie de la diversité génétique des saumons rouges des rivières Skeena et Nass, et certains sont récoltés dans des pêches autochtones à petite échelle en rivière ou en estuaire qui soutiennent les économies locales [@GottesfeldSkeenaBook].


@BeachamWithlerSeaType décrivent trois différentes stratégies de cycle biologique observées chez les jeunes saumons rouges anadromes (migrant vers l’océan).

* *Le saumon rouge de type lacustre* qui éclot dans un lac ou un affluent de lac et qui y grandit pendant au moins un an après l’éclosion.
* *Le saumon rouge de type océanique* qui éclot dans un affluent ou un chenal latéral du cours principal et qui passe ensuite plusieurs mois à grandir dans les eaux estuariennes avant de migrer vers l’océan, ce qui représente un séjour total en eau douce de moins d’un an.
* *Le saumon rouge de type fluvial* qui éclot dans un affluent ou un chenal latéral du cours principal et qui grandit ensuite dans l’environnement fluvial pendant au moins un an avant de migrer vers l’océan.

La plupart des grands stocks de saumon rouge sur la côte du Pacifique sont de type lacustre. Les saumons rouges de type fluvial et océanique peuvent avoir un potentiel d’adaptation plus élevé, étant moins attachés à des sites précis et plus polyvalents dans leur utilisation d’habitats variables ou changeants [section 9.2 dans @HoltbyCiruna2007]. On continue d’étudier les liens évolutifs entre les populations de saumon rouge de type lacustre, océanique et fluvial [p. ex. @WoodetalLifeHist1987;  @BeachamWithlerSeaType; @WoodLifeHist1995;@WoodetalLifeHist2008; @Beachametal2004transboundary].

La plupart des saumons rouges qui proviennent des bassins versants des rivières Skeena et Nass ont un cycle biologique de type lacustre, mais des populations de type fluvial frayent dans les deux bassins. Il y a également au moins deux populations de type océanique qui frayent dans le cours inférieur de la rivière Nass, dans les ruisseaux Gingit et Gityzon [@Beveridgeetal2015GingitSea]. Leur contribution au regroupement de la rivière Nass a augmenté ces dernières années. Les saumons rouges de type océanique du cours inférieur de la rivière Nass, dont la population reproductrice la plus abondante (celle du ruisseau Gingit) fait l’objet d’un relevé régulier depuis 2000 [@Beveridgeetal2015GingitSea], représentaient environ 31 % de la remonte en 2019 [@NFWD2020].

En vertu de la PSS, les saumons anadromes canadiens ont été regroupés en UC distinctes, définies comme « [u]n groupe de saumon sauvage suffisamment isolé des autres groupes qui, s’il disparaissait, aurait peu de chances de se recoloniser de manière naturelle dans une limite de temps acceptable » [@WSP]. Pour les saumons rouges des rivières Nass et Skeena, les définitions des UC sont fondées sur les désignations préliminaires établies en 2009 qui, pour le saumon rouge de type lacustre, correspondent généralement au lac de croissance d’origine [@HoltbyCiruna2007]. La plupart des stocks identifiés dans nos analyses correspondent à une seule UC, mais pour certains stocks plus petits, nous avons combiné deux ou trois UC soit parce qu’elles se reproduisent dans des lacs reliés entre eux et que la structure de la population n’est pas claire, soit parce qu’elles sont évaluées ensemble et que les données ne peuvent pas être séparées. L’UC Babine, la plus grande, est divisée en cinq stocks distincts en fonction du statut de mise en valeur et du moment de la montaison.


Les saumons rouges des rivières Nass et Skeena ont été répartis en 31 stocks aux fins de cet examen : les 7 stocks de la rivière Nass et les 24 stocks de la rivière Skeena décrits dans @SkeenaNassSkDataRep et résumés à la figure \@ref(fig:PopStrucGeneral). Les stocks peuvent être regroupés en fonction du cycle biologique et de la zone d’adaptation, ainsi que par bassin versant.


###  Projet de mise en valeur dans le lac Babine (ruisseau Pinkut et rivière Fulton)Babine Lake Development Project (Pinkut and Fulton)

Le lac Babine est le plus grand lac d’eau douce naturel de la Colombie-Britannique et le plus grand producteur de saumon rouge dans le bassin versant de la rivière Skeena; il comprend des populations sauvages et mises en valeur. Les saumons rouges qui en proviennent représentent de 87 à 93 % des remontes du regroupement de la rivière Skeena depuis 2000 [@CoxRogersSpilsted2012Babine]. Le PMVLB consiste en une série de frayères artificielles et de structures de contrôle du débit qui ont été aménagées dans le ruisseau Pinkut et la rivière Fulton depuis la fin des années 1960 afin d’augmenter la production de saumon rouge provenant du lac Babine.

Le saumon rouge du lac Babine fraye dans des dizaines d’affluents et des lacs dans le bassin versant principal du lac Babine, dans des tronçons de la rivière Babine entre le lac Babine et le lac Nilkitkwa, ainsi que dans le lac Morrison et le ruisseau Tahlo. Ici, le « saumon rouge du lac Babine » désigne tous les saumons rouges qui remontent jusqu’au lac Babine et aux zones en amont, et comprend les saumons rouges sauvages et mis en valeur des UC Babine-Nilkitkwa et Morrison-Tahlo, qui sont évaluées ensemble au déversoir de Babine ainsi que dans le cadre des programmes de dénombrement des smolts et des procédures de reconstitution des remontes. Les saumons rouges remontant la rivière Fulton et le ruisseau Pinkut, ainsi que le stock à montaison tardive de la rivière Babine, étaient les plus grandes populations de saumon rouge du lac Babine avant le PMVLB.

Du point de vue des pêches, le PMVLB s’est révélé un programme de mise en valeur réussi qui a considérablement augmenté la taille des remontes de saumon rouge des rivières Babine et Skeena. West et Mason (1987) ont estimé que pendant les deux premières décennies suivant l’achèvement des frayères et de l’infrastructure connexe dans le cadre du PMVLB, les remontes totales de saumon rouge de la rivière Skeena ont presque doublé, passant de 1,3 à près de 2,5 millions de poissons, avec des hausses proportionnelles des débarquements dans la pêche. 


(ref:PopStrucGeneral) Structure des populations de la zone de gestion du saumon rouge des rivières Skeena et Nass. Cette figure récapitule tous les stocks et les délimitations actuelles des unités de conservation (UC), les regroupant en fonction de leur cycle biologique (type lacustre, fluvial ou océanique) et de la zone d’adaptation en eau douce. Les stocks du ruisseau Pinkut et de la rivière Fulton mis en valeur font partie du cycle biologique et de la zone d’adaptation (CBZA) de type lacustre du cours moyen de la rivière Skeena.

```{r PopStrucGeneral, out.width= 350,  fig.cap="(ref:PopStrucGeneral)",fig.pos="H"}
include_graphics("data/1_FrenchFigs/Fig1_Pop_Structure_Small_French.PNG")
```

Les remontes de saumon rouge dans les réseaux comptant des frayères artificielles ont dépassé la capacité de l’habitat de fraie disponible dans les frayères et les tronçons gérés du ruisseau Pinkut et de la rivière Fulton la plupart des années depuis le début du PMVLB (Wood 1995). Les remontes de saumon rouge dépassant la capacité de reproduction sont considérées comme une production excédentaire. Certains de ces poissons sont pêchés dans le lac Babine dans le cadre du programme d’excédent de saumon par rapport aux besoins de géniteurs après que les cibles de charge pour les réseaux sauvages et mis en valeur sont atteintes, mais ces pêches n’ont pas lieu chaque année.

Des préoccupations ont été soulevées au sujet des effets négatifs du saumon rouge mis en valeur du lac Babine sur les saumons rouges sauvages du lac Babine et d’autres stocks de saumon rouge de la rivière Skeena. L’augmentation des remontes de saumon rouge mis en valeur du lac Babine a exercé de nouvelles pressions sur les stocks sauvages du lac Babine et d’autres stocks de la rivière Skeena dans les pêches de stocks mixtes, les stocks sauvages moins productifs ayant subi des taux d’exploitation plus élevés à la suite de l’intensification des pêches de stocks mixtes ciblant les remontes des stocks mis en valeur. Les remontes de saumon rouge dans les réseaux sauvages du lac Babine et les nombreuses populations de saumon rouge de la rivière Skeena autres que celles du lac Babine, qui étaient déjà en déclin avant le début du PMVLB, ont continué à décliner depuis sa mise en œuvre.

D’autres interactions potentielles entre les populations sauvages et mises en valeur du lac Babine comprennent la possibilité d’errance de saumons rouges issus de la mise en valeur dans les affluents utilisés pour la fraie par les poissons sauvages, et la concurrence liée à la dépendance à la densité dans les habitats de croissance en eau douce et en mer. Les frayères artificielles du PMVLB ont été construites à la suite d’évaluations limnologiques menées dans les années 1950 et 1960 qui ont révélé que la capacité de croissance du saumon rouge du lac Babine était sous-utilisée [@Brett1951; @Johnston1956] et que la production de saumon rouge était limitée par l’habitat de fraie disponible. À l’époque, on estimait que le lac Babine était en mesure d’accueillir 300 millions d’alevins de saumon rouge en croissance [@West1987]. De plus, la taille et la taille selon l’âge ont diminué pour le saumon rouge de la rivière Skeena, comme pour les autres populations de saumon dans le Pacifique Nord [@Oke2020RecentDeclinesBodySize], de même que la fécondité, et les diminutions modestes de la longueur totale sont associées à des baisses beaucoup plus importantes de la fécondité. Par exemple, la fécondité moyenne des saumons rouges du ruisseau Pinkut et de la rivière Fulton, mesurée dans les installations de mise en valeur, a diminué de plus de 10 % depuis les années 1980. Un déclin marqué de la fécondité des saumons rouges sauvages et mis en valeur de la rivière Skeena pourrait contribuer à la productivité réduite de ces populations. Un objectif d’échappée fondé sur l’abondance en fonction du nombre de géniteurs, qui suppose une production constante d’œufs dans le temps, ne tient peut-être pas compte des changements démographiques, comme les variations de la taille, de la composition selon l’âge ou du sex ratio, qui ont le potentiel d’augmenter les échappées nécessaires pour atteindre le RMD au fil du temps [@Staton2021].

L’intégration des limites de capacité du PMVLB dans l’élaboration d’un objectif d’échappée pour le regroupement de saumon rouge de la rivière Skeena pose des défis, car les cibles de charge pour les frayères artificielles et les tronçons à débit contrôlé du ruisseau Pinkut et de la rivière Fulton sont fixées de manière à maintenir des densités optimales de géniteurs et à maximiser la production d’alevins. Les modèles géniteurs-recrues, comme les modèles de Ricker, qui ont été utilisés pour établir des points de référence biologiques pour d’autres stocks, exigent une fourchette d’échappées de géniteurs (c.-à-d. un contraste dans les données) et peuvent ne pas produire d’estimations de paramètres utiles pour les stocks mis en valeur.

Les saumons rouges mis en valeur du ruisseau Pinkut et de la rivière Fulton représentent actuellement la composante la plus importante du regroupement de saumon rouge de la rivière Skeena et constituent donc un facteur important pour l’établissement d’un objectif d’échappée pour ce regroupement. Cependant, un examen complet et des recommandations mises à jour pour les cibles de charge et les procédures opérationnelles nécessiteront des commentaires et des conseils de l’exploitant de l’installation (Programme de mise en valeur des salmonidés du MPO); ils ne font pas partie de la portée de l’examen actuel, qui cherchait à définir des objectifs d’échappée biologiques pour les stocks sauvages de saumon rouge des rivières Skeena et Nass. Dans ce document, nous résumons les méthodes d’estimation et les reconstitutions des remontes pour le saumon rouge du lac Babine, et nous avons évalué les tendances de la production excédentaire et le ratio entre les saumons rouges sauvages et mis en valeur de la rivière Skeena afin d’élaborer un avis pour intégrer les stocks mis en valeur dans un objectif d’échappée pour le regroupement du saumon rouge de la rivière Skeena.

Pour répondre aux questions concernant les effets de la mise en valeur sur la production de saumon rouge sauvage, nous avons examiné et mis à jour les données sur la production de saumon rouge sauvage et mis en valeur du lac Babine afin d’évaluer les tendances générales des montaisons d’adultes, la qualité des échappées (taille, sex ratio et fécondité), la production d’œufs et la production d’alevins. Il ne s’agissait pas d’une évaluation exhaustive de la production de saumon rouge du lac Babine ni d’une analyse détaillée des effets des installations du PMVLB sur les stocks sauvages du lac et d’autres stocks de saumon rouge de la rivière Skeena. Nous donnons plutôt un aperçu de haut niveau des tendances observées de la production en eau douce en nous fondant sur les renseignements accessibles, et nous formulons des recommandations en vue de poursuivre le travail sur ce sujet.


## APERÇU DE L’ANALYSE {#AnalysisOverview}

### Portée et organisation du document de recherche {#PaperScopeOrg}

Les analyses présentées dans ce document de recherche ont été circonscrites en fonction des discussions sur la portée dans l’ensemble du processus d’examen des objectifs d’échappée des saumons rouges des rivières Skeena et Nass depuis 2019. Ces discussions comprenaient un processus du GTT, un atelier de détermination de la portée et les commentaires de deux examinateurs indépendants (section \@ref(EGProcess)), ainsi que le processus d’examen par les pairs [@SkeenaNassSkSAR; @SkeenaNassSkPRO], avec la réunion principale d’examen régional par les pairs en avril 2022 et un processus ultérieur de suivi dirigé par le SCAS afin d’élaborer des recommandations sur des approches de rechange pour l’élaboration de points de référence de gestion pour les regroupements de saumon rouge des rivières Skeena et Nass.

La plupart des problèmes liés aux données ont été résolus par le GTT avant la réunion d’examen par les pairs, et les détails ont été consignés dans un rapport distinct [@SkeenaNassSkDataRep]. Les révisions déterminées pendant l’examen par les pairs étaient axées sur des clarifications et quelques essais supplémentaires des choix de traitement des données, comme les procédures pour combler les années d’éclosion manquantes.

Les discussions sur la portée de nos analyses concernaient les trois sujets désignés comme des priorités clés pour l’analyse par le GTT (section \@ref(EGProcess)) :

* méthodes pour tenir compte de la productivité variable dans le temps dans l’ajustement du modèle géniteurs-recrues;
* méthodes d’élaboration de points de référence de gestion au niveau du regroupement en fonction des ajustements du modèle géniteurs-recrues au niveau du stock;
* traitement des saumons mis en valeur du ruisseau Pinkut et de la rivière Fulton dans les analyses.

Ce document est structuré selon une série d’étapes modulaires et comprend des tests de sensibilité approfondis des étapes initiales (l’élaboration de modèles géniteurs-recrues possibles et la sélection d’un modèle) ou des « éléments de base » qui sont utilisés dans les analyses suivantes. Nous donnons des exemples pour chacune des étapes suivantes afin de démontrer comment l’information biologique produite durant les étapes initiales pourrait servir à produire des avis scientifiques pour l’élaboration de points de référence. Selon le contexte et les exigences propres à l’examen actuel des objectifs d’échappée pour les saumons rouges des rivières Skeena et Nass et des processus futurs, ces avis peuvent être appliqués pour élaborer des questions précises et prioriser les analyses précises qui permettront d’y répondre. Les exemples fournis dans la deuxième partie des résultats visent à aider à déterminer ces priorités.

Les trois prochaines sections récapitulent comment nous avons abordé chacun de ces trois éléments et comment notre travail se compare aux analyses précédentes. La section \@ref(TORLink) décrit comment ce document de recherche aborde chacun des objectifs du cadre de référence, en établissant un lien avec des sections précises du document.


### Ajustements du modèle géniteurs-recrues, scénarios de productivité et points de référence biologiques {#AnalysisOverviewSR}


La portée et l’approche des travaux antérieurs sur les ajustements du modèle géniteurs-recrues et les points de référence biologiques pour les saumons rouges des rivières Skeena et Nass sont très différentes (tableau \@ref(tab:PastWorkTable)). Les travaux antérieurs cherchaient principalement à estimer des points de référence fondés sur les géniteurs-recrues au niveau du stock, comme Srmd, Smax et Urmd [@BockingetalMeziadinBM;  @Waltersetal2008ISRP; @KormanEnglish2013; @Hawkshaw2018Diss]. @PacificSalmonExplorer a également inclus des points de référence fondés sur le centile. Les détails du modèle géniteurs-recrues variaient considérablement d’une analyse à l’autre.

Afin de répondre aux priorités et aux objectifs du document de recherche énumérés à la section \@ref(PaperObj), nous avons ajusté d’autres formes du modèle de Ricker à chaque ensemble de données géniteurs-recrues (les regroupements des rivières Skeena et Nass et les 20 stocks qui les composent pour lesquels les données sur les géniteurs-recrues étaient suffisantes). Les types de modèles utilisés pour chaque stock dépendaient de l’accessibilité des données. Nous nous sommes concentrés sur l’élaboration de modèles simples à un seul stock et de formes du modèle variables dans le temps (modèles AR1 et bayésien récursif) afin d’explorer les profils de productivité sous-jacents pour les différents stocks et de calculer des points de référence biologiques pour d’autres scénarios de productivité.

À la suite d’un examen des ajustements possibles du modèle géniteurs-recrues par les membres du GTT, nous avons sélectionné le modèle qui décrit le mieux la dynamique de chaque stock. Nous avons procédé à un échantillonnage à partir des distributions a posteriori de certains modèles à un seul stock pour caractériser des scénarios de productivité élevée, faible, récente et à long terme, que nous avons ensuite utilisés comme intrants pour des exemples de méthodes de rechange pour l’élaboration de points de référence de gestion au niveau du regroupement, comme nous le résumons dans la section suivante.

Des points de référence biologiques standard fondés sur les paramètres géniteurs-recrues ont été calculés pour chaque ajustement du modèle (Srmd, Smax, Urmd; section \@ref(BMMethods)). Nous avons comparé les estimations des points de référence ainsi obtenues à l’abondance observée des géniteurs et aux estimations de la capacité des lacs.

Certains membres du GTT ont recommandé d’élaborer une version actualisée du modèle bayésien hiérarchique (MBH) utilisé dans un processus précédent pour estimer les points de référence biologiques pour le saumon rouge de la rivière Skeena, afin de pouvoir comparer directement les résultats générés à l’aide des données mises à jour. McAllister et Challenger (annexe \@ref(app:HBMFits)) ont fourni les résultats préliminaires pour les ajustements du MBH actualisé. Nous comparons les résultats initiaux du MBH à notre modèle géniteurs-recrues à un seul stock et examinons les sources potentielles des écarts observés (sections \@ref(HBMExploration) et \@ref(HBMResultsComp)). Cependant, nous n’avons pas inclus les résultats initiaux du MBH dans le processus de sélection du modèle pour définir les différents scénarios de productivité utilisés pour les exemples de résultats dans le reste du document. Cela pourrait être une priorité pour les travaux futurs.

La version initiale du document de recherche a été présentée lors de la réunion d’examen par les pairs en avril 2022 et comprenait d’autres ajustements du modèle géniteurs-recrues et d’autres scénarios de productivité. Les révisions déterminées pendant le processus d’examen par les pairs étaient axées sur des tests de sensibilité supplémentaires (p. ex. des scénarios de productivité supplémentaires), des comparaisons des méthodes de rechange (p. ex. ajustements du modèle à un seul stock et du modèle hiérarchique) et la clarification des étapes analytiques (p. ex. correction du biais log-normal, lissage des estimations des paramètres variables dans le temps).


### Approches de rechange pour l’élaboration de points de référence de gestion pour les regroupements de stocks {#AltApproachesOverview}

Les analyses présentées dans ce document de recherche visent à guider un examen des objectifs de gestion des saumons rouges des rivières Skeena et Nass. L’élaboration d’objectifs de gestion doit tenir compte de considérations sociales et économiques, en plus d’objectifs biologiques, et des travaux supplémentaires sont nécessaires après l’estimation des points de référence biologiques, comme Srmd [p. ex. @HoltIrvineBMvsRP].

Les facteurs à prendre en considération pour choisir une approche appropriée pour l’élaboration des cibles de gestion sont les suivants:

* *Type de stratégie de récolte* : L’objectif des points de référence de gestion est de déclencher une réaction à l’évolution des conditions. Par conséquent, l’approche d’élaboration des points de référence de gestion doit s’ajuster à la stratégie de récolte utilisée (règle fondée sur une échappée fixe, un taux d’exploitation fixe ou l’abondance).
* *Objectifs précis* : Des objectifs clairement définis sont nécessaires pour permettre une comparaison structurée et uniforme des points de référence de gestion de rechange.

Une approche simple, largement appliquée dans la gestion des pêches des saumons du Pacifique, consiste à choisir l’estimation d’un point de référence biologique, comme Srmd, comme objectif de gestion. Cette approche suppose : (1) une stratégie de récolte dans laquelle le stock dans son ensemble est géré en fonction d’un objectif d’échappée fixe, et tous les adultes en montaison dépassant l’objectif d’échappée sont récoltés; (2) un objectif de gestion visant à maximiser la récolte totale en moyenne sur le long terme, indépendamment de la variabilité annuelle de la récolte (la plus grande partie de la variation d’une année à l’autre des remontes se traduit par une variation des récoltes).



(ref:PastWorkTable) Aperçu des travaux précédents sur les points de référence biologiques et les objectifs d’échappée pour les saumons rouges des rivières Skeena et Nass. Les travaux antérieurs diffèrent sur le plan de la portée, de l’approche d’estimation, du type d’analyse et de l’évaluation du rendement. 

```{r PastWorkTable, echo = FALSE, results = "asis"}


past.work.df <- read.csv("data/Reference Tables/PastWork_Overview_French.csv",stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM") 


colnames(past.work.df ) <- linebreak(c("Référence", 
"Objectif\nprincipal","Portée","Estimation des\nparamètres", 
                                         "Simulation \nprospective","Évaluation\ndu rendement"))


past.work.df %>%
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "l",
                  caption = "(ref:PastWorkTable)") %>%
  kableExtra::column_spec(1, width = "4em") %>%
  kableExtra::column_spec(2, width = "7em") %>%
  kableExtra::column_spec(3, width = "9em") %>%
   kableExtra::column_spec(4, width = "12em") %>%
  kableExtra::column_spec(5, width = "7em") %>%
  kableExtra::column_spec(6, width = "9em") %>%
  # kableExtra::column_spec(7, width = "8em") %>%
   kableExtra::row_spec(1:  (dim(past.work.df)[1] -1), hline_after = TRUE)


```




\clearpage


Les solutions de rechange à cette approche de base ont évolué pour :

* refléter d’autres objectifs (p. ex. réduire la variabilité de la récolte);
* tenir compte de considérations pratiques (p. ex. contraintes imposées à la mise en œuvre des récoltes cibles);
* tenir compte des conditions actuelles (p. ex. remontes et productivité récentes);
* tenir compte de plusieurs stocks dans un regroupement.

La version initiale du document de recherche comprenait des exemples de sept approches de rechange. Chacune avait déjà été utilisée ou recommandée pour l’élaboration d’objectifs d’échappée de saumons du Pacifique, et certaines avaient déjà été appliquées aux saumons rouges des rivières Skeena ou Nass (tableau \@ref(tab:PastWorkTable)). Les forces et les limites de ces approches de rechange ont fait l’objet de discussions approfondies au cours de l’examen régional par les pairs du SCAS, mais les participants ne sont pas parvenus à un consensus et n’ont pas formulé de recommandation ferme au sujet d’une approche unique. Ils ont recommandé d’inclure une approche possible supplémentaire. Ce document donne des exemples de huit approches de rechange.

Les participants à l’examen par les pairs ont recommandé d’élaborer un avis clair sur le choix d’une approche appropriée. Un sous-groupe de participants a ensuite été créé pour élaborer cet avis, qui comportait : (1) la détermination des critères d’évaluation des approches de rechange; (2) l’exécution d’une évaluation détaillée de chaque approche; (3) la production d’un tableau récapitulatif des comparaisons, ainsi qu’un aperçu des défis pratiques pour les approches de rechange. Cette comparaison structurée des approches est un produit clé du processus d’examen par les pairs et est documentée à la section \@ref(AltApproachesComp). Ce document de recherche donne des exemples concrets pour chaque approche, afin que les décideurs disposent d’une base tangible pour établir l’ordre de priorité des prochaines étapes.



### Approches de rechange pour inclure les saumons rouges du ruisseau Pinkut et de la rivière Fulton mis en valeur dans les analyses {#AltApproachEnhanced}

La plus grande partie de la production de saumon rouge de la rivière Skeena se fait dans les frayères artificielles gérées activement dans le ruisseau Pinkut et la rivière Fulton; les stocks mis en valeur sont fondamentalement différents des stocks sauvages parce que l’on peut utiliser les variations de l’abondance observée des géniteurs et de la production qui en résulte pour ajuster les modèles géniteurs-recrues et estimer les points de référence biologiques. Cela crée des défis pour la modélisation de la dynamique des stocks et l’élaboration d’objectifs de gestion pour le regroupement des stocks.

Ce document de recherche porte sur les stocks sauvages de saumon rouge des rivières Skeena et Nass. Un examen exhaustif de la dynamique de la production issue de la mise en valeur et des cibles de charge des frayères sort de la portée de ce travail et peut être abordé dans un processus distinct dirigé par le Programme de mise en valeur des salmonidés du MPO, qui exploite les installations de mise en valeur du lac Babine. Une analyse explicite des compromis entre les objectifs de la production issue de la mise en valeur et les objectifs relatifs aux stocks sauvages dépasse également la portée du projet actuel.
 
Les analyses précédentes du saumon rouge de la rivière Skeena, récapitulées dans le tableau \@ref(tab:PastWorkTable), ont traité les stocks du ruisseau Pinkut et de la rivière Fulton de la même façon que les stocks sauvages, soit dans une analyse de stock unique, soit dans le cadre du regroupement. Nous supposons que la série chronologique des géniteurs utilisée pour ces analyses était la charge réelle de la frayère, plus la capacité en aval de la barrière, plutôt que l’échappée brute (c.-à-d. qu’elle tenait compte des suppléments et des excédents non reproducteurs). Cependant, cela n’était pas facile à vérifier.

Les processus d’examen par les pairs et de suivi ont examiné trois types d’approche pour la comptabilisation des saumons rouges mis en valeur de la rivière Skeena provenant du ruisseau Pinkut et de la rivière Fulton :

* *Élaborer des points de référence fondés sur les relations géniteurs-recrues pour tous les stocks sauvages et mis en valeur* : Ajuster d’autres formes du modèle géniteurs-recrues afin de générer des ensembles de paramètres pour d’autres scénarios de productivité pour les stocks sauvages et mis en valeur, et inclure les points de référence obtenus pour les stocks mis en valeur dans l’élaboration d’autres objectifs de gestion. Les mises en garde doivent être clairement énoncées au sujet des ajustements du modèle géniteurs-recrues pour les stocks mis en valeur et de leur incidence sur les résultats présentés, ainsi que des nuances pour l’interprétation. Par exemple, d’une part, les estimations de Srmd pour les stocks du ruisseau Pinkut et de la rivière Fulton sont très incertaines en raison du faible contraste dans l’abondance des géniteurs, de l’autre, elles ne sont pas pertinentes pour l’approche de gestion actuelle, qui met l’accent sur des objectifs de charge qui maximisent la production d’alevins.
* *Modéliser explicitement la dynamique distincte de gestion et de production des stocks mis en valeur* : Selon l’approche de regroupement choisie, la dynamique des stocks mis en valeur pourrait être intégrée de différentes façons. Par exemple, on pourrait ajouter les stocks du ruisseau Pinkut et de la rivière Fulton à une simulation prospective en élaborant des composantes supplémentaires du modèle, qui comprendraient un sous-modèle amélioré de la dynamique des populations (p. ex. deux étapes reliant la charge de la frayère à la production d’alevins, puis la survie entre les stades de l’alevin et de l’adulte), un sous-modèle de la récolte qui reflète les différences entre la récolte de stocks sauvages et mis en valeur (p. ex. les pêches ciblant l’excédent mis en valeur non reproducteur certaines années) et un sous-modèle de la gestion des frayères qui représente la façon dont les objectifs de charge pourraient changer en fonction de l’abondance du regroupement et du stock. Ce type de modèle pourrait mettre à l’essai les interactions entre les stratégies de récolte de rechange et les différentes stratégies d’exploitation des frayères, et explorer les compromis entre les prises totales, y compris les poissons issus des stocks mis en valeur, et les objectifs de conservation des stocks sauvages.
* *Établir une distinction claire entre les saumons rouges sauvages et mis en valeur de la rivière Skeena* : Compte tenu des exigences de la PSS, estimer les points de référence biologiques pour les stocks sauvages et appliquer les approches possibles pour les objectifs de gestion au regroupement du stock sauvage de la rivière Skeena, puis examiner comment mettre à l’échelle les objectifs de gestion obtenus pour couvrir l’ensemble de la remonte de la rivière Skeena.


La troisième option est la plus réalisable pour le projet actuel, car il est très complexe de refléter la dynamique de la production issue de la mise en valeur. Ainsi, la plupart des analyses dans le présent document portent sur les stocks sauvages, mais nous avons inclus un examen des renseignements disponibles sur les installations de mise en valeur du saumon rouge du lac Babine (annexe \@ref(ChannelReview)) et nous donnons deux exemples de la façon dont les objectifs de gestion pour le *stock sauvage de la rivière Skeena* pourraient être mis à l’échelle pour tenir compte de la production issue de la mise en valeur (section \@ref(SkeenaExpResults)). Nous recommandons que des travaux futurs intègrent davantage les considérations relatives aux stocks mis en valeur et sauvages dans le cadre d’une évaluation de la stratégie de gestion du saumon rouge de la rivière Skeena. À titre de référence, nous présentons certains résultats du modèle géniteurs-recrues pour les stocks mis en valeur à l’annexe \@ref(ChannelReview).



### Comment le présent document de recherche respecte le cadre de référence {#TORLink}

Nous avons examiné et mis à jour les données géniteurs-recrues et nous avons établi un cadre pour ajuster et sélectionner d’autres modèles géniteurs-recrues. Nous avons ensuite utilisé les ajustements du modèle géniteurs-recrues choisi dans des exemples concrets d’approches de rechange pour l’élaboration de points de référence de gestion pour les regroupements. Notre approche a été conçue en mettant l’accent sur la souplesse et la capacité de réagir rapidement à des conditions changeantes, à de nouvelles données, à d’autres formules pour les points de référence, à des demandes pour d’autres scénarios de productivité et à la comparaison d’autres objectifs. Il reste beaucoup de travail à faire pour relier pleinement l’information et les décideurs dans un processus structuré, comme une évaluation de la stratégie de gestion.

Nous récapitulons brièvement ci-dessous la façon dont le présent document de recherche aborde chaque objectif du cadre de référence.

1. *Élaborer une approche pour l’évaluation et la sélection de paramètres d’ajustement du modèle géniteurs-recrues en utilisant d’autres ensembles de données et d’autres formes de modèles – notamment des modèles variant dans le temps –, et appliquer cette approche au niveau des stocks et des regroupements pour les saumons rouges de la Skeena et de la Nass*. Nous avons préparé une liste de contrôle afin de déterminer les modèles géniteurs-recrues possibles pour les ajustements du modèle à un seul stock, couvrant trois formes de modèle de rechange (figure \@ref(fig:CandidateModels)). Nous avons testé les modèles possibles sur 20 ensembles de données au niveau du stock et trois ensembles de données au niveau du regroupement (sections \@ref(CandidateStockModels) et \@ref(CandidateAggModels)), et nous avons effectué des tests de sensibilité approfondis (section \@ref(Convergence)). De plus, nous avons comparé le modèle à un seul stock aux résultats d’une analyse sur les relations géniteurs-recrues indépendante effectuée par McAllister et Challenger (annexe \@ref(app:HBMFits)) à l’aide d’un MBH pour les mêmes ensembles de données (section \@ref(HBMResultsComp)).
2. *Élaborer une approche visant à déterminer d’autres scénarios de productivité plausibles (p. ex. la productivité moyenne à long terme par rapport à la productivité actuelle), ainsi que les ensembles de paramètres géniteurs-recrues correspondants*. Nous avons défini quatre autres scénarios de productivité visant à fournir une dynamique du stock contrastante, et établi des lignes directrices pour l’échantillonnage des ensembles de paramètres à partir des modèles géniteurs-recrues possibles afin d’alimenter les scénarios (figure \@ref(fig:ModelSelection)).
3. *Établir des points de référence biologiques pour les stocks au moyen d’ensembles de données actuelles et de méthodes appropriées pour les stocks de saumon rouge sauvages et mis en valeur des rivières Skeena et Nass, ce qui comprend* :
   a. *L’estimation et l’évaluation des points de référence biologiques candidats (p. ex. Srmd, Smax, Sgen, Urmd) à partir des ajustements du modèle basés sur les autres scénarios plausibles de productivité pour les stocks sauvages de saumon rouge de la Skeena et de la Nass*. La section \@ref(BMResults) présente les estimations au niveau du regroupement et au niveau du stock selon d’autres hypothèses de productivité.
   b. *L’examen de la capacité des chenaux et des tendances de productivité observées pour les stocks de saumon rouge de la Skeena qui ont été mis en valeur dans le cadre du projet d’aménagement du lac Babine*. L’annexe \@ref(ChannelReview) résume les renseignements accessibles sur la production issue des installations de mise en valeur du lac Babine.
4. *Faire une comparaison avec d’autres approches pour choisir des points de référence biologiques propres aux regroupements pour les saumons rouges des rivières Skeena et Nass, évaluer les avantages et les inconvénients de chaque approche et comparer les incertitudes dans les points de référence pour les regroupements générés au moyen de différentes approches*. Nous avons illustré les éléments de base de huit autres approches de regroupement (sections \@ref(BMResults) à \@ref(ProjBesdResults)) : ajustements du modèle de regroupement, somme simple des points de référence de l’abondance au niveau du stock, comparaison des taux d’exploitation durables au niveau du regroupement et du stock, profils d’équilibre au niveau du stock fondés sur des objectifs fixes pour les géniteurs, profils d’équilibre au niveau du regroupement fondés sur des taux d’exploitation fixes, points de référence pour les regroupements fondés sur l’état, points de référence pour les regroupements fondés sur la régression logistique et simulations prospectives. Les résultats des simulations comprennent deux extensions fortement prioritaires déterminées pendant l’examen par les pairs (covariation de la productivité, incertitude des résultats). Nous comparons les résultats dans la section \@ref(AggregationComparison).
5.*Établir des priorités pour les travaux futurs afin d’appuyer l’élaboration d’objectifs d’échappée propres aux stocks et de points de référence propres aux regroupements*. Les priorités pour les travaux futurs sont énumérées dans la section \@ref(PrioritiesFuture).
6. *Relever et examiner les incertitudes relatives aux points de référence propres aux stocks par la comparaison des résultats générés à l’aide d’autres formes de modèles géniteurs-recrues et d’autres ensembles de données, et faire de même pour les incertitudes relatives aux points de référence propres aux regroupements. Nous avons effectué des tests de sensibilité approfondis des ajustements du modèle géniteurs-recrues, y compris d’autres formes du modèle et d’autres hypothèses relatives aux valeurs a priori (section \@ref(SingleStockSRResults)), et comparé les résultats aux estimations d’un MBH de McAllister et Challenger (annexe \@ref(app:HBMFits)) sur le même ensemble de données (section \@ref(HBMResultsComp)). De plus, nous avons testé d’autres options de traitement des données (annexe \@ref(AltSRTest)), d’autres méthodes de calcul pour les points de référence biologiques (annexe \@ref(BMCalcTest) et l’effet de l’inclusion d’une correction du biais log-normal sur le paramètre de productivité ln.alpha (annexe \@ref(BiasCorrtest)). Nous comparons les approches de regroupement dans la section \@ref(AggregationComparison).

 

<!--chapter:end:01-Introduction.Rmd-->

\clearpage
# MÉTHODES

Ce chapitre décrit les six étapes de nos analyses.

- *Examen des données sur les géniteurs-recrues* : résume l’examen des données, la reconstitution des remontes, les hypothèses sur la composition selon l’âge et les données sur les géniteurs-recrues accessibles par stock.
- *Examen de la production issue de la mise en valeur* : décrit brièvement les sources de données et la compilation de l’information accessible pour les stocks issus du PMVLB (ruisseau Pinkut et rivière Fulton).
- *Ajustements du modèle géniteurs-recrues* : décrit les autres formes du modèle, la mise en œuvre bayésienne et les critères de détermination des modèles possibles pour chaque stock, selon les données accessibles.
- *Scénarios de productivité* : décrit la façon dont les ensembles de paramètres ont été échantillonnés à partir des ajustements du modèle géniteurs-recrues présélectionné pour représenter des scénarios de productivité moyenne à long terme, récente, élevée et faible pour chaque stock.
- *Points de référence biologiques* : décrit comment les points de référence standard (p. ex. Srmd) ont été calculés pour chaque stock, compte tenu d’autres hypothèses de productivité.
- *Points de référence de gestion* (section \@ref(AnalysisOverview)) : décrit des approches de rechange pour élaborer des points de référence de gestion (p. ex. profils d’équilibre, simulations prospectives) compte tenu d’autres hypothèses de productivité et de divers exemples d’objectifs de gestion, ainsi que la justification des exemples fournis dans le présent document de recherche.



## SOURCES DES DONNÉES {#DataSources}



### Examen des données{#DataReview}

Le type et la qualité des données accessibles façonnent le type et la qualité des avis scientifiques qui peuvent être élaborés au sujet des stratégies de gestion du saumon [p. ex. @AdkisonData]. L’importance d’élaborer un ensemble à jour et convenu de données sur les géniteurs-recrues a été un sujet récurrent des discussions du GTT et de l’atelier de détermination de la portée. De ce fait, le GTT a consacré une part importante de l’effort du projet à un examen approfondi des données sur les saumons rouges des rivières Skeena et Nass, qui est documenté dans un rapport autonome [@SkeenaNassSkDataRep] et brièvement résumé ici. Les données sur les géniteurs-recrues mises à jour ont jeté une base solide pour les travaux futurs et le processus d’examen a aidé à simplifier le flux de travail pour les mises à jour futures des données.

L’examen des données a porté sur le premier objectif de l’examen des objectifs d’échappée (section \@ref(EGProcess)) et sur des éléments clés des données sur les géniteurs-recrues : estimations du nombre de géniteurs pour les réseaux repères et les extensions connexes, reconstitutions des remontes dans la rivière (y compris les hypothèses sur la période de montaison des adultes), estimations des prises des Premières Nations et estimations de la composition selon l’âge. Plusieurs autres examens pertinents et mises à jour des ensembles de données ont eu lieu en même temps, notamment un examen du modèle de reconstitution des remontes de saumon rouge à la limite nord (RRSRLN) et un examen complet des programmes d’estimation de l’abondance des saumons de la rivière Nass. Nous avons utilisé les renseignements les plus récents tirés de ces examens en date de décembre 2021, jusqu’à l’année de montaison 2019.

L’examen des données comportait six étapes : (1) examen de la structure des stocks pour les populations de saumon rouge des rivières Skeena et Nass; (2) mises à jour des données de base pour intégrer toute l’information accessible; (3) traitement automatisé des données; (4) vérifications automatisées des données axées sur le contraste, les changements au fil du temps et les valeurs aberrantes potentielles; (5) collaboration avec le GTT pour générer des notes de données pour chaque stock; (6) réalisation de tests de sensibilité approfondis (p. ex. estimations rétrospectives des points de référence biologiques à l’aide d’estimations déterministes simples des paramètres de Ricker).



### Estimations des géniteurs {#SpnEst}

Les estimations de l’abondance des géniteurs pour les saumons rouges des rivières Skeena et Nass proviennent d’une combinaison de programmes d’évaluation menés dans les deux bassins versants. Les programmes d’évaluation des stocks comprennent des tourniquets dans la basse Nass et une pêche d’essai dans la basse Skeena, qui produisent des estimations de l’abondance ainsi que de la composition selon l’âge et des stocks pour les deux regroupements. Des dénombrements par recensement de haute précision sont effectués pour les plus grands réseaux de chaque bassin versant (passe migratoire du lac Meziadin pour le regroupement de la rivière Nass et déversoir de la rivière Babine pour le regroupement de la rivière Skeena). Les programmes de dénombrement dans les frayères, y compris les dénombrements aux déversoirs, les relevés aériens, les programmes de marquage-recapture et les observations à pied dans les cours d’eau, génèrent des dénombrements des géniteurs pour les réseaux repères, qui sont extrapolés pour estimer l’échappée totale pour chaque stock.

Le saumon rouge du lac Babine est dénombré chaque année au déversoir de la rivière Babine, en aval du lac Nilkitkwa, depuis 1949. Le déversoir de la rivière Babine, qui est actuellement exploité par la Nation de Lake Babine, en vertu d’un contrat conclu avec le MPO, fournit des dénombrements quotidiens de toutes les espèces de saumon de la mi-juillet à la fin septembre et englobe la majeure partie de la remonte de saumon rouge. L’exploitation du déversoir a été prolongée jusqu’à la fin novembre quelques années. On suppose que le programme des déversoirs fournit un dénombrement complet pour la plupart des années, mais il a été ajusté certaines années pour tenir compte du passage estimé pendant les périodes où la barrière n’était pas opérationnelle.

On réalise des estimations visuelles des échappées dans un maximum de 30 affluents utilisés pour la fraie par les saumons rouges sauvages du lac Babine, chaque année, au moyen de relevés à pied ou aériens menés par le MPO et la Nation de Lake Babine. Les estimations tirées des relevés visuels de l’échappée pour les réseaux abritant des saumons sauvages du lac Babine sont ajustées pour tenir compte du biais de sous-estimation à l’aide des méthodes décrites dans @WoodLifeHist1995. La [base de données NuSEDS du MPO](https://open.canada.ca/data/en/dataset/c48669a3-045b-400d-b730-48aafe8c5ee6) tient à jour le dénombrement annuel dans chaque réseau du lac Babine. Les estimations brutes des géniteurs pour les différents réseaux abritant des saumons sauvages du lac Babine sont extrapolées et combinées en estimations ajustées pour les composantes à montaison précoce et moyenne et la composante sauvage à l’aide d’une procédure de reconstitution des remontes décrite dans @WoodLifeHist1995.

Le MPO a effectué un examen approfondi des estimations des géniteurs dans les cours d’eau repères des rivières Skeena et Nass. Nous avons compilé et examiné tous les renseignements accessibles sur les échappées dans les cours d’eau provenant des fonds de données locaux et régionaux afin de déterminer si des données supplémentaires étaient accessibles pour les cours d’eau repères et les années désignés comme manquants dans les versions précédentes de la base de données NCCDSB et pour vérifier l’exactitude des estimations publiées dans la base de données NuSEDS. Pour les années où les données sur les dénombrements des différents cours d’eau étaient disponibles (à partir de 1998 pour la plupart des réseaux), les estimations des échappées ont été recalculées et comparées aux données de la base de données NuSEDS pour déterminer les écarts. 

\clearpage
### Estimation de l’excédent biologique du ruisseau Pinkut et de la rivière Fulton issu de la mise en valeur {#SurplusEst}

Les géniteurs du lac Babine issus de la mise en valeur sont dénombrés aux déversoirs situés en aval du ruisseau Pinkut et de la rivière Fulton, et lorsqu’ils entrent dans les frayères. Les géniteurs sauvages sont dénombrés au moyen de dénombrements visuels effectués à pied et de relevés aériens afin de produire des estimations de l’aire sous la courbe pour les affluents occupés par des populations sauvages. Les saumons rouges sauvages du lac Babine sont répartis en trois groupes (montaison précoce, moyenne et tardive) en fonction de la période de montaison des adultes.

Le nombre de poissons à la barrière sur la rivière Babine dépasse habituellement la somme des échappées aux installations de mise en valeur et des estimations visuelles des échappées pour les réseaux occupés par les populations sauvages, qui sont généralement sous-estimées dans les relevés visuels. La différence non comptabilisée entre le dénombrement aux barrières et les estimations des échappées est principalement considérée comme une production excédentaire. L’excédent peut représenter une grande proportion des remontes du saumon rouge du lac Babine chaque année. Les relevés en plongée effectués dans les années 1990 ont confirmé que ces poissons supplémentaires ne sont pas des géniteurs lacustres efficaces manqués pendant les relevés dans les cours d’eau.


@WoodLifeHist1995 a élaboré une procédure de reconstitution pour estimer la production excédentaire après avoir corrigé les estimations visuelles des échappées pour les groupes d’affluents occupés par les populations sauvages pour le biais de sous-estimation, qui ont été mises à jour dans @Woodetal1998Babine ainsi que @CoxRogersSpilsted2012Babine et conservées dans une base de données du MPO. @Woodetal1998Babine ainsi que @CoxRogersSpilsted2012Babine décrivent la justification de ces ajustements. Les équations sont présentées dans le tableau 2 de l’annexe de  @CoxRogersSpilsted2012Babine et les calculs sont résumés dans l’annexe C.3 de @SkeenaNassSkDataRep. Pour résumer, les ajustements sont calculés selon les étapes suivantes.

* On suppose que les unibermarins (mâles immatures, âge 3) contribuent très peu à la population reproductrice, de sorte que les dénombrements d’unibermarins à la barrière sur la rivière Babine sont exclus des estimations. Cependant, les dénombrements de saumons rouges unibermarins au déversoir de la rivière Babine sont intégrés aux estimations de la remonte totale du regroupement de la rivière Skeena et aux estimations de la composition selon l’âge.
* Les dénombrements des géniteurs dans les affluents occupés par les populations sauvages sont combinés par groupe de période de montaison (précoce, moyenne et tardive) en dénombrements non ajustés pour les trois stocks sauvages (à montaison précoce, moyenne et tardive).
* Les reproducteurs efficaces dans les réseaux mis en valeur sont estimés comme la somme des poissons qui ont traversé les déversoirs du ruisseau Pinkut ou de la rivière Fulton, et de la capacité estimée des frayères naturelles en aval du chenal (5 000 géniteurs pour le ruisseau Pinkut et 40 000 géniteurs pour la rivière Fulton).
* Les estimations combinées pour les affluents occupés par les populations sauvages sont extrapolées pour tenir compte du biais de sous-estimation des dénombrements visuels [@Woodetal1998Babine], et l’estimation ajustée est répartie par groupe de période de montaison des poissons sauvages en fonction de leur abondance relative dans les relevés visuels.
* L’excédent issu de la mise en valeur du lac Babine est calculé comme la différence entre le dénombrement au déversoir de la rivière Babine, les géniteurs sauvages ajustés, les géniteurs mis en valeur efficaces et la récolte en amont du déversoir de la rivière Babine. Ces adultes supplémentaires, qui ne frayent pas, sont considérés comme un excédent biologique qui contribue aux nutriments du lac Babine, mais sont exclus des estimations de l’abondance des géniteurs. Ils sont toutefois inclus dans les estimations de la remonte.


### Estimations des prises


Les estimations des prises sont dérivées de nombreux programmes de surveillance des prises en milieu marin et dulcicole qui consignent le nombre de poissons récoltés dans les différentes pêches; on en échantillonne certains pour déterminer leur âge et la composition des stocks, à l’aide de la variation des motifs des écailles ou de la fréquence des allèles génétiques.

Le comité technique de la limite nord de la Commission du saumon du Pacifique estime les prises dans les pêches au Canada et en Alaska, les taux d’exploitation et les remontes totales des populations des regroupements des rivières Skeena et Nass à l’aide du modèle RRSRLN depuis 1982.

Les récoltes des Premières Nations, qui sont regroupées par zone de pêche, sont intégrées dans des modèles fluviaux qui estiment le taux d’exploitation total pour les regroupements et les stocks qui les composent. Des groupes autochtones pêchent le saumon rouge dans les bassins versants des rivières Skeena et Nass. Leurs pêches diffèrent selon la zone, la période et le type d’engin, et sont soumises à des exigences différentes en matière de gestion et de déclaration des prises. Les membres du GTT ont collaboré avec les gestionnaires des pêches du MPO et les groupes des Premières Nations des vallées des rivières Skeena et Nass pour mettre à jour les estimations des prises pour chaque zone de pêche.



### Estimations de la composition selon l’âge


Les estimations de la composition selon l’âge, qui sont utilisées pour estimer le recrutement par année d’éclosion, sont disponibles dans les programmes d’échantillonnage d’écailles et d’otolithes. Des estimations annuelles de la composition selon l’âge sont disponibles pour les deux regroupements, mais pas pour la plupart des stocks individuels des rivières Skeena et Nass, qui ont été rarement échantillonnés.

#### Données sur la composition selon l’âge des regroupements

Les estimations annuelles de la composition selon l’âge des regroupements des stocks de saumon rouge des rivières Skeena et Nass proviennent de programmes de pêche d’essai ciblant des regroupements, y compris la pêche d’essai à Tyee (Skeena, de 1955 à aujourd’hui), la pêche d’essai aux tourniquets des Nisga’a (de 1992 à aujourd’hui) et la pêche d’essai à Monkley Dump (Nass, avant 1992), ainsi que des pêches commerciales du Canada et des États-Unis en milieu marin (Skeena et Nass, jusqu’à la fin des années 1990). L’Alaska Department of Fish and Game (ADFG), depuis 2000, et le MPO (pour les pêches canadiennes), les années précédentes, déterminent l’âge des poissons dont des écailles ont été prélevées lors des pêches commerciales et d’essai.

Des ajustements sont apportés pour tenir compte des pêches sélectives en fonction de la taille dans les pêches d’essai. Pour la rivière Skeena, des échantillons sont prélevés sur les gros saumons rouges dans la pêche d’essai à Tyee afin de déterminer la proportion des principales classes d’âge, qui sont appliquées à l’échappée totale de gros poissons pour répartir la remonte des gros poissons dans les principales classes d’âge. On ajoute les saumons rouges d’âge 3 en montaison provenant du dénombrement aux barrières en estuaire à la remonte totale, et on recalcule la remonte annuelle pour chaque classe d’âge afin d’estimer les proportions de toutes les classes d’âge dans la remonte totale. Pour le regroupement de la rivière Nass, on calcule les estimations annuelles pour les mâles précoces en extrapolant la prise totale d’unibermarins aux tourniquets à l’aide du taux annuel de marquage des adultes qui est ajusté pour tenir compte de l’hypothèse d’un taux de marquage plus élevé pour les petits poissons.


#### Données sur la composition selon l’âge de chaque stock

Les données annuelles sur la composition selon l’âge ne sont pas disponibles pour la plupart des stocks de saumon rouge des rivières Skeena et Nass, sauf pour le saumon rouge de la rivière Meziadin. L’âge de la plupart des poissons échantillonnés, à quelques exceptions près (ci-dessous), a été déterminé au laboratoire de détermination de l’âge à partir des écailles du MPO à la Station biologique du Pacifique, et les données sont stockées dans une base de données régionale (PADS) dans des dossiers numérisés de lectures d’âge individuel (de 1989 à 2019) ou sous forme de cartes numérisées de l’âge selon des écailles/otolithes (avant 1989).

Nous avons examiné et mis à jour les données sur l’âge disponibles pour tous les stocks de saumon rouge des rivières Skeena et Nass afin de nous assurer que toutes les données accessibles étaient intégrées aux analyses des géniteurs-recrues. Tous les enregistrements d’âge disponibles pour les stocks de saumon rouge des rivières Skeena et Nass ont été téléchargés depuis la base de données PADS. Pour les années antérieures à 1989, le nombre de poissons de chaque classe d’âge était calculé à partir de cartes de l’âge numérisées pour chaque stock/année pour lequel des données étaient accessibles. Les proportions selon l’âge pour chaque cours d’eau/année ont été calculées comme le nombre de chaque classe d’âge divisé par le nombre total d’échantillons pour l’année en question, à l’exclusion des âges partiels ou des échantillons illisibles.

Compte tenu des données accessibles, les calculs du recrutement pour la plupart des stocks sont actuellement fondés sur une composition selon l’âge moyenne (tableau \@ref(tab:Age2Stock)). Les estimations de la composition selon l’âge de chaque stock ont été utilisées pour la plupart des stocks de la rivière Nass, mais pour la majorité des stocks de la rivière Skeena, y compris les cinq stocks de la rivière Babine, nous nous sommes fondés sur la composition selon l’âge moyenne du regroupement de la rivière Skeena. Des estimations annuelles de la composition selon l’âge ont été utilisées pour les saumons rouges de type océanique et fluvial de la basse Nass (tableau \@ref(tab:Age2Stock)).

L’utilisation de la composition selon l’âge moyenne peut introduire un biais dans les paramètres géniteurs-recrues [@Zabelagecomp]. En plus de notre examen des données sur l’âge pour veiller à ce que toutes les données accessibles sur l’âge soient intégrées aux analyses des géniteurs-recrues, nous avons effectué des tests de sensibilité pour évaluer l’effet de l’utilisation de la composition moyenne plutôt que de la composition annuelle selon l’âge dans le cadre de l’examen des données [@SkeenaNassSkDataRep]. Pour les stocks pour lesquels des données annuelles sur l’âge étaient accessibles, y compris les stocks des rivières Meziadin, Kwinageese et Babine, nous avons recalculé les estimations du recrutement pour la composition selon l’âge annuelle et moyenne, et nous avons estimé les différences entre les paramètres géniteurs-recrues obtenus. Pour ces stocks, la différence de Srmd découlant de l’utilisation de la composition selon l’âge moyenne par rapport à la composition selon l’âge annuelle variait de -2 à 31 %.

 

\clearpage
(ref:Age2Stock) Estimations de la composition selon l’âge de chaque stock utilisées dans les calculs du recrutement. Les tableaux \@ref(tab:StockOverview) et \@ref(tab:DataOverviewSkeena)indiquent les noms complets des stocks et le nombre d’années d’éclosion pour lesquelles des données sur les géniteurs-recrues sont accessibles, en fonction des compositions selon l’âge correspondantes tirées de ce tableau.

```{r Age2Stock, echo = FALSE, results = "asis"}


ages.match <- read.csv("data/Reference Tables/Match_Age2Stock.csv",stringsAsFactors = FALSE) %>%
                  left_join(stock.info %>% select(Stock, LHAZShort,LHAZSeq, Watershed, StkNmS), by= "Stock") %>%
                     arrange(LHAZSeq, Watershed, StkNmS) 


ages.match$LHAZShort[duplicated(ages.match$LHAZShort)] <- ""
ages.match$Watershed[duplicated(ages.match$Watershed)] <- ""
avg.idx <- grepl("Avg",ages.match$AgeComp)
ages.match$Type <- NA
ages.match$Type[avg.idx] <- "Moyenne"
ages.match$Type[!avg.idx] <- "Annuelle"

ages.match$AgeComp <- gsub("Avg","",gsub("Annual","",ages.match$AgeComp))
ages.match$AgeComp <- gsub("Babine","Skeena",ages.match$AgeComp)

ages.match <- ages.match %>% select(LHAZShort, Watershed, StkNmS, Type, AgeComp) %>%
               dplyr::rename(Stock= StkNmS, CBZA = LHAZShort, 'Bassin versant' = Watershed,
			   'Comp. âge' = AgeComp )

ages.match %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = "l",
                  caption = "(ref:Age2Stock)") %>%
   kableExtra::row_spec(c(1,6,7,13,22,30), hline_after = TRUE) %>%
   kableExtra::row_spec(c(3:5,9:12,18,20,21,23:27), extra_latex_after = "\\cmidrule(l){2-5}") 

```


 
 
 
\clearpage
### Relevés dans les lacs

Les lacs de croissance du saumon rouge des rivières Skeena et Nass sont évalués régulièrement ou périodiquement lors des relevés suivants.

* *Relevés des juvéniles* : Des relevés des juvéniles avec transects hydroacoustiques et échantillonnage biologique sont effectués en rotation pour évaluer l’abondance et la biomasse des alevins dans les lacs de croissance du saumon rouge des rivières Skeena et Nass. À l’exception du lac Babine, tous les grands lacs ont fait l’objet de relevés à plusieurs reprises depuis la fin des années 1990. Lorsqu’elles sont accessibles, les données sur l’abondance des alevins fournissent une contre-vérification des estimations de la reconstitution des échappées de géniteurs obtenues à partir de relevés visuels (c.-à-d. si l’abondance estimée des géniteurs est réaliste par rapport à l’abondance observée des alevins pour une année d’éclosion donnée).
* *Évaluations de la productivité* : Des évaluations périodiques de la productivité des lacs ont été effectuées pour la plupart des lacs de croissance du saumon rouge des rivières Skeena et Nass à l’aide des rendements photosynthétiques pour estimer la capacité de croissance en eau douce de chaque lac (estimations de la capacité fondées sur le rendement photosynthétique). Les estimations de la capacité fondées sur le rendement photosynthétique fournissent des renseignements utiles sur les limites de la capacité de croissance en eau douce, que l’on peut utiliser directement pour estimer l’abondance optimale de géniteurs pour un réseau donné, ou incorporer dans la modélisation des géniteurs-recrues en tant que valeurs a priori de la capacité de croissance. Il convient de noter que la plupart des estimations de la capacité fondées sur le rendement photosynthétique pour les lacs de croissance du saumon rouge des rivières Skeena et Nass n’ont pas été mises à jour depuis le milieu des années 2000.



### Reconstitutions des remontes {#RunReconEst}


On applique des méthodes de longue date et bien documentées pour combiner les données sur les prises et les échappées pour les saumons rouges des rivières Skeena et Nass afin d’obtenir des estimations cohérentes de l’abondance des géniteurs, des remontes et des taux d’exploitation pour les deux regroupements de stocks et pour la plupart des stocks qui les composent. Les données sur la production de saumon rouge de la rivière Skeena, tenues à jour par la Division de l’évaluation des stocks de la côte nord du MPO, intègrent les données sur les prises et les échappées provenant de différentes sources. Les prises totales, les taux d’exploitation et les remontes du saumon rouge des rivières Skeena et Nass sont estimés chaque année depuis 1982 par le comité technique bilatéral de la limite nord à l’aide du modèle RRSRLN [@GazeyEnglish2000NBRR; @Englishetal2004NBRR]. De plus amples renseignements sont fournis à l’annexe C de @SkeenaNassSkDataRep. Les taux d’exploitation de chaque stock de saumon rouge des rivières Skeena et Nass, y compris le saumon rouge du lac Babine, sont dérivés de modèles en rivière @Englishetal2017SSIRR] qui combinent les données des récoltes en rivière et de la période de montaison du stock avec les extrants du modèle RRSRLN afin de produire des estimations annuelles des remontes totales pour chaque stock, résumées à l’annexe C de @SkeenaNassSkDataRep.

Les méthodes et les hypothèses pour les modèles de reconstitution des remontes sont documentées dans une série de rapports techniques [p. ex. @Englishetal2006CSAFW; @Englishetal2012CUInd; @Englishetal2013StAD; @Englishetal2004NBRR; @Englishetal2019NCCReview].


Les principales étapes de l’analyse pour la reconstitution des remontes du saumon rouge des rivières Skeena et Nass sont les suivantes.

* Expansion des estimations des géniteurs pour estimer le nombre total de géniteurs en fonction du nombre observé dans les relevés. Ces expansions tiennent compte des poissons qui n’ont pas été dénombrés (selon la méthode de relevé et la mise en œuvre annuelle), ainsi que des poissons provenant de réseaux qui n’ont pas fait l’objet de relevés.
* Pour les stocks du lac Babine, des calculs supplémentaires des géniteurs sont effectués pour prendre en compte (1) la différence entre les dénombrements des regroupements à la barrière sur la rivière Babine et les estimations dans les frayères, et (2) la capacité effective des frayères et de l’habitat de fraie naturel. L’abondance des géniteurs dans les réseaux mis en valeur est le nombre de géniteurs admis dans les frayères artificielles et les tronçons des cours d’eau mis en valeur, plus toute échappée brute jusqu’à la capacité estimée des frayères naturelles en aval des installations de mise en valeur. Les remontes supplémentaires d’adultes qui ne frayent pas dans les affluents occupés par les populations sauvages du lac Babine sont considérées comme un excédent biologique et sont exclues des estimations de l’abondance des géniteurs, mais elles sont incluses dans les estimations de la remonte.
* Reconstitutions des remontes pour les deux regroupements de stocks, qui tiennent compte des prises canadiennes et américaines en milieu marin dans les eaux d’approche [@GazeyEnglish2000NBRR; @Englishetal2004NBRR; @NBRRDeck2018]. Les reconstitutions des remontes pour les deux regroupements sont élaborées bilatéralement chaque année dans le cadre du processus du comité technique de la limite nord de la Commission du saumon du Pacifique.
* Reconstitutions des remontes pour les différents stocks dans chaque bassin versant, qui tiennent compte des prises en rivière et de la période de montaison du stock.



Les prises en rivière représentent une part importante des prises totales de saumon rouge des rivières Skeena et Nass au Canada. Les estimations des prises en rivière, des taux d’exploitation et de la remonte totale pour les différents stocks sont calculées à l’aide de différentes approches pour les saumons rouges des rivières Skeena et Nass.

Pour les pêches dans la rivière Skeena, le modèle de *reconstitution de la remonte du saumon rouge de la rivière Skeena* [RRSRRS, @Englishetal2013StAD; @Englishetal2017SSIRR] combine l’information provenant des récoltes en rivière (période et abondance), sur l’échappée et la période de montaison pour répartir les prises entre les stocks en fonction de la période de montaison et de l’emplacement de la pêche. Le modèle RRSRRS utilise la même approche que le modèle de reconstitution des remontes examiné par les pairs pour le saumon chinook du fleuve Fraser [@English2007FrSkRR]. Le modèle RRSRRS établit des estimations de la remonte le long de la migration en amont par les pêches, à partir des estimations de la pêche d’essai à Tyee. Il modélise les prises en rivière pour 12 zones de pêche en rivière autochtones dans l’ensemble du bassin versant de la rivière Skeena avec un pas de temps d’une journée, d’après les estimations quotidiennes de la remonte pour le regroupement du saumon rouge de la rivière Skeena provenant de la pêche d’essai à Tyee dans la basse Skeena.

Pour le saumon rouge de la rivière Nass, les taux annuels de prises en rivière pour le regroupement de stocks (c.-à-d. la récolte totale en rivière divisée par la remonte entrant dans la rivière Nass) ont été appliqués de façon égale à tous les sous-stocks de saumon rouge de la rivière Nass. Cette approche a été jugée appropriée pour les stocks de saumon rouge de la rivière Nass parce que la grande majorité des prises en rivière a lieu dans la basse Nass, où tous les stocks sont vulnérables. Les taux de récolte en rivière sont combinés aux taux d’exploitation marine du modèle RRSRLN pour estimer le taux d’exploitation total pour les différents sous-stocks. 

La reconstitution des remontes en rivière se fait au niveau du stock. Dans certains cas, les stocks sont modélisés en tant que groupe, parce que les méthodes actuelles d’identification génétique des stocks ne peuvent pas différencier les différents stocks pour estimer les courbes de période de montaison individuelles. Le modèle suppose donc une période de montaison égale pour les différents stocks. À l’heure actuelle, le modèle RRSRRS modélise 20 stocks de saumon rouge de la rivière Skeena et les taux de prise annuels en rivière pour le regroupement des stocks de saumon rouge de la rivière Nass sont appliqués à chacun des 10 sous-stocks de la rivière Nass. @Englishetal2019NCCReview ont utilisé le même nombre de stocks, avec certains changements dans les groupes de stocks (p. ex. Brown Bear/Cranberry, Gingit/Zolzap, et l’ajout du ruisseau Strohn).

La base de données NCCDSB a été mise à jour en 2021 afin d’intégrer les examens des estimations des géniteurs dans les réseaux repères, les données sur la composition selon l’âge pour les regroupements et les stocks individuels, ainsi que les hypothèses sur la période pour les sous-stocks des rivières Skeena et Nass, à l’aide des données d’identification génétique des stocks mises à jour recueillies entre 2000 et 2020 dans le cadre des programmes de pêche d’essai à Tyee et de tourniquets dans la rivière Nass, des récoltes en milieu marin et fluvial des Premières Nations du bassin versant de la rivière Skeena, ainsi que des années supplémentaires de données pour l’année de montaison 2019.


### Estimations du recrutement

Nous avons appliqué les estimations de la composition selon l’âge et calculé le recrutement par année d’éclosion pour chaque stock (regroupements et stocks qui les composent). Nous avons utilisé les estimations du recrutement fondées sur les principales classes d’âge (les âges qui ont contribué à plus de 2 % de la remonte au moins une fois).

Les données sur les géniteurs-recrues pour la composante sauvage du regroupement de la rivière Skeena (données sur le regroupement du stock sauvage de la rivière Skeena) ont été calculées en soustrayant les géniteurs et les remontes calculés pour le saumon rouge du ruisseau Pinkut et de la rivière Fulton des estimations du regroupement, puis en utilisant la composition annuelle par âge du regroupement pour recalculer les recrues.

Les mises à jour des estimations des recrues au niveau du stock et du regroupement étaient le résultat des mises à jour des reconstitutions des remontes et des estimations de la composition selon l’âge décrites ci-dessus.



### Vérifications des données et tests de sensibilité

Des paramètres de la qualité des données ont été calculés pour chaque stock en examinant l’ensemble de la série chronologique et les observations individuelles [@SkeenaNassSkDataRep]. Des préoccupations potentielles liées aux données ont été cernées si les valeurs des paramètres tombaient en dessous, au-dessus ou en dehors de la plage des valeurs de déclenchement spécifiées par l’utilisateur, selon le paramètre. Les valeurs de déclenchement ont été choisies en fonction des directives publiées, le cas échéant, ou par consensus du GTT.

Les paramètres suivants ont été utilisés pour effectuer un examen systématique de ces considérations pour les 31 stocks de saumon rouge des rivières Skeena et Nass.

* *Contraste* : Un faible contraste dans les données sur les géniteurs est signalé si Max(Spn)/Min(Spn)<4, en utilisant le seuil défini dans @Clarketal2014PercBM.
* *Nombre d’observations* : Des données insuffisantes pour l’ajustement des modèles géniteurs-recrues sont signalées si le nombre d’années d’éclosion pour lesquelles on dispose d’estimations des géniteurs et des recrues est inférieur à 10. Cette valeur de déclenchement a été sélectionnée en fonction de l’expérience générale avec d’autres stocks de saumon rouge. Il s’agit d’identifier les stocks ayant « peu » de données à l’aide d’une définition uniforme.
* *Grandes et petites estimations qui sortent de l’ajustement du modèle* : Les grandes estimations des géniteurs ou des recrues qui sortent de l’ensemble de données sur les géniteurs-recrues sont signalées si la plus grande valeur observée est plus du double de la plus grande valeur pour les années d’éclosion pour lesquelles on dispose d’estimations des géniteurs et des recrues. Les petites estimations des géniteurs ou des recrues en dehors de l’ensemble de données sur les géniteurs-recrues sont signalées si la plus petite valeur observée est inférieure à la moitié de la plus grande valeur pour les années d’éclosion pour lesquelles on dispose d’estimations des géniteurs et des recrues. Ces valeurs de déclenchement ont été sélectionnées pour repérer les valeurs extrêmes.
* *Facteur d’expansion important* : L’expansion des géniteurs dans les cours d’eau repères à l’estimation du nombre total de géniteurs est signalée si l’expansion moyenne pour l’ensemble de la série chronologique est supérieure à 3 (c.-à-d. que les observations sont multipliées par plus de 3).


Des commentaires qualitatifs ont été compilés pour décrire les données sur les géniteurs, les données sur les prises, les données sur la composition selon l’âge, les estimations du recrutement et les données sur les relevés dans les lacs, et ils comprenaient les considérations suivantes pour chacune de ces catégories.

* *Indicator quality*: commentary on quality of spawner surveys (i.e., sum of estimates from indicator streams), based on survey types and coverage. Weirs and fishways were generally categorized as highly accurate, but if they capture multiple stocks then quality of stock composition estimates and relative abundance of the component stocks was also considered. 
* *Expansions*: categorizes the total expansion factor applied to the estimate from indicator streams into 4 categories. Expansion factors were taken from the previously published run reconstruction estimates .
* *Total spawner estimate quality*: Commentary on overall quality of the spawner estimate, considering the quality of the index estimate and the expansion factor.
* *Overall rating for spawner estimate*: The quality of spawner estimates was assessed on a 5-point scale from Very Good to Very Poor, based on the commentary for *TotalSpn*.

* *Qualité de l’indicateur* : Commentaire sur la qualité des relevés des géniteurs (somme des estimations des cours d’eau repères), en fonction du type des relevés et de leur couverture. Les déversoirs et les passes migratoires ont généralement été classés comme étant très précis, mais s’ils capturent plusieurs stocks, on a également tenu compte de la qualité des estimations de la composition des stocks et de l’abondance relative des différents stocks.
* *Expansion* : Catégorise le facteur d’expansion total appliqué à l’estimation à partir des cours d’eau repères en quatre catégories. Les facteurs d’expansion ont été tirés des estimations publiées précédemment [p. ex. @Englishetal2019NCCReview].
* *Qualité globale de l’estimation des géniteurs* : Commentaire sur la qualité globale de l’estimation des géniteurs, compte tenu de la qualité de l’estimation pour les cours d’eau repères et du facteur d’expansion.
* *Note globale pour l’estimation des géniteurs* : La qualité des estimations des géniteurs a été évaluée sur une échelle de 5 points, allant de « très bonne » à « très mauvaise », en fonction du commentaire pour TotalSpn (total des géniteurs).



Qualité des estimations des prises par stock 


* *Milieu marin* : Commentaires sur la question de savoir si la voie de migration marine pour le stock est probablement semblable à celle du regroupement utilisée dans le modèle RRSRLN, en tenant compte du cycle biologique (p. ex. type océanique ou lacustre) et de la taille du stock (c.-à-d. que le modèle saisit mieux la dynamique des grands stocks : le lac Meziadin sur les stocks de la rivière Nass et le lac Babine sur les stocks de la rivière Skeena); cela a ensuite une incidence sur la question de savoir si la proportion de prises en milieu marin du regroupement dans les principales pêches pour ce stock est probablement semblable à la composition du stock dans le projet d’évaluation du cours inférieur de la rivière (tourniquets sur la rivière Nass, pêche d’essai à Tyee), en tenant compte du comportement migratoire et de la taille du stock.
* *Milieu fluvial* : Commentaires couvrant 1) la qualité des estimations de la période de montaison et de la vitesse de migration; 2) la qualité des estimations des prises dans différents tronçons du cours d’eau modélisés.
* *Qualité de l’estimation des prises totales* : Commentaire sur la qualité globale des estimations des prises totales, compte tenu de la qualité des éléments ci-dessus.
* *Note globale pour l’estimation des prises* : Qualité des estimations des prises évaluée sur une échelle de 5 points, allant de « très bonne » à « très mauvaise », en fonction du commentaire pour TotalCt (prises totales).



Qualité des estimations du recrutement par stock: 

* *Note pour la remonte* : Décrit la qualité des estimations de la remonte sur une échelle de 5 points, allant de « très bonne » à « très mauvaise », en fonction des notes des commentaires pour les estimations étendues des géniteurs et les estimations des prises totales, ainsi que de l’ampleur relative des prises et de l’abondance des géniteurs (p. ex. une très mauvaise estimation des prises a peu d’effet sur la qualité de l’estimation de la remonte si les prises sont très faibles).
* *Structure selon l’âge* : Catégorise la structure selon l’âge du stock – stable (très peu de changement d’une année à l’autre), variable (une certaine variation des proportions relatives, mais classe d’âge dominante constante) ou très variable (la classe d’âge dominante varie).
* *Données sur l’âge* : Catégorise les estimations de la composition selon l’âge – annuelles (estimations accessibles pour la plupart des années), remplies (estimations accessibles pour de nombreuses années, les années restantes étant remplies avec la moyenne) ou moyenne (quelques années de données, en utilisant la moyenne pour toutes les années).
* *Qualité de l’estimation du recrutement total* : Commentaire sur la qualité globale des estimations du recrutement, compte tenu de la qualité des estimations de la remonte totale et de la composition selon l’âge.
* *Note globale pour l’estimation du recrutement* : Qualité des estimations du recrutement évaluée sur une échelle de 5 points, allant de « très bonne » à « très mauvaise », en fonction des commentaires pour TotalRec.


Des tests de sensibilité ont été utilisés pour évaluer si les problèmes de données potentiels relevés dans la section précédente étaient susceptibles d’influer sur les estimations des points de référence biologiques standard (p. ex. $S_{RMD}$, $S_{MAX}$), afin de faciliter la définition de la portée du modèle et de déterminer les domaines d’incertitude prioritaires à prendre en compte dans les analyses subséquentes [@SkeenaNassSkDataRep].

Nous avons effectué trois séries de tests de sensibilité : les variations des données, l’incertitude dans les données (bootstrap) et l’incertitude dans l’ajustement du modèle (estimations bayésiennes). Des tests de sensibilité ont été mis en œuvre avec le progiciel RapidRicker [@RapidRicker], qui reproduit un ensemble complet de variations des données pour calculer des points de référence biologiques standard. Les ajustements du modèle et les calculs des points de référence sont générés à l’aide d’un ajustement de régression linéaire simple et d’un ajustement bayésien à l’aide du code JAGS adapté de @MillerPestalTakuSk.

Les résultats de ces tests de sensibilité initiaux de l’ajustement du modèle géniteurs-recrues, documentés dans le rapport de données [@SkeenaNassSkDataRep], ont servi à déterminer les priorités d’analyse pour ce document de recherche (section \@ref(EGProcess)).



### Données sur les géniteurs-recrues accessibles {#AvailableSRData}


Des séries chronologiques complètes sur les géniteurs et les recrues sont disponibles pour le regroupement de la rivière Nass depuis 1982 et pour celui de la rivière Skeena depuis 1970. Les reconstitutions des remontes au niveau du stock pour la rivière Skeena sont disponibles depuis 1960.

Les stocks de saumon rouge des rivières Nass et Skeena ont été répartis en 31 stocks (tableau \@ref(tab:StockOverview)) : 7 stocks de la rivière Nass et 24 stocks de la rivière Skeena [@SkeenaNassSkDataRep]. Les stocks peuvent être regroupés par cycle biologique et zone d’adaptation (CBZA), ainsi que par bassin versant. Les variations du cycle biologique sont le type lacustre (TL), le type fluvial (TF) et le type océanique (TO). Les taux d’exploitation sont estimés pour les réseaux repères (stocks ou groupes de stocks ayant des périodes de montaison semblables et des estimations fiables des prises et de l’abondance des géniteurs). En vertu de la PSS [@WSP], les saumons anadromes canadiens sont regroupés en UC distinctes. Pour les saumons rouges des rivières Nass et Skeena, la plupart des stocks correspondent à une seule UC. Certains des plus petits stocks combinent deux ou trois UC, soit parce qu’ils remontent dans des lacs reliés entre eux, soit parce qu’ils sont évalués ensemble et qu’il n’est pas possible de séparer les données. L’UC Babine/Nilkitkwa, la plus grande UC de la rivière Skeena, a été divisée en cinq stocks distincts en fonction de la mise en valeur et de la période de montaison.

La longueur et la qualité des séries chronologiques sur les géniteurs-recrues varient d’un stock à l’autre (tableau \@ref(tab:DataOverviewSkeena), figure \@ref(fig:SRDataOverview)). Les stocks plus grands ont généralement plus d’années de données de meilleure qualité. Le GTT a élaboré une note de qualité uniforme pour les estimations du nombre de géniteurs, de la remonte et du recrutement en fonction des types de données et de calculs @SkeenaNassSkDataRep. Les notes d’estimation des géniteurs tiennent compte de la qualité du relevé dans un cours d’eau repère ainsi que du facteur d’expansion (p. ex. un recensement aux barrières de tous les géniteurs est considéré comme *très bon*, mais un relevé aérien couvrant moins du cinquième du stock est considéré comme *mauvais*). Les notes des estimations de la remonte tiennent compte du fait qu’un stock est bien représenté dans la comptabilisation des prises et dans les analyses de reconstitution des stocks (p. ex. un grand stock avec une identification fiable et les estimations temporelles qui en découlent est considéré comme *bon*, mais un petit stock incertain est considéré comme *mauvais*). Les notes des estimations du recrutement combinent la qualité de l’estimation de la remonte avec les considérations de la qualité et de la quantité des données sur la composition selon l’âge (p. ex. données annuelles sur l’âge par stock par rapport à la composition selon l’âge moyenne d’un stock témoin).

Les 31 stocks ont été répartis en trois groupes en fonction de l’abondance relative et des données accessibles. Le groupe 1 comprend 14 grands stocks avec de longues séries chronologiques de données sur les géniteurs-recrues, qui représentent environ 98 % des remontes totales combinées de saumon rouge des rivières Nass et Skeena; le groupe 2 comprend 9 stocks plus petits avec certaines données sur les géniteurs-recrues, qui représentent ensemble environ 2 % des remontes totales combinées, et le groupe 3 comprend 8 stocks sans aucune donnée sur les géniteurs-recrues.

Nous avons exclu les observations sur les géniteurs-recrues invraisemblables et rempli les vides pour pouvoir ajuster les formes du modèle qui nécessitent une série chronologique complète. Plus précisément, nous avons exclu les années d’éclosion où l’estimation du nombre de recrues par géniteur (R/S) dépassait 45, nous avons rempli les vides d’un an dans les estimations du nombre de géniteurs en utilisant la moyenne des estimations précédentes et suivantes, nous avons rempli la remonte correspondante en utilisant l’estimation du taux d’exploitation de l’année tirée des modèles de reconstitution des remontes, puis appliqué les données remplies dans le calcul des recrues. Les procédures de filtrage et de remplissage n’ont été appliquées qu’à certains stocks (tableau \@ref(tab:DataOverviewSkeena), figure \@ref(fig:SRDataOverview)). Nous avons testé l’effet de ces étapes du traitement des données à l’aide du modèle de Ricker de base : l’élimination des valeurs aberrantes par filtrage a généralement eu un effet plus important sur les estimations des paramètres que l’étape du remplissage (annexe \@ref(AltSRTest)).


De nouveaux renseignements sont récemment devenus accessibles pour certains stocks et n’ont pas été intégrés à la version actuelle des analyses, mais nous considérons qu’il s’agit d’une priorité élevée pour la mise à jour dans les futurs travaux.

* *Bowser* : La population du lac Bowser a probablement contribué de façon importante à la remonte du regroupement de saumon rouge de la rivière Nass certaines années. Les estimations visuelles des échappées, qui sont faussées par une turbidité glaciaire élevée, n’ont pas été effectuées régulièrement pour le saumon rouge du lac Bowser, qui est principalement une population qui fraye près des rives. Les estimations antérieures de l’abondance du saumon rouge du lac Bowser ont été obtenues à l’aide de différentes méthodes, notamment l’identification des stocks au moyen d’analyses des motifs des écailles et, plus récemment, l’identification génétique des stocks appliquée aux échappées du regroupement de la rivière Nass. Ces différentes méthodes ont produit des estimations divergentes pour certaines années et une évaluation plus poussée est nécessaire pour rapprocher ces estimations avant de pouvoir élaborer une série chronologique sur les géniteurs-recrues. 
* *Bear/Azuklotz* : D’après les résultats préliminaires d’un nouveau programme d’évaluation (déversoir vidéo installé en 2021 sur la rivière Bear en aval du lac Bear), les échappées combinées des géniteurs fondées sur des relevés visuels aériens pourraient sous-estimer la population reproductrice réelle d’un facteur beaucoup plus important que celui qui a été pris en compte dans les facteurs d’expansion utilisés actuellement dans les procédures de reconstitution des remontes. Pour nos analyses, nous avons utilisé la série chronologique existante des abondances reconstituées qui ne tiennent pas compte des nouveaux renseignements provenant du programme du déversoir vidéo, sachant que ces données pourraient changer dans un proche avenir.
* *Skeena, type fluvial* : On considère actuellement que les données sur ce stock sont insuffisantes, car il n’y a pas suffisamment d’information sur l’abondance des géniteurs ou la répartition du saumon rouge de la rivière Skeena de type fluvial pour estimer l’abondance totale dans le bassin versant pour ces populations. Bien que de petites populations de géniteurs de type fluvial persistent et soient dénombrées chaque année dans le bassin versant de la rivière Kispiox et la rivière Bulkley, on ne sait pas si ces populations représentent la plupart ou seulement une petite partie des géniteurs de type fluvial dans le bassin versant de la rivière Skeena. Des renseignements anecdotiques tirés de relevés historiques et récents donnent à penser que des populations persistantes ou éphémères sont également présentes dans les affluents de la haute Skeena. La structure de la population des géniteurs de type fluvial dans le bassin versant de la rivière Skeena n’est pas claire, en raison du peu d’échantillons dans la base de référence génétique et d’une faible différenciation entre certaines populations de type fluvial des rivières Skeena et Nass. On ne sait pas si les types fluviaux de la rivière Skeena représentent une ou plusieurs populations, ou une seule population pour les types fluviaux de la rivière Skeena et de la haute Nass. 



\clearpage
(ref:StockOverview) Structure des populations de saumon rouge des rivières Skeena et Nass. Les 31 stocks se répartissent en 7 groupes distincts en fonction du type de cycle biologique et de la zone d’adaptation en eau douce (CBZA) et 21 bassins versants. Nous utilisons des étiquettes abrégées des stocks (Stock) pour les tableaux et les figures tout au long du document de recherche. Des indicateurs du taux d’exploitation (IndTE) sont disponibles pour la plupart des stocks. Les stocks correspondent à une ou plusieurs UC. La population de la rivière Babine est actuellement désignée comme une seule UC, mais évaluée et analysée comme cinq stocks distincts (marqués par un *).

```{r StockOverview, echo = FALSE, results = "asis"}

n.nass <- sum(stock.info$Basin == "Nass")
n.skeena <- sum(stock.info$Basin == "Skeena")

table.in <- stock.info %>% #dplyr::filter(Basin == "Nass") %>%
   select(LHAZShort,Watershed,Stock,StkNmS,ERInd,NumCU) %>%
   dplyr::rename(CBZA = LHAZShort, Étiquette = StkNmS,UC= NumCU,IndTE = ERInd)

table.in[table.in$Watershed == "Babine","UC"] <- "*"


table.in$CBZA[duplicated(table.in$CBZA)] <- ""
table.in$Watershed[duplicated(table.in$Watershed)] <- ""
   
table.in <- table.in %>% dplyr::rename('Bassin versant' = Watershed)  
   
   
table.in %>%   
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l","l","l","r"),
                  caption = "(ref:StockOverview)") %>%
   kableExtra::row_spec(rep(n.nass,2), hline_after = TRUE) %>%
      kableExtra::row_spec(c(1,6,13,22,30), hline_after = TRUE) %>%
    kableExtra::row_spec(c(2,4,5,9:12,14,16,17,23:25,28,29), extra_latex_after = "\\cmidrule(l){2-6}") 

```





\clearpage

(ref:DataOverviewSkeena) Récapitulatif des données sur les géniteurs et les recrues (GR) accessibles par stock. Les stocks sont triés en fonction de leur taille relative, calculée en pourcentage de l’abondance cumulative des géniteurs (pGén) depuis 2000 dans les deux regroupements de stocks. Les notes de qualité pour les estimations des géniteurs (Gén), des remontes (Rem) et des recrues (Rec) sur une échelle de 5 points allant de « très mauvaise » (VP) à « très bonne » (VG) sont fondées sur le consensus du GTT déterminé à l’aide d’un ensemble de critères qualitatifs (Pestal et al. en cours d’examen). nG est le nombre de géniteurs estimé. Le nombre d’années d’éclosion avec des données sur les géniteurs-recrues varie selon le traitement des données pour certains stocks : Orig = ensemble de données original tiré de Pestal et al. (en cours d’examen), Filtre = nombre d’années d’éclosion filtrées en raison de R/S > 45, Remplissage = nombre d’années pour lesquelles un écart d’un an dans les estimations des géniteurs et de la remonte pourrait être comblé, Utilisation = nombre d’années d’éclosion avec les estimations des géniteurs et des recrues après le filtrage et le remplissage.

```{r DataOverviewSkeena, echo = FALSE, results = "asis"}



table.in <- stock.info %>% arrange(desc(PercEffSpn)) %>%
   left_join(data.notes.tab %>%  select(Stock,SpnRating,RunRating,RecRating), by= "Stock") %>%
   left_join(alt.sr.data %>% dplyr::filter(Label == "Main") %>% group_by(Stock) %>%
   summarize(Orig = sum(!is.na(RpS))), by = "Stock") %>%
      left_join(alt.sr.data %>% dplyr::filter(Label == "Filter45_Infill") %>% group_by(Stock) %>%
   summarize(Use = sum(!is.na(RpS)),NumFiltered = sum(Filter), NumInfilled = sum(Infill)), by = "Stock") %>%
   # left_join(table.in <- alt.sr.test1 %>% dplyr::filter(DataVersion == "Main") %>% mutate_all(as.character) %>%
    #         dplyr::rename(nSRo = NumBrYr) %>% select(Stock, nSRo), by = "Stock") %>%
   #left_join(table.in <- alt.sr.test1 %>% dplyr::filter(DataVersion == "Filter45_Infill") %>% mutate_all(as.character) %>%
   #          dplyr::rename(nSR = NumBrYr) %>% select(Stock, NumFiltered,NumInfilled,nSR), by = "Stock") %>%
   select(LHAZShort,StkNmS,PercSpnLabel, NumSpn, SpnRating,RunRating,RecRating,
          Orig, NumFiltered,NumInfilled, Use) %>%
   dplyr::rename(CBZA = LHAZShort, Stock = StkNmS,pGén =  PercSpnLabel, nG = NumSpn, 
                 Gén = SpnRating,Rem = RunRating,Rec =RecRating,Filtre = NumFiltered,
                 Remplissage =NumInfilled, Utilisation = Use)

table.in[table.in == "Data Deficient"] <- "DD"
table.in[table.in == "Good"] <- "G"
table.in[table.in == "Moderate"] <- "M"
table.in[table.in == "Poor"] <- "P"
table.in[table.in == "Very Good"] <- "VG"
table.in[table.in == "Good to V. Gd"] <- "G to VG"
table.in[table.in == "Very poor"] <- "VP"


table.in[is.na(table.in)] <- "-"
table.in[table.in == "NA"] <- "-"
 
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","r","r","l","l","l","r","r","r","r"),
                  caption = "(ref:DataOverviewSkeena)") %>%
   add_header_above(c(" " = 4, "Qualité" = 3, "Données sur les GR" = 4),bold=TRUE) #%>% 
    # kableExtra::row_spec(c(6,15,23), hline_after = TRUE) %>%
     #kableExtra::column_spec(3, width = "10em") %>%
        #kableExtra::row_spec(c(1:5,7:14,16:22), extra_latex_after = "\\cmidrule(l){2-8}") 

```




\clearpage
(ref:SRDataOverview) Disponibilité des données géniteurs-recrues – par stock. Chronologie des données accessibles par année d’éclosion, les stocks étant regroupés en fonction du cycle biologique et de la zone d’adaptation. Les cercles bleu foncé sont les années d’éclosion avec des estimations de géniteurs et de recrues. Les points bleu pâle sont les années d’éclosion avec seulement des estimations de géniteurs. Les diamants rouge pâle marquent les années d’éclosion où un écart d’un an dans les estimations des géniteurs a été rempli. Les diamants rouge foncé indiquent les années d’éclosion remplies où une estimation correspondante des recrues pourrait être calculée. Un « X » rouge indique les observations filtrées (R/S > 45) qu’il n’a pas été possible de remplir. Les nombres entre parenthèses représentent la part de l’abondance cumulative des géniteurs depuis 2000 dans les deux regroupements de stocks..

```{r SRDataOverview,  out.width= 415, fig.cap="(ref:SRDataOverview)"}
include_graphics("data/1_FrenchFigs/Fig2_DataOverview_ResDoc_Filter45Infill_French.png")
```




\clearpage
## AJUSTEMENT DU MODÈLE GÉNITEURS-RECRUES {#SRFitting}

### Formes du modèle de Ricker {#ModelForms}


La dynamique des populations de saumon rouge est couramment modélisée avec la courbe de Ricker, qui suppose que la productivité, exprimée en ln(Rec/Gén), diminue à mesure que l’abondance des géniteurs augmente, ce qui donne une relation en forme de dôme entre les géniteurs et le nombre total de recrues. Les principaux jalons des travaux antérieurs sur les saumons rouges des rivières Skeena et Nass (tableau \@ref(tab:PastWorkTable)) ont tous utilisé la courbe de Ricker, mais les détails de la mise en œuvre variaient considérablement d’une analyse à l’autre.


* @BockingetalMeziadinBM ont utilisé un modèle déterministe de Ricker pour un seul stock (Meziadin, le plus grand stock de la rivière Nass).
* @Waltersetal2008ISRP and @Hawkshaw2018Diss ont élaboré des modèles de Ricker d’espace d’états pour 9 UC de la rivière Skeena.
* @Hawkshaw2018Diss a également modélisé le saumon rouge de la rivière Skeena comme un seul stock avec la dynamique de Ricker dans une simulation plurispécifique.
* @CoxRogersetal2010 ont utilisé des estimations de la capacité des lacs de croissance fondées sur le rendement photosynthétique pour élaborer des paramètres de Ricker pour 28 lacs de croissance, les saumons rouges du lac Babine étant traités comme formant un seul stock. 
* @KormanEnglish2013 ont utilisé des ajustements de Ricker bayésiens hiérarchiques pour 17 UC de la rivière Skeena, divisant Babine en une UC mise en valeur et trois UC sauvages et utilisant la capacité de croissance des lacs fondée sur le rendement photosynthétique comme valeurs a priori pour Smax pour toutes les UC, à l’exception de Babine-Nilkitkwa. 
* @PacificSalmonExplorer a mis à jour l’analyse de  @KormanEnglish2013, mais a modifié la division de Babine en ajoutant une UC mise en valeur et deux UC sauvages, et en ajoutant quatre UC pour le saumon rouge de la rivière Nass.
* @Atlasetal2021HabitatDyn ont utilisé le modèle de Ricker bayésien hiérarchique ajusté aux données GS pour 54 UC de saumon rouge de la côte nord (1 modèle pour chaque zone biogéoclimatique).


@Fleischmanetal2013CJFASStateSpace ont indiqué que « Le modèle de Ricker est, de loin, le choix le plus courant pour les analyses de la relation géniteurs-recrues chez les saumons du Pacifique, probablement parce que (i) il peut tenir compte de la surcompensation, qui est évidente dans de nombreux ensembles de données sur les saumons du Pacifique, et (ii) il est prudent en ce qui concerne les niveaux optimaux d’échappées (pour les valeurs fixes du paramètre de productivité et de la capacité de charge, SRMD est toujours plus élevé dans l’hypothèse d’un modèle de Ricker que dans celle d’un modèle de Beverton-Holt). » [traduction]

Nous avons utilisé trois formes du modèle de Ricker pour la relation géniteurs-recrues afin d’évaluer la sensibilité des estimations des points de référence biologiques aux hypothèses de rechange au sujet des résiduels observés.

*Modèle de Ricker de base (BR)*

Un ajustement standard bayésien du modèle de Ricker, fondé sur une relation linéaire fixe entre $ln(R/S)$  (productivité) et $S$ (abondance des géniteurs). Le modèle de Ricker de base, comme toute régression linéaire de base, suppose que les résiduels ont une distribution normale aléatoire $N$ avec une moyenne de 0 et une variance fondée sur l’échantillon, sans profil des écarts dans le temps. L’ajustement du modèle de Ricker de base constitue une bonne base de référence, même si le profil observé dans la productivité contredit cette hypothèse (à quoi ressembleraient les estimations des points de référence si on ignorait les changements dans le temps?). Pour l’année d’éclosion $i$

\begin{equation} 
  ln(R_{i}/S_{i}) = ln(\alpha) - \beta * S_{i} + \varepsilon_{i}
  (\#eq:BasicRicker)
\end{equation} 

\begin{equation} 
  \epsilon_i \sim N(0,\sigma^2)
  (\#eq:BasicRickerResid)
\end{equation} 



*Ricker with lag-1 autocorrelation (AR1)*

Une extension du modèle de Ricker, qui est encore une fois fondée sur une relation linéaire fixe entre $ln(R/S)$ (productivité) et $S$ (abondance des géniteurs), mais qui cherche également un profil sous-jacent dans les résiduels (estimer un résiduel précis pour chaque année d’éclosion $i$).

\begin{equation} 
  ln(R_{i}/S_{i}) = ln(\alpha) - \beta * S_{i} + \varepsilon_{i}
  (\#eq:AR1Ricker)
\end{equation} 


\begin{equation} 
  \varepsilon_i = \phi \varepsilon_{i-1} + \nu_i
  (\#eq:AR1RickerResid1)
\end{equation} 

\begin{equation} 
  \nu_i \sim N(0,\sigma_{\nu}^2)
  (\#eq:AR1RickerResid2)
\end{equation} 



*Modèle de Ricker avec productivité variable dans le temps (PVT)* 

Une extension du modèle de Ricker, fondée sur une relation linéaire variable dans le temps entre $ln(R/S)$ (productivité) et $S$  (abondance des géniteurs) [p. ex. @PetermanPyperGrout2000ParEst;  @PetermanPyperMacGregor2003KF; @HoltMichielsens2020RecBayes; @Freshwateretal2020Selectivity; @FraserSkRPASAR; @Huangetal2021FraserSkRPA]. Pour chaque année d’éclosion, le paramètre de productivité $ln(\alpha)$  est estimé en fonction de la productivité observée pour l’année en question et de l’estimation de $ln(\alpha)$ pour l’année d’éclosion précédente, afin de générer une série plus ou moins lisse de paramètres $ln(\alpha)$.


\begin{equation} 
  ln(R_{i}/S_{i}) = ln(\alpha)_i - \beta * S + \varepsilon_{i}
  (\#eq:RecBayesRicker1)
\end{equation} 

\begin{equation} 
  ln(\alpha)_{i} = n(\alpha)_{i-1} + \omega_{i}
  (\#eq:RecBayesRicker2)
\end{equation} 


\begin{equation} 
  \omega_i \sim N(0,\sigma_{\omega}^2)
  (\#eq:RecBayesRicker3)
\end{equation} 

\begin{equation} 
  \epsilon_i \sim N(0,\sigma^2)
  (\#eq:RecBayesRicker4)
\end{equation} 

Deux approches de rechange pour ajuster un modèle de Ricker avec PVT ont été appliquées pour les données sur les saumons du Pacifique : (1) le filtre de Kalman [@PetermanPyperGrout2000ParEst;  @PetermanPyperMacGregor2003KF] et (2) le modèle bayésien récursif [@HoltMichielsens2020RecBayes; @Freshwateretal2020Selectivity; @FraserSkRPASAR; @Huangetal2021FraserSkRPA]. Bien que la structure mathématique de ces modèles soit la même (équations \@ref(eq:RecBayesRicker1),  \@ref(eq:RecBayesRicker4)), la mise en œuvre computationnelle de l’étape d’estimation est très différente. Une différence clé réside dans le fait que les mises en œuvre du filtre de Kalman comprenaient une étape de lissage, contrairement aux mises en œuvre du modèle bayésien récursif.
Le modèle avec PVT décrit ici utilise la version bayésienne récursive, conformément aux travaux récents sur le saumon rouge du fleuve Fraser [@FraserSkRPASAR; @Huangetal2021FraserSkRPA].


*Résumé*

Les modèles de Ricker de base et AR1 sont tous deux ajustés à une relation géniteurs-recrues qui est censée décrire les propriétés inhérentes du stock qui demeurent constantes au fil du temps. La forme du modèle AR1 a déjà été utilisée dans les analyses des objectifs d’échappée pour les stocks de saumon rouge de l’Alaska et transfrontaliers du Nord [p. ex.  @MillerPestalTakuSk; @Connorsetal2022]. La version avec productivité variable dans le temps du modèle de Ricker suppose que la productivité change réellement au fil du temps et tente d’extraire un profil plus ou moins lissé, en déterminant les périodes de productivité élevée et faible. Le modèle avec PVT a été utilisé pour certains stocks de saumon transfrontaliers du Nord [p. ex. @PestalJohnstonTakuCo] et dans plusieurs applications du saumon rouge du Fraser [p. ex. @FrSkWSPBM;@Huangetal2021FraserSkRPA; @PetermanDorner2011Fraser].

### Estimation bayésienne des paramètres à l’aide de la méthode de Monte-Carlo par chaîne de Markov (MCCM) 

Nous avons dérivé des estimations des paramètres à l’aide des méthodes bayésiennes pour les modèles géniteurs-recrues possibles au niveau du stock et au niveau du regroupement selon la méthode MCCM à l’aide du moteur d’échantillonnage JAGS [@Plummer03jags]  et de la fonction *jags()* du progiciel *R2jags* [@R2jags]. L’annexe \@ref(SingleStockSRFits) décrit la configuration du code et indique le code JAGS pour les trois formes du modèle.

L’estimation MCCM combine les hypothèses sur les valeurs a priori de chaque paramètre avec la probabilité de différentes valeurs des paramètres d’après les données pour générer un échantillon a posteriori des valeurs des paramètres. Les hypothèses sur les valeurs a priori peuvent être non informatives (p. ex. la productivité des stocks peut varier de très élevée à très faible, et nous ne précisons pas de préférence) ou informatives (p. ex. nous pensons que la productivité du stock est semblable à la productivité moyenne de plusieurs stocks proches ayant un cycle biologique semblable).

Le moteur d’échantillonnage commence par quelques valeurs aléatoires échantillonnées à partir de la distribution a priori, puis recherche des variations de tous les paramètres pour déterminer les valeurs qui relient vraisemblablement les données observées à la relation spécifiée (p. ex. une fonction de Ricker). L’échantillonnage doit être configuré de manière à ce que les valeurs des paramètres se stabilisent (*convergence*) et que les parties antérieures de la chaîne d’échantillonnage soient éliminées (*rodage*).

Les mises en œuvre de la méthode MCCM nécessitent une mise à l’essai minutieuse des hypothèses sur les valeurs a priori et la vérification du comportement d’échantillonnage afin d’évaluer la qualité des estimations qui en découlent. Nous avons compilé une liste de contrôle des diagnostics MCCM et l’avons utilisée pour sélectionner une liste restreinte de modèles géniteurs-recrues ajustés à chaque stock (section \@ref(ModelSelection)).


### Valeurs a priori {#Priors}

Les ajustements bayésiens pour les trois formes de modèle de Ricker (de base, AR1 et avec PVT) exigent des distributions a priori pour le paramètre de productivité $ln(\alpha)$ et le paramètre de capacité $S_{max}$.

Nous avons utilisé des valeurs a priori de la productivité non informatives pour tous les modèles géniteurs-recrues à un seul stock, mises en œuvre comme une distribution normale avec une moyenne de 0 et un très large écart :


\begin{equation} 
  ln.alpha \sim normal(0,100)
  (\#eq:ProductivityPrior)
\end{equation} 

Pour les valeurs a priori de la capacité, nous avons testé des distributions a priori uniformes et log-normales pour $S_{max}$, avec des bornes supérieures soit larges (scalaire = 3), soit plafonnées (scalaire = 1,5) :

\begin{equation} 
  Smax \sim uniform(0,Spn_{ref}*Scalar)
  (\#eq:UniformCapacityPrior)
\end{equation} 

\begin{equation} 
  Smax \sim lognormal(Spn_{ref},CV) [0,Spn_{ref} * Scalar]
  (\#eq:LognormalCapacityPrior)
\end{equation} 

Les valeurs a priori de la capacité informatives fondées sur le rendement photosynthétique observé dans les lacs de croissance peuvent améliorer la précision des estimations de la capacité (distribution a posteriori plus étroite de $S_{max}$) pour les stocks lorsqu’elles sont appropriées compte tenu du cycle biologique, des propriétés du lac, du nombre de stocks qui grandissent dans un lac, ainsi que la plausibilité de l’estimation de Smax fondée sur le rendement photosynthétique [p. ex. @Bodtkeretal2007; @Atlasetal2021HabitatDyn; @Atlasetal2020Limno]. Nous avons utilisé les estimations de la capacité des lacs comme valeurs initiales propres au stock, le cas échéant, et la plus grande abondance observée des géniteurs comme valeur de référence pour les stocks restants.

@SkeenaNassSkDataRep ont compilé les estimations, publiées et inédites, de $S_{max}$ fondé sur le rendement photosynthétique pour 26 lacs de croissance du saumon rouge dans les bassins versants des rivières Skeena et Nass (leur annexe B.4) et nous avons utilisé ces estimations pour 20 des lacs afin de spécifier des valeurs informatives de $Spn_{ref}$ pour 15 stocks (tableau Table \@ref(tab:PRPriorsTable)), selon les considérations suivantes.

* Les valeurs a priori de la capacité informatives utilisant la somme des estimations de $S_{max}$ fondées sur le rendement photosynthétique pour les grands lacs de croissance ne s’appliquent pas aux ajustements du modèle au niveau du regroupement, compte tenu de la combinaison des cycles biologiques et des propriétés des lacs dans les stocks qui le composent.
* Les valeurs a priori de la capacité fondées sur le rendement photosynthétique ne s’appliquent pas au saumon rouge de type fluvial ou océanique, qui ne grandit pas dans un lac.
* Nous n’avons pas utilisé les valeurs a priori de la capacité fondées sur le rendement photosynthétique pour les stocks du lac Babine, en raison (1) de la taille du lac et (2) de la difficulté de répartir les estimations de la capacité du lac entre cinq stocks, y compris les deux stocks mis en valeur au moyen de frayères (ruisseau Pinkut, rivière Fulton).
* Pour les stocks utilisant plusieurs lacs de croissance, nous avons généralement fait la somme des estimations de $S_{max}$ fondées sur le rendement photosynthétique disponibles pour les principaux lacs de fraie (Bear/Azuklotz, Fred Wright/Kwinageese, Swan/Stephens/Club, Sustut/Johansen). Pour le lac Mcdonell, nous n’avons utilisé que l’estimation de la capacité du lac Mcdonell, mais nous avons exclu les lacs Aldrich et Dennis, parce que tous les géniteurs observés dans les relevés grandissent dans le lac Mcdonell. Le stock du lac Slamgeesh comprend les lacs Slamgeesh et Damshilgwit, mais les estimations fondées sur le rendement photosynthétique ne sont disponibles que pour le lac Slamgeesh.
* Pour certains stocks dont les valeurs a priori de la capacité sont fondées sur le rendement photosynthétique, il n’y a pas suffisamment de données pour ajuster les modèles géniteurs-recrues à un seul stock (rivière Bowser).
* Les valeurs a priori de la capacité de certains stocks ont été ajustées d’après un examen des distributions a posteriori à partir des ajustements préliminaires du modèle.

Dans l’ensemble, nous avons testé quatre autres valeurs a priori de la capacité (tableau \@ref(tab:AltCapPriorsTable)) et nous avons utilisé la valeur a priori uniforme plafonnée comme scénario de base pour les ajustements du modèle indiqués dans le présent document.

Lorsque des estimations de la capacité fondées sur le rendement photosynthétique étaient disponibles, nous les avons utilisées pour limiter l’ajustement du modèle géniteurs-recrues, mais dans une valeur a priori uniforme bornée, l’estimation basée sur le lac est moins pondérée que dans une valeur a priori log-normale, à moins que la valeur a priori log-normale soit utilisée avec un coefficient de variation important, auquel cas elle se comporte presque comme une valeur a priori uniforme. Nous avons choisi de réduire ainsi l’information fondée sur les lacs parce que (1) la plupart des estimations disponibles fondées sur le rendement photosynthétique remontent à 20 ans ou plus et (2) un examen uniforme stock par stock des facteurs limitatifs n’a pas été effectué pour les saumons rouges des rivières Skeena et Nass.

Les problèmes potentiels liés à l’utilisation d’estimations de la capacité fondées sur le rendement photosynthétique sont illustrés par le saumon rouge de la rivière Kitwanga : l’estimation de $S_{max}$  fondée sur le rendement photosynthétique de 2003 est de 36 984 géniteurs (tableau \@ref(tab:PRPriorsTable)), mais l’abondance médiane observée des géniteurs depuis 1960 est de 1 258. La plus grande abondance observée des géniteurs était de 20 804 en 2010, et la deuxième en importance était de 13 699 en 2014. Toutes les autres observations étaient inférieures à 6 000 géniteurs. Cet écart peut s’expliquer de plusieurs façons : soit les données sur les géniteurs-recrues sont biaisées à la baisse, soit l’estimation de la capacité est biaisée à la hausse, soit le stock a été gravement appauvri avant 1960, soit la production de la rivière Kitwanga n’est pas limitée par le lac. De plus, les conditions du lac ont probablement changé au cours des 20 années qui ont suivi la production de l’estimation. Dans un examen des objectifs d’échappée axé sur un ou deux stocks, on pourrait explorer et évaluer ces hypothèses de rechange pour déterminer si l’estimation de la capacité fondée sur le rendement photosynthétique est valide. Cependant, cela n’était pas possible ici, compte tenu du nombre de stocks visés par l’analyse actuelle.

\clearpage

(ref:PRPriorsTable) Estimations de $S_{max}$ fondées sur le rendement photosynthétique utilisées pour spécifier les valeurs a priori de la capacité informatives. Le tableau indique l’année du dernier relevé limnologique (*DernierLim*) utilisé pour calculer $S_{max}$ fondé sur le rendement photosynthétique (*Est*). Les intervalles de confiance à 95 % (limites *inférieure* et *supérieure*) reposaient sur un coefficient de variation présumé de 20 % et une distribution normale (Cox-Rogers et Hume comm. pers., MPO 2012). Les estimations pour les lacs de la rivière Skeena proviennent de Cox-Rogers et de Hume (comm. pers., MPO 2012, à partir des ensembles de données tenus par le Cultus Lake Salmon Research Laboratory), y compris les ajustements propres au lac pour les concurrents autres que le saumon rouge (p. ex. les épinoches) et la concurrence des juvéniles. Les estimations pour les lacs de la rivière Nass proviennent @Atlasetal2020Limno, qui ne comprennent pas les ajustements. Cependant, les ajustements seraient probablement minimes pour les lacs de la rivière Nass. Les mises à jour ou les tests de sensibilité des estimations de la capacité $S_{max}$ fondées sur le rendement photosynthétique, élaborées dans les années 1990 et au début des années 2000 (p. ex., l’hypothèse d’un coefficient de variation de 20 %), sortaient de la portée du projet actuel.



```{r PRPriorsTable, echo = FALSE, results = "asis"}


table.in <- read.csv("data/Reference Tables/PRbasedPrior_Info.csv",stringsAsFactors = FALSE, fileEncoding = "UTF-8-BOM") %>% arrange(Basin,Stock,-Smax_Spn) %>% select(-CV, - Cap) %>%
   mutate(Smax_Spn = prettyNum(round(Smax_Spn),big.mark = ",")) %>%
   mutate(Smax_Spn_Lower = prettyNum(round(Smax_Spn_Lower),big.mark = ",")) %>%
   mutate(Smax_Spn_Upper = prettyNum(round(Smax_Spn_Upper),big.mark = ",")) %>%
   #mutate(Cap = prettyNum(round(Cap),big.mark = ",")) %>%
   dplyr::rename(Est = Smax_Spn,Inférieure = Smax_Spn_Lower,Supérieure = Smax_Spn_Upper, DernierLim = LastLim, Lac = Lake ) #%>% 
   #select(-EstCV)
   

table.in$Basin[duplicated(table.in$Basin)] <- ""
table.in$Stock[duplicated(table.in$Stock)] <- ""
table.in[is.na(table.in)] <- ""
table.in[table.in == "NA"] <- ""
   
table.in <- table.in %>% dplyr::rename('Bassin versant' = Basin)   
   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l","r","r","r","r","r"),
                  caption = "(ref:PRPriorsTable)") %>%
add_header_above(c(" " = 4, "PR-based Smax" = 3),bold=TRUE) %>%
        kableExtra::row_spec(c(5), hline_after = TRUE) %>%
       kableExtra::row_spec(c(1,4,6,9:17,20), extra_latex_after = "\\cmidrule(l){2-7}") 

```


\clearpage

(ref:AltCapPriorsTable) Autres valeurs a priori pour le paramètre de capacité $S_{max}$. Les quatre versions ont été mises à l’essai avec l’ajustement du modèle de Ricker de base et les deux versions de la valeur a priori uniforme ont été testées avec l’ajustement du modèle AR1 et du modèle de Ricker avec PVT. La valeur a priori uniforme plafonnée (*CU*) a été retenue comme résultat par défaut dans le présent document.

```{r AltCapPriorsTable, echo = FALSE, results = "asis"}


table.in <- read.csv("data/Reference Tables/AltCapPriors.csv", fileEncoding = "latin1")
   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l","r","r","r","r","r"),
                  caption = "(ref:AltCapPriorsTable)") %>%
        kableExtra::row_spec(2:dim(table.in)[1]-1, hline_after = TRUE) %>%
	kableExtra::column_spec(3, width = "31em") 

```


\clearpage
### Ajustements possibles du modèle géniteurs-recrues pour les regroupements {#CandidateAggModels}

Les considérations clés pour la modélisation des géniteurs-recrues pour les deux regroupements de stocks sont la productivité variable dans le temps et la contribution des stocks mis en valeur par des frayères aux remontes de saumon rouge de la rivière Skeena. Étant donné que de longues séries chronologiques de données continues sur les géniteurs-recrues sont disponibles pour les regroupements des rivières Skeena et Nass, les trois formes possibles du modèle (section \@ref(ModelForms)) peuvent être appliquées, ce qui permet une évaluation explicite des variations de la productivité au fil du temps. Le principal défi pour les ajustements des géniteurs-recrues au niveau du regroupement consiste à déterminer si la méthode d’analyse convient à cette échelle.


*Regroupement de la rivière Nass*

Pour la plupart des séries chronologiques disponibles, le lac Meziadin représente la majeure partie de l’abondance totale des géniteurs. L’ensemble de données sur le regroupement présente un bon contraste dans l’ensemble, mais la première partie de la série chronologique constitue la majeure partie du contraste dans les données. Depuis le milieu des années 1990, le contraste a été beaucoup plus faible (< 4), mais cela est en partie attribuable à l’évolution de la composition des stocks, en particulier à la récente augmentation de l’abondance et de la contribution relative du saumon rouge de type océanique et de type fluvial de la basse Nass. Compte tenu de leur cycle biologique différent, nous considérons qu’il est plus approprié d’ajuster les modèles géniteurs-recrues séparément à ces deux stocks principaux, mais nous avons inclus les ajustements du modèle pour le regroupement aux fins de comparaison.


*Regroupement de la rivière Skeena*


Le contraste limité des données sur les géniteurs et la diffusion bruyante des points de données créent une grande incertitude dans les ajustements du modèle géniteurs-recrues, car une grande proportion de regroupement provient des installations du PMVLB. Il n’est pas possible d’ajuster une relation géniteurs-recrues dépendante de la densité aux données obtenues, car l’ajustement est très sensible à de légers changements dans les choix de traitement des données (p. ex. inclure ou exclure quelques années d’éclosion anciennes ou récentes; figure \@ref(fig:AltFitPlotSkeena)). On peut utiliser des valeurs a priori bayésiennes pour forcer le modèle à s’ajuster à une productivité ou à une capacité particulière jugée plausible, mais nous avons choisi ici d’exclure les stocks mis en valeur et d’ajuster les modèles géniteurs-recrues à la composante sauvage du regroupement (section \@ref(DataSources)). La section \@ref(ChannelReview) donne un aperçu des renseignements disponibles sur la production issue de la mise en valeur.


\clearpage
(ref:AltFitPlotSkeena) Ajustements du modèle de Ricker déterministe simple pour l’ensemble du regroupement de la rivière Skeena, y compris le ruisseau Pinkut et la rivière Fulton mis en valeur, à l’aide de toutes les années d’éclosion disponibles par rapport à divers sous-ensembles des données. L’ajustement de régression varie considérablement et inverse même le sens, selon que l’on inclut les années d’éclosion 1994 et 2013 dans l’analyse. Les points pleins sont les données utilisées pour l’ajustement du modèle. Les cercles ouverts sont les observations exclues. Les ajustements pourraient être plus stables si on pouvait intégrer des renseignements supplémentaires, comme les covariables environnementales (p. ex. les conditions océaniques pendant la dévalaison des smolts) ou la covariation de la productivité entre les réseaux hydrographiques. Cependant, cette simple illustration montre que les données sur les géniteurs-recrues en elles-mêmes renseignent peu sur la relation dépendante de la densité entre les géniteurs et la productivité. 

```{r AltFitPlotSkeena,   fig.cap="(ref:AltFitPlotSkeena)"}
include_graphics("data/1_FrenchFigs/Fig3_SkeenaScatterVariations_French.PNG")
```





\clearpage
### Ajustements possibles du modèle géniteurs-recrues à un seul stock  {#CandidateStockModels}

Les principales considérations pour la modélisation des géniteurs-recrues pour chacun des stocks de saumon rouge des rivières Skeena et Nass sont les caractéristiques du stock, les données accessibles et les changements observés au fil du temps (p. ex. la qualité des données, la productivité). Nous avons préparé une liste de contrôle des facteurs à prendre en considération pour déterminer un ensemble initial de variations possibles du modèle géniteurs-recrues pour chaque stock (figure \@ref(fig:CandidateModels)).


Le nombre minimal de points de données sur les géniteurs-recrues requis pour l’ajustement du modèle a été discuté pendant le processus d’examen par les pairs. S’il y a relativement peu de points de données sur les géniteurs-recrues, les estimations des paramètres et les points de référence biologiques qui en découlent peuvent être très incertains et systématiquement biaisés, en particulier lorsque l’erreur d’observation est relativement élevée et qu’il y a une forte corrélation d’une année à l’autre dans la survie. Un consensus s’est dégagé sur le fait que le seuil devrait être d’au moins 10, parce que les participants ont constaté que les ajustements du modèle géniteurs-recrues à moins de 10 observations sont vulnérables à de graves biais dans les estimations des paramètres et les points de référence qui en découlent. Les participants ont également envisagé un seuil plus élevé fondé sur des travaux inédits de Brendan Connors (comm. pers., MPO 2022, documentés dans un [référentiel GitHub](https://github.com/brendanmichaelconnors/PSE-pop-SAC/blob/master/How-many-data-points/2020-07-30_How-many-SR-pairs-are-too_few.pdf)), qui a exploré la quantité de biais dans les estimations de Srmd pour divers nombres de points de données inclus dans l’analyse et qui a constaté qu’au moins 13 années de paires de données sur le stock-recrutement sont nécessaires, en général, pour obtenir des estimations de Srmd non biaisées. Le plus préoccupant était le fait que moins de 13 points produisent généralement des sous-estimations de Srmd, le biais le plus important étant produit par les populations les moins productives. Ce biais dans Srmd était généralement plus faible dans une analyse du MBH que dans les analyses d’une seule UC.


Nous avons maintenu le seuil d’au moins 10 points de données dans la liste de contrôle de la figure \@ref(fig:CandidateModels), mais le seuil plus élevé d’au moins 13 points de données n’aurait aucune incidence sur nos analyses (tableau \@ref(tab:DataOverviewSkeena)). Le stock de type fluvial de la haute Skeena avec 4 années de données d’éclosion est exclu de toute façon, et tous les autres stocks ont plus de 13 années de données d’éclosion. Les stocks des lacs Slamgeesh et Johnston ont 14 années de données d’éclosion jusqu’à la remonte de 2019 et en auront 16 dès que la prochaine mise à jour de la reconstitution des remontes (jusqu’à l’année de remonte 2022) sera mise en œuvre.

Les ajustements du modèle géniteurs-recrues n’ont été appliqués qu’aux stocks *sauvages* ayant au moins 10 années de données d’éclosion sur les géniteurs-recrues.

* Pour huit stocks dont la série de données présente des trous (après le filtrage et le remplissage, section \@ref(AvailableSRData)), seul le modèle de Ricker de base a été ajusté.
* Pour 12 stocks ayant au moins 25 années d’éclosion continues de données sur les géniteurs-recrues, les trois formes du modèle ont été ajustées (de base, AR1, avec PVT).
* Les deux stocks mis en valeur (ruisseau Pinkut et rivière Fulton) ont été exclus en raison de problèmes d’ajustement, comme l’illustre la figure \@ref(fig:AltFitPlotSkeena) pour le regroupement de la rivière Skeena.
* Les huit stocks dont les données sont insuffisantes ont également été exclus. Il convient de noter que pour l’un des stocks qui sont ici considérés comme ayant des données insuffisantes, le stock du lac Bowser (Nass), les discussions se poursuivent au sujet de l’utilisabilité des estimations disponibles, et il pourrait être inclus dans les mises à jour futures de cette analyse.

\clearpage

(ref:CandidateModels) Liste de contrôle pour la détermination d’un ensemble de base de modèles possibles pour les ajustements du modèle géniteurs-recrues à un seul stock. Nous nous sommes concentrés sur les variations du modèle de Ricker pour les stocks sauvages ayant au moins 10 années de données d’éclosion sur les géniteurs-recrues après avoir rempli les trous d’un an dans l’abondance des géniteurs ou la remonte. Pour les stocks ayant au moins 25 années de données d’éclosion continues sur les géniteurs-recrues, nous avons testé trois autres formes du modèle. Pour les stocks qui ne répondent pas à cette exigence, nous n’avons ajusté qu’un modèle de Ricker de base.


```{r CandidateModels,  fig.cap="(ref:CandidateModels)"}
include_graphics("data/1_FrenchFigs/Fig4_Diagram_CandidateModels_REV_French.PNG")
```

\clearpage


Nous avons effectué deux séries de tests de sensibilité pour l’ajustement du modèle de Ricker de base.

* *Données complètes ou tronquées* : Comparer les ajustements en utilisant toutes les données accessibles aux ajustements comportant des données tronquées, à l’exclusion des premières années d’éclosion. Le seuil pour les données tronquées différait selon le stock, mais nous avons généralement utilisé le milieu à la fin des années 1990. Par exemple, les données sur les géniteurs-recrues pour le lac Alastair remontent à 1960, mais le modèle tronqué n’utilise que les années d’éclosion à partir de 1998. Il convient de noter que pour le lac Kitsumkalum, nous avons utilisé les données sur les géniteurs-recrues tronquées en 1990 comme scénario de référence et toutes les années de données dans un essai de sensibilité, en raison des changements radicaux observés dans la dynamique de la production depuis la construction d’une frayère artificielle à la fin des années 1980. Il faut noter que la frayère de Kitsumkalum diffère des frayères du ruisseau Pinkut et de la rivière Fulton parce que les abondances de géniteurs ne sont pas gérées activement en fonction d’une cible, et les données recueillies depuis 1990 montrent clairement un profil dépendant de la densité.
* *Autres valeurs a priori de la capacité* : Comparer les estimations des points de référence à l’aide de quatre autres valeurs a priori de la capacité, soit uniforme plafonnée, uniforme large, log-normale plafonnée et log-normale large (section \@ref(Priors)). Le cas échéant, nous avons utilisé des estimations de la capacité fondées sur le rendement photosynthétique du lac pour limiter les valeurs a priori de la capacité.




### Exploration d’un ajustement du modèle géniteurs-recrues hiérarchique pour les stocks de saumon rouge de la rivière Skeena  {#HBMExploration}


Dans le cadre du processus du GTT, McAllister et Challenger (annexe \@ref(app:HBMFits)) ont mis à jour l’approche d’ajustement du MBH pour les stocks de saumon rouge de la rivière Skeena d’après @KormanEnglish2013 afin de comparer les estimations précédentes générées selon la même méthodologie, mais avec un ensemble de données sur les géniteurs-recrues actualisé. L’un des avantages des MBH est que l’information peut être partagée entre les stocks, en exploitant les similitudes des données accessibles pour extraire les profils sous-jacents communs (p. ex. productivité intrinsèque semblable entre les stocks ayant des cycles biologiques semblables, profils communs dans l’évolution de la productivité), ce qui peut améliorer la précision des estimations pour les stocks ayant des données bruyantes ou manquantes.

On trouvera à l’annexe D des détails sur les méthodes des MBH, les ajustements des modèles et les résultats. En bref, l’approche consiste à modéliser la productivité au niveau du stock à l’aide de deux composantes : (1) une distribution sous-jacente commune avec une tendance centrale partagée entre les stocks (appelée *hyperparamètre*) et (2) un écart pour chaque stock par rapport à cette distribution partagée. Dans le cas des stocks pour lesquels les données sont très informatives, les estimations de la productivité qui en résultent peuvent s’éloigner davantage du paramètre commun de productivité. Dans le cas des stocks pour lesquels les données sont bruyantes ou manquantes, l’estimation des paramètres sera tirée plus fortement vers le centre général de la distribution pour le groupe de stocks. Ce rétrécissement se produit pour tous les stocks dont les paramètres de productivité diffèrent de la productivité moyenne du groupe de stocks inclus dans le MBH, mais le niveau de rétrécissement varie selon le stock en fonction de la force du signal dans les données (section \@ref(HBMShrinkage)).

Les défis connus pour les ajustements bayésiens hiérarchiques sont les suivants.

* *Complexité du modèle* * : De nombreux paramètres sont estimés simultanément. Les estimations des paramètres peuvent être très sensibles à d’autres réglages et des interactions inattendues pourraient fausser les résultats. Bien que ce soit le cas pour tous les ajustements de modèles bayésiens, le problème potentiel augmente avec le nombre de paramètres.
* *Similitudes présumées entre les stocks* * : Dans sa forme la plus simple, la mise en œuvre d’un MBH estime la productivité de tous les stocks constituants par rapport à un seul hyperparamètre sous-jacent, mais des structures de stock plus nuancées peuvent être intégrées (p. ex. les stocks du groupe doivent correspondre à la structure spatiale du bassin versant). Étant donné que l’information est échangée entre les stocks, il est important de tenir compte des cycles biologiques et des profils observés de la productivité des stocks reliés dans une structure de modèle hiérarchique.

En plus de fournir une comparaison avec les estimations qui ont déjà été élaborées à l’aide d’un cadre de modélisation semblable pour le saumon rouge de la rivière Skeena, les résultats mis à jour du modèle MBH confirment le profil général des déclins de la productivité du saumon rouge de la rivière Skeena à l’échelle du bassin versant dans la forme de la courbe de l’effet annuel commun, et les résultats du MBH présentés à l’annexe \@ref(app:HBMFits) appuient également d’autres objectifs du présent document de recherche, notamment :

* contribuer à une vérification croisée entièrement indépendante des estimations des paramètres pour un seul stock pour les stocks de saumon rouge de la rivière Skeena (objectif 3);
* donner l’occasion d’explorer les sources des différences observées (c.-à-d. forme du modèle, hypothèses sur les valeurs a priori; objectif 6).


À l’appui de ces objectifs, le MBH a été mis en œuvre à l’aide des mêmes ensembles de données et a intégré certains tests de sensibilité conçus pour être semblables à la mise en œuvre pour un stock unique. L’intention était que les différences observées dans les résultats soient principalement attribuables à la structure hiérarchique, mais il était difficile d’isoler clairement l’effet de l’hypothèse hiérarchique des autres nuances méthodologiques pour les stocks lorsque des différences entre les extrants du modèle à un seul stock et du modèle MBH ont été observées.

## SSCÉNARIOS DE SÉLECTION DU MODÈLE GÉNITEURS-RECRUES À UN SEUL STOCK ET DE PRODUCTIVITÉ {#ModelSelection}

Nous avons ajusté un nombre total de 163 modèles possibles, en raison des autres formes du modèle (de base, AR1 et avec PVT), des tests de sensibilité (c.-à-d. les autres valeurs a priori, les séries chronologiques complètes et tronquées) et du grand nombre de stocks (20 stocks sauvages, 2 stocks mis en valeur, 3 versions de l’ajustement pour le regroupement). Afin d’améliorer l’uniformité, nous avons élaboré des lignes directrices pour sélectionner d’abord une courte liste d’ajustements du modèle pour chaque stock ou regroupement, puis pour élaborer d’autres scénarios de productivité fondés sur les ajustements présélectionnés du modèle (figure \@ref(fig:ModelSelection)).


Étant donné que *« tous les modèles sont erronés, mais certains sont utiles »* [@BoxModelsWrong], l’approche des ajustements présélectionnés du modèle doit être adaptée à leur but. Par exemple, dans les analyses des géniteurs-recrues appliquées pour les mêmes stocks de saumon rouge du fleuve Fraser, à l’aide des mêmes données, le processus annuel de prévision [p. ex. @Grantetal2013FC] utilise un ensemble différent de modèles possibles et une approche différente de sélection du modèle par rapport à la simulation utilisée pour une évaluation du potentiel de rétablissement [@Huangetal2021FraserSkRPA]. Les deux approches combinent des critères quantitatifs pour le choix du modèle (p. ex. convergence MCCM, erreur moyenne en pourcentage absolu d’un test rétrospectif) et un jugement d’expert sur la plausibilité et l’utilité des ajustements possibles du modèle géniteurs-recrues.

Pour l’examen des objectifs d’échappée du saumon rouge des rivières Skeena et Nass, le GTT et les examinateurs indépendants ont déterminé que les variations de la productivité dans le temps, ainsi que les différences de productivité entre les stocks, étaient les principales priorités pour l’analyse (section \@ref(PaperObj)). Par conséquent, nous avons formulé la question fondamentale pour le choix du modèle comme suit : « Parmi les ajustements du modèle géniteurs-recrues qui convergeaient sur les estimations des paramètres biologiquement plausibles, lesquels sont utiles pour décrire d’autres scénarios de productivité qui sont pertinents pour les processus décisionnels subséquents », où nous définissons : « utile » comme *« qui aide à démontrer l’ampleur des variations des points de référence biologiques et des analyses subséquentes découlant de différentes hypothèses de productivité »*. Cela met l’accent sur le contraste entre les scénarios de productivité, et il s’agit d’une approche très différente de la recherche du modèle unique avec le « meilleur » ajustement. Ces scénarios de productivité ne sont pas des prédictions ou des recommandations du meilleur ajustement du modèle en soi. Des processus décisionnels à venir devront déterminer les scénarios qu’ils jugent plausibles, puis évaluer les conséquences pour les éléments de base précis sur lesquels ils choisissent de se concentrer (p. ex. évaluations de la situation ou profils d’équilibre ou simulations de la stratégie de récolte).

Nous avons utilisé trois étapes pour dresser une liste restreinte des modèles possibles pour chaque stock (figure \@ref(fig:ModelSelection)).

1.	*Considérations d’ordre statistique* : Les modèles qui sont très mal ajustés ou qui ne convergent pas, d’après les critères énumérés dans le tableau Table \@ref(tab:MCMCDiagnostics), ont été écartés.
2. *Considérations relatives à la capacité* : Nous avons comparé les estimations de la capacité entre les ajustements du modèle pour éliminer celles qui étaient considérées comme très invraisemblables. Si la solution de rechange plausible restante était sensiblement différente, nous avons examiné si la différence s’expliquait le plus probablement par la forme du modèle, le choix de la valeur a priori de la capacité informative/non informative, la troncature des données ou la dispersion des points de données, et avons fait des choix propres à chaque cas. Dans la mesure du possible, nous avons généralement choisi les ajustements du modèle avec des valeurs a priori de la capacité uniformes, plafonnées en fonction du rendement photosynthétique du lac.
3. *Considérations relatives à la productivité* : Nous avons comparé les estimations de la productivité entre les ajustements du modèle pour un stock et entre les stocks, afin d’éliminer celles qui étaient considérées comme invraisemblables. Lorsqu’il était possible d’ajuster les modèles AR1 et avec PVT, nous avons comparé les estimations de paramètres variables dans le temps aux estimations du modèle de Ricker de base et avons fait des choix propres à chaque cas. Nous avons généralement choisi les ajustements du modèle en utilisant toutes les données accessibles, à moins qu’il y ait des problèmes clairs avec les données. Lorsque les données et les ajustements du modèle indiquaient des changements récents de la dynamique des populations, nous avons généralement choisi les ajustements du modèle AR1 ou avec PVT plutôt que du modèle de Ricker de base, et nous utilisons toutes les données accessibles plutôt que les données tronquées dans les ajustements.

Les lignes directrices générales suivantes ont été appliquées pour générer d’autres scénarios de productivité fondés sur le sous-échantillonnage des distributions a posteriori à partir des ajustements des modèles présélectionnés (figure \@ref(fig:ModelSelection)).

* Pour décrire la productivité moyenne à long terme, nous avons tiré un échantillon de l’ajustement du modèle AR1, le cas échéant, et de l’ajustement du modèle de Ricker de base. L’utilisation de l’ajustement du modèle avec PVT nécessiterait un calcul de la moyenne ou un sous-échantillonnage pour toutes les années d’éclosion, et nous avons donc jugé qu’il était plus approprié d’utiliser simplement les estimations des paramètres AR1, si les modèles AR1 et avec PVT étaient tous deux disponibles.
* Pour décrire la productivité récente et les extrémités de la productivité élevée/faible, nous avons tiré un échantillon de l’ajustement du modèle avec PVT lorsqu’il était disponible, et du modèle de Ricker de base sinon.
   * Lorsqu’un ajustement du modèle avec PVT était disponible, nous avons tiré notre sous-échantillon des échantillons annuels ln.alpha pour une génération complète, en utilisant la génération la plus récente pour le scénario de productivité récente, et la génération centrée sur la productivité la plus faible/la plus élevée pour les extrémités. À titre de test de sensibilité, nous avons également généré deux versions de rechange du scénario de productivité récente, en utilisant les deux ou trois dernières générations complètes (8 et 12 années d’éclosion pour un stock dont les remontes sont principalement composées de poissons d’âge 4).
   * Lorsque seul un ajustement du modèle de Ricker de base était disponible, nous avons vérifié le profil des résiduels de Ricker et déterminé un ajustement approximatif de la productivité sous la forme d’un centile de la valeur a posteriori. Nous avons ensuite sélectionné la moitié de l’échantillon en dessus et en dessous de ce centile pour générer un scénario récent. Pour les extrémités élevée et faible, nous avons choisi le sous-échantillon de façon à ce que la médiane de ln.alpha corresponde aux 10e et 90e centiles de la distribution a posteriori originale.
   
\clearpage
(ref:ModelSelection) Considérations pour la sélection du modèle et lignes directrices pour la production de scénarios de productivité. 

```{r ModelSelection,   fig.cap="(ref:ModelSelection)", out.width = "90%"}
include_graphics("data/1_FrenchFigs/Fig5_Diagram_ModelSelection_French.PNG")
```



\clearpage

(ref:MCMCDiagnostics) Liste de contrôle des diagnostics MCCM. Les diagnostics normalisés suivants ont été utilisés pour évaluer l’échantillonnage MCCM et l’ajustement du modèle. Tableau adapté et étendu de  @PestalJohnstonTakuCo. 

```{r MCMCDiagnostics, echo = FALSE, results = "asis"}


table.in <- read.csv("data/Reference Tables/MCMC_Diagnostics.csv",
                     stringsAsFactors = FALSE, fileEncoding = "UTF-8-BOM")

table.in$Examples <- gsub("None","-",table.in$Examples)

   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l","l"),
                  caption = "(ref:MCMCDiagnostics)") %>%
    kableExtra::column_spec(1, width = "7em") %>%
       kableExtra::column_spec(2, width = "10em") %>%
       kableExtra::column_spec(3, width = "17em") %>%
       kableExtra::column_spec(4, width = "5em") %>%
     kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) 

```




\clearpage
## POINTS DE RÉFÉRENCE BIOLOGIQUES ET POINTS DE RÉFÉRENCE DE L’ÉTAT {#BMMethods}

Nous avons calculé quatre points de référence biologiques standard et deux points de référence de l’état connexes de la PSS (tableau \@ref(tab:BMDefs)). Les estimations provisoires des points de référence ont été examinées dans le cadre du processus du GTT afin de repérer les erreurs et les anomalies potentielles. Au cours de la première étape du processus d’examen du GTT, les valeurs de Sgen ont été signalées comme étant trop faibles pour plusieurs des stocks. Cela a déclenché un examen détaillé et la mise à l’essai de toutes les étapes du calcul des points de référence.

Smax et Seq peuvent être calculés directement à partir des paramètres géniteurs-recrues. Srmd et Sgen ont besoin d’une solution plus complexe, et nous avons testé quatre solutions de rechange pour chacun. D’après les tests résumés à l’annexe E.3, nous avons décidé d’utiliser (1) la méthode de @Scheuerell2016 pour Srmd, parce que c’est la seule solution exacte, et (2) la version de @Connorsetal2022 de l’optimisateur de Sgen, car c’est la seule approche d’optimisation qui a généré des solutions pour toutes les combinaisons de paramètres testées (tableau \@ref(tab:BMCalcs). L’annexe \@ref(BMFuns) indique le code R correspondant.

Certaines analyses antérieures des objectifs d’échappée ont utilisé une correction du biais log-normal sur le paramètre de la productivité (tableau \@ref(tab:BMCalcs)), mais la mise en œuvre a varié entre les organismes, les régions et les projets. Les évaluations de l’état selon la PSS ont utilisé des points de référence sans correction du biais [@FrSkWSPStatus2012; @FrSkWSPStatus2017; @SBCCkWSPStatus2012SAR;@IFCohoWSPStatus2013SAR]. Les analyses des objectifs d’échappée de l’Alaska incluaient habituellement la correction du biais [@EggersBernard2011Alsek; @FleishmanEvenson2010; @McPhersonetal2010; @Fairetal2011]. Les analyses des objectifs d’échappée pour les stocks de saumon transfrontaliers du Nord incluaient les deux versions il y a quelques années [p. ex. @PestalJohnstonTakuCo], mais elles ne déclarent récemment que la version avec correction du biais [@Connorsetal2022; @MillerPestalTakuSk].

Cela n’est pas propre au saumon du Pacifique. Dans leur examen de la modélisation stock-recrutement, @Subbeyetal2014SRReview notent que les deux versions ont été largement utilisées et que le choix pour une application particulière devrait tenir compte de la façon dont les estimations seront utilisées par la suite. Les lignes directrices générales sont les suivantes :

* utiliser des valeurs *avec* correction du biais lorsque l’objectif de gestion est défini en termes de valeurs moyennes (p. ex. Srmd moyen);
* utiliser des valeurs *sans* correction du biais lorsque l’objectif de gestion est défini en termes de valeurs médianes (p. ex. Srmd médian) ou lorsque les estimations des paramètres sont utilisées comme données d’entrée dans d’autres modèles (p. ex. simulation prospective).

Les essais systématiques des données sur les géniteurs-recrues pour les saumons rouges des rivières Skeena et Nass (annexe \@ref(BiasCorrtest)) ont démontré que l’effet de la correction du biais est généralement faible pour les stocks productifs avec de bons ajustements du modèle géniteurs-recrues (c.-à-d. que sigma est petit par rapport à ln.alpha), mais peut être important pour les stocks dont la productivité est faible et dont les ajustements du modèle géniteurs-recrues sont mauvais (c.-à-d. que sigma est plus grand par rapport à ln.alpha). La correction du biais augmente généralement les estimations de Srmd et abaisse celles de Sgen.

Compte tenu de ces effets observés et des différences d’approche ces dernières années, nous avons choisi de déclarer les médianes et les centiles de l’estimation du paramètre a posteriori sans correction du biais log-normal dans tout ce document, mais avons inclus la version avec correction du biais à l’annexe \@ref(BiasCorrectedBM).


 


\clearpage
(ref:BMDefs) Définition des points de référence biologiques standard et des points de référence de l’état de la PSS pour le paramètre de l’abondance relative. Il convient de noter que nous définissons les points de référence en termes de médiane des recrues et de médiane du rendement et que, par conséquent, nous présentons les estimations des points de référence sans correction du biais log-normal dans la majorité du document. Les estimations moyennes avec correction du biais des points de référence biologiques sont incluses à \@ref(BiasCorrectedBM).


```{r BMDefs, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Reference Tables/BM_Definitions.csv")

table.in$Type[duplicated(table.in$Type)] <- ""


table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:BMDefs)")  %>%
	kableExtra::row_spec(4, hline_after = TRUE) %>%
   #  kableExtra::column_spec(1, width = "8em") %>%
   #     kableExtra::column_spec(2, width = "8em") %>%
        kableExtra::column_spec(3, width = "30em") %>%
   kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```


(ref:BMCalcs) Méthode de calcul des points de référence biologiques.

```{r BMCalcs, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Reference Tables/BM_Calc_Equations.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:BMCalcs)")  #%>%
	#kableExtra::row_spec(4, hline_after = TRUE) %>%
     #kableExtra::column_spec(1, width = "8em") %>%
       # kableExtra::column_spec(2, width = "8em") %>%
        #kableExtra::column_spec(3, width = "30em") %>%
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



(ref:BiasCorrCalcs) Correction du biais log-normal pour le paramètre de productivité par forme du modèle.

```{r BiasCorrCalcs, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Reference Tables/Bias_Corr_Equations.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:BiasCorrCalcs)") %>%
	kableExtra::row_spec(1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "12em")  %>%
       kableExtra::column_spec(2, width = "12em") %>%
        kableExtra::column_spec(3, width = "20em")# %>%
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```







\clearpage

## APPROCHES DE RECHANGE POUR L’ÉLABORATION DE POINTS DE RÉFÉRENCE DE GESTION POUR LES REGROUPEMENTS DE STOCKS {#AltApproachesComp}

### Overview of Alternative Approaches

Le présent document et le code R sous-jacent ont été structurés de manière à séparer clairement les étapes (1) des analyses biologiques pour ajuster les modèles géniteurs-recrues et générer d’autres scénarios de productivité et (2) de l’utilisation des ensembles de paramètres géniteurs-recrues pour élaborer des points de référence de gestion pour les regroupements des stocks de saumon rouge des rivières Skeena et Nass. Ces étapes sont fondamentalement différentes en ce qui concerne l’information et le processus dont elles ont besoin. Le fait de garder les analyses modulaires nous a permis d’établir un cadre pour les mises à jour futures et les processus collaboratifs.

Nous donnons des exemples pratiques de huit autres approches de regroupement pour la deuxième étape, ainsi qu’une justification des raisons pour lesquelles ces exemples sont inclus dans le présent document à la section \@ref(AnalysisOverview). Ces exemples utilisent les ajustements géniteurs-recrues et les scénarios de productivité précis décrits ci-dessus, mais peuvent être rapidement régénérés avec d’autres ensembles de paramètres (p. ex. si les participants à un atelier de planification proposent un scénario de productivité différent ou fournissent d’autres ensembles de paramètres géniteurs-recrues fondés sur d’autres analyses).

Le tableau Table \@ref(tab:TableAltApproaches)récapitule les autres approches et définit les étiquettes abrégées que nous utilisons dans le reste du document. Les approches sont présentées par ordre croissant de complexité, qui peut être attribuable à des exigences analytiques, à des exigences de processus ou aux deux. Les approches les plus simples utilisent directement les estimations de points de référence biologiques comme Srmd ou Urmd (Regr. Srmd, Somme Srmd, Comp. Urmd). Viennent ensuite les approches qui peuvent être calculées directement à partir des paramètres géniteurs-recrues en utilisant des hypothèses d’équilibre à long terme (Prof. équ., Compr. regr.). Les approches de regroupement qui tiennent explicitement compte de l’état du stock (État, Rég. log.) sont simples sur le plan informatique, mais nécessitent un processus collaboratif pour s’entendre sur les critères d’état. La simulation prospective (Sim) est l’approche la plus complexe, parce qu’en plus de l’ajustement géniteurs-recrues, de nombreuses itérations de détermination de la portée, de prototypage et d’examen doivent être menées dans le cadre d’un processus collaboratif. Pour donner des exemples pratiques pour chaque approche, nous avons supposé des objectifs quantitatifs et utilisé des exemples conformes à des travaux antérieurs (tableau \@ref(tab:TableAltApprObj)).


Six des huit approches de rechange ont déjà été utilisées pour des analyses du saumon rouge de la rivière Skeena ou de la rivière Nass (tableau \@ref(tab:PastWorkTable)):

* *Regr. Srmd* : Les objectifs actuels d’échappée pour les saumons rouges des rivières Skeena et Nass sont fondés sur les estimations de Srmd pour le regroupement établies en 1958 pour la rivière Skeena, avant la mise en place des frayères artificielles de la rivière Babine, et dans les années 1990 pour la rivière Nass. 
* *Somme Srmd* : En 2016, le Skeena First Nations Technical Committee a recommandé de relever de 400 000 à 600 000 géniteurs le point de référence limite pour le regroupement de saumon rouge de la rivière Skeena en fonction de la somme des estimations de Srmd au niveau du stock et de la composition observée des stocks [@NCIFMP2019].
* *Comp. Urmd*: @Waltersetal2008ISRP ont inclus une comparaison des estimations par stock de Fmax, les taux d’exploitation maximaux qui peuvent être appliqués de façon durable sans causer de disparition (leur figure 14). L’exemple pratique que nous donnons ici compare les estimations par stock d’un point de référence des taux d’exploitation.
* *Prof. équ.* : Ces données n’ont pas été publiées auparavant pour le saumon rouge des rivières Skeena ou Nass, mais elles constituent un extrant standard pour les examens des objectifs d’échappée effectués pour les stocks transfrontaliers du Nord [p. ex. @MillerPestalTakuSk].
* *Agg Tradeoff*: C’était un résultat clé dans @Waltersetal2008ISRP, qui a amené des changements à l’approche canadienne de gestion des prises intérieures.
* *État* : @KormanEnglish2013 et la @PacificSalmonExplorer (2021) ont inclus des évaluations synoptiques, ou « grossières », de l’état selon une approche générale et uniforme fondée sur un seul paramètre d’état. Bien que cette approche ne couvre pas toutes les considérations prises en compte dans les évaluations d’état intégrées et à plusieurs critères réalisées dans le cadre de la PSS [@FrSkWSPStatus2012; @FrSkWSPStatus2017; @SBCCkWSPStatus2012SAR; @IFCohoWSPStatus2013SAR], elle utilise les mêmes points de référence pour le paramètre d’abondance relative (Sgen, 80 % de Srmd) et donne des résultats comparables pour les UC pour lesquelles les évaluations intégrées de l’état étaient effectuées en fonction de ce paramètre.
* *Rég. log.* : Il s’agit de l’une des deux approches possibles décrites pour l’élaboration des points de référence limites (PRL) pour les zones de gestion des stocks en vertu de la Loi sur les pêches modernisée (2019). Le @LRPGuidelinesSAR résume trois études de cas et conclut que « Les PRL de régression logistique » *ont plusieurs limites et ne doivent être utilisés que lorsque (i) des PRL supplémentaires fondés sur l’abondance agrégée sont nécessaires et (ii) toutes les hypothèses du modèle de régression logistique peuvent être respectées* ». Nous avons inclus un exemple concret pour cette méthode afin de vérifier si les défis cernés par le @LRPGuidelinesSAR  se posent pour les saumons rouges des rivières Skeena et Nass.
* *Sim.* : @CoxRogersetal2010 ont testé l’effet de différents taux de récolte (en boucle ouverte) sur 15 ans (simulation courte) et 100 ans (simulation longue). @Hawkshaw2018Diss a utilisé des techniques d’optimisation (simulation longue) pour comparer d’autres types de stratégies de récolte (en boucle ouverte et fermée). Les taux de récolte indiqués dans @CoxRogersetal2010 ont été appliqués également à tous les stocks. @Hawkshaw2018Diss a exploré d’autres règles de contrôle des prises et d’autres plans de pêche pour la pêche plurispécifique de stocks mixtes (cinq espèces de saumon du Pacifique et la truite arc-en-ciel anadrome, chacune modélisée comme un stock unique).






\clearpage
(ref:TableAltApproaches) Approches de rechange afin d’élaborer des points de référence biologiques pour les regroupements. 

```{r TableAltApproaches, echo = FALSE, results = "asis"}


table.in <- read_csv("data/AggregationApproachTables/AggregationTable_AltApproaches.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableAltApproaches)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "8em") %>%
     kableExtra::column_spec(2, width = "6em") %>%
     kableExtra::column_spec(3, width = "32em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```


\clearpage

(ref:TableAltApprObj) Objectifs utilisés dans les exemples pratiques pour chaque approche de regroupement. 

```{r TableAltApprObj, echo = FALSE, results = "asis"}


table.in <- read_csv("data/AggregationApproachTables/AggregationTable_ExampleObjectives.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableAltApprObj)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "8em") %>%
     kableExtra::column_spec(2, width = "38em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage
### Evaluation of Alternative Approaches

There are advantages and disadvantages for each of the different approaches for developing management reference points described here. For example, calculating Smsy for aggregate stocks using the full time series of available data is the most computationally straightforward method to produce a single estimate and comparatively simple to implement in a management framework, but may not meet conservation objective for smaller, less productive stocks within each aggregate. A status-based approach, which may better address WSP or other Canadian legislative requirements, does not provide explicit target abundances as reference points and may not meet requirements for developing international harvest sharing agreements. Although simulation modeling is the most computationally intense approach, it may better address considerations about variability in future productivity than the sum of lower benchmarks developed assuming average long-term productivity for the different stocks.  

The initial version of this Research Document did not make a clear recommendation for which approach should be used to inform aggregate escapement goals for Skeena and Nass Sockeye. Likewise, the CSAS review committee did not reach a consensus during the Regional Peer Review. A subgroup of participants was convened in a follow-up process to develop this advice, which included (1) identifying criteria for evaluating the alternative approaches, (2) completing a detailed evaluation of each approach, and (3) generating a summary table of comparisons, along with an overview of practical challenges for the alternative approaches. This structured comparison of approaches is a key product of the peer review process.

11 evaluation criteria were identified and grouped into three types (Table \@ref(tab:TableCriteria)). Five of the criteria relate to parameter estimation, four relate to the type of outcome the approach produces, and two reflect practical considerations for implementation. In a virtual workshop, we scored each approach against all 11 criteria (Table \@ref(tab:TableSummary)) and drafted a brief rationale for each score (Appendix \@ref(AggregationAppendix)). Key challenges for each approach were also identified (Table \@ref(tab:TableChallenges)).

Appropriate aggregation approaches can be selected depending on which criteria are identified as critical for a specific application. For example:

* If abundance-based aggregate escapement goals that consider stock level diversity are required, then the only approaches that meet these criteria are aggregate equilibrium tradeoff plots, logistic regression, and a forward simulation approach. 
* Logistic regression is not  appropriate for Nass stocks, because past aggregate abundance was found to be not correlated with stock-level performance measures. 
* This leaves the aggregate equilibrium tradeoff plots and forward simulation as the only viable options within this example. 
* Of these, closed loop forward simulation within a Management Strategy Evaluation (MSE) framework is the only aggregation approach identified that meets all of the criteria identified by CSAS review committee, while the aggregate equilibrium trade-off approach can be implemented within a relatively short time frame.

The aggregate equilibrium trade-off approach was recommended for evaluating alternative goals and harvest management rules for Skeena Sockeye in the report prepared by the 2008 Skeena Independent Science Review Panel (ISRP) (Walters et al. 2008). At the time, the ISRP report and preliminary trade-off analyses led to changes in the harvest rule for Canadian marine commercial fisheries for Skeena Sockeye implemented in 2009, which substantially reduced the harvest rate in these fisheries.

A full simulation model and associated MSE framework and process would require a considerable investment of time to develop (1) agreed-upon objectives, (2) agreed-upon model scope, and (3) agreed-upon scenarios for testing through a structured process. Depending on the available time to select an escapement goal, evaluating aggregate tradeoff plots may be the best option for developing an aggregate escapement goal in the short term. If a full MSE is not feasible within the available time frame, a simplified forward simulation can also be used to provide a complementary set of results for aggregate tradeoff considerations in a relatively short period of time.



\clearpage

(ref:TableCriteria) Description of criteria for evaluating the alternative approaches described in Table \@ref(tab:TableAltApproaches). An initial list of criteria was identified during the peer review meeting, then modified as the evaluations were being filled in during the follow-up process. Criteria can be grouped into three distinct types: Estimation criteria are relevant to SR model fitting or simulation model scoping. Outcome criteria relate to the type of end-product generated by the aggregation method. Implementation criteria relate to how the end-product can be used, and when it could be available.

```{r TableCriteria, echo = FALSE, results = "asis"}


table.in <- read_csv("data/AggregationApproachTables/AggregationTable_Criteria.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteria)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "4em") %>%
     kableExtra::column_spec(2, width = "14em") %>%
     kableExtra::column_spec(3, width = "28em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage

(ref:TableSummary) Summary of characteristics of 8 alternative methods for developing aggregate reference points. The peer-review process compared alternative approaches for developing aggregate reference points (Table \@ref(tab:TableAltApproaches)) based on a set of 10 criteria (Table \@ref(tab:TableCriteria)). A YES/NO/MAYBE rating was assigned for each criterion to provide a comparison of aggregation methods. YES identifies that the aggregation approach meets the criterion. MAYBE means that current approach could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.  The Critical column values are provided by the review participants and identify criteria that are critical (Yes) or to be determined (TBD). Appendix \@ref(AggregationAppendix) briefly summarizes the rationale for each rating.

\begin{landscapepage}



```{r TableSummary, echo = FALSE, results = "asis"}



table.in <- read_csv("data/AggregationApproachTables/AggregationTable_Summary.csv")

col.names.use <- c("Criterion","Crit?","Agg\nSmsy","Sum\nSmsy","Umsy\nComp","Equ.\nProf",
"Agg\nEqu.\nProf.","Status","Log\nReg","Sim")


#table.cols <- table.in
#table.cols[,] <- "white"
#table.cols[table.in == "YES"] <- "green" 
#table.cols[table.in == "NO"] <- "orange" 

col3.cols <- rep("white", 11)
col3.cols[table.in[,3]  == "YES"]  <- '#b8e186'
#col3.cols[table.in[,3]  == "NO"] 	 <- '#d01c8b'
col3.cols[table.in[,3]  == "MAYBE"] 	 <- "#F1CF31"	  #'#f1b6da'

col4.cols <- rep("white", 11)
col4.cols[table.in[,4]  == "YES"]  <- '#b8e186'
col4.cols[table.in[,4]  == "MAYBE"] 	 <- "#F1CF31"	 

col5.cols <- rep("white", 11)
col5.cols[table.in[,5]  == "YES"]  <- '#b8e186'
col5.cols[table.in[,5]  == "MAYBE"] 	 <- "#F1CF31"	 

col6.cols <- rep("white", 11)
col6.cols[table.in[,6]  == "YES"]  <- '#b8e186'
col6.cols[table.in[,6]  == "MAYBE"] 	 <- "#F1CF31"	 

col7.cols <- rep("white", 11)
col7.cols[table.in[,7]  == "YES"]  <- '#b8e186'
col7.cols[table.in[,7]  == "MAYBE"] 	 <- "#F1CF31"	
	
col8.cols <- rep("white", 11)
col8.cols[table.in[,8]  == "YES"]  <- '#b8e186'
col8.cols[table.in[,8]  == "MAYBE"] 	 <- "#F1CF31"	
	
col9.cols <- rep("white", 11)
col9.cols[table.in[,9]  == "YES"]  <- '#b8e186'
col9.cols[table.in[,9]  == "MAYBE"] 	 <- "#F1CF31"	
	
col10.cols <- rep("white", 11)
col10.cols[table.in[,10]  == "YES"]  <- '#b8e186'
col10.cols[table.in[,10]  == "MAYBE"] 	 <- "#F1CF31"	
	
#col11.cols <- rep("white", 11)
#col11.cols[table.in[,11]  == "YES"]  <- '#b8e186'
#col11.cols[table.in[,11]  == "MAYBE"] 	 <- "#F1CF31"	

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = "l",
              col.names = linebreak(col.names.use), #landscape = TRUE,
                  caption = "(ref:TableSummary)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "20em") %>%
  kableExtra::column_spec(3, background =  col3.cols) %>%
  kableExtra::column_spec(4, background =  col4.cols) %>%
	  kableExtra::column_spec(5, background =  col5.cols) %>%
	  kableExtra::column_spec(6, background =  col6.cols) %>%
	  kableExtra::column_spec(7, background =  col7.cols) %>%
	  kableExtra::column_spec(8, background =  col8.cols) %>%
	  kableExtra::column_spec(9, background =  col9.cols) %>%
	  kableExtra::column_spec(10, background =  col10.cols) #%>%
	  #kableExtra::column_spec(11, background =  col11.cols)
	
	
	
	
	
	
	
	#table.cols[,3]) #%>%	

# COLOR NOT WORKING! WHY??????


```

\end{landscapepage}

\clearpage
(ref:TableChallenges) Key challenges for alternative aggregation approaches. Aggregation approaches are grouped into 4 types based on their fundamental ingredient (i.e., benchmarks, status, equilibrium profiles, or forward simulations).

```{r TableChallenges, echo = FALSE, results = "asis"}


table.in <- read_csv("data/AggregationApproachTables/AggregationTable_KeyChallenges.csv")


table.in$Type[duplicated(table.in$Type)] <- ""


table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 9,align = "l",
              #col.names = linebreak(col.names.use),
                  caption = "(ref:TableChallenges)")  %>%
	kableExtra::row_spec(c(3,5,7), hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "8em") %>%
         kableExtra::column_spec(2, width = "15em") %>%
        kableExtra::column_spec(3, width = "20 em") %>%
	kableExtra::row_spec(c(1,2,4,6,8), extra_latex_after = "\\cmidrule(l){2-3}") 


```







\clearpage

## IMPLEMENTATION OF ALTERNATIVE APPROACHES FOR DEVELOPING MANAGEMENT REFERENCE POINTS FOR STOCK AGGREGATES {#AltApproachesMethods}


This section briefly describes the implementation of the alternative approaches for developing management reference points that were discussed in the initial version of this Research Document and during the CSAS RPR.

### Aggregate Estimates of Biological Benchmarks (Agg Smsy)

This approach simply calculates Smsy estimates for aggregate-level SR model fits (Sec. \@ref(CandidateAggModels)). 


### Sum of Stock-Level Biological Benchmarks for Abundance (Sum Smsy)

Calculate the sum of Smsy estimtes for wild stocks. For the example presented here, we compare the sum of mean and median Smsy and Smax estimates across modelled stocks to the corresponding estimates for aggregate-level SR model fits. Percentiles of the distributions are shown for the single-stock and aggregate fits. If percentiles for the sum of stock-level estimates are required, they can be calculated by summing the individual MCMC samples, then calculating the percentiles. 

### Compare Stock-level Biological Benchmarks for Exploitation Rate (Umsy Comp)

Although estimates of Umsy cannot be summed across stocks, but it is informative to compare them. We include two types of comparison:

* Visual comparison of posterior distributions for stock-level and aggregate-level Umsy estimates. 
* Frequency distribution of median Umsy across stocks, adapting the approach from Figure 14 in @Waltersetal2008ISRP.

### Calculation of Spawner-based Equilibrium Profiles (Equ. Prof) {#EqProfilesMethods}

Recent escapement goal analyses for Alaskan salmon stocks have equilibrium  probability profiles as a standard part of the results. Initial applications focused on yield profiles that capture the notion of “pretty good yield” (PGY) as defined by @HilbornPGY, but other types of profiles have also been explored (e.g., recovery profiles). While implementation methods continue to evolve, these profiles were generated with the same basic approach.

For example, the "80-60 range" for a yield profile is derived as follows:

* specify a reference value $Ref$ for yield at 60% of MSY.
*  for each spawner abundance $S_i$, calculate the percent of MCMC samples for which the expected number of recruits is at least $S_i + Ref$, which captures the expected yield under equilibrium conditions if the stock were managed to a fixed escapement goal at $S_i$ and all returns above $S_i$ were harvested. 
* Identify the range of $S_i$ for which the percent of samples meeting the criterion is at least 80%.

Examples include summer Chum Salmon in the East Fork of the Andreafksy River (Fleishman and Evenson 2010), Taku River Chinook Salmon (McPherson et al. 2010), Alsek River Sockeye (Eggers and Bernard 2011), and salmon stocks in the Copper and Bering rivers (Fair et al. 2011). Equilibrium probability profiles have also been included in recent escapement goal analyses for northern transboundary salmon stocks [e.g., @PestalJohnstonTakuCo; @MillerPestalTakuSk].

We implemented the yield profiles as follows: at each increment of spawner abundance, we compare the distribution of yields (Rec-Spn) across parameter samples to the specified % of the median yield at median Smsy, and count the proportion that are larger. The resulting profile shows the probability of meeting or exceeding this average target, which is an anchor point for subsequent planning processes tasked with choosing spawning goals. 

These yield profiles differ from the version included in recent ADFG and transboundary analyses [e.g., @EggersBernard2011Alsek; @MillerPestalTakuSk], which plot the probability of meeting the implied target for each parameter set (i.e., at each spawner increment, compare yield to a chosen % of MSY for that parameter set). @PestalJohnstonTakuCo compared the two approaches. Both have the same intent, and we consider them equally valid. They simply differ in the details of the calculation. We did not include a side-by-side comparison in this paper, but the alternate version can easily be calculated if a future planning process requests it.

We included three alternative yield profiles to illustrate the importance of specifying the exact objectives:

* Probability that equilibrium yield > 80% MSY
* Probability that equilibrium yield > 60% MSY
* Probability that equilibrium yield > stock-specific reference value (e.g., 1,000, 10,000, etc.)

For each profile, we show two curves corresponding to the long-term average and recent productivity scenarios, using the *same* reference value (i.e., both are compared to the long-term average MSY). The intent is to highlight the difference in expected yield between the two productivity scenarios.
 

### Calculation of ER-based Equilibrium Profiles (Agg Tradeoff)


Using the approach by Walters et al. (2008), the equilibrium state for each component stock was calculated at different levels of fixed exploitation rate (i.e., what spawner abundance and catch would the stock eventually settle down to, if each ER were applied for many years, in the absence of inter-annual variation?). Equilibrium spawner abundances and catches were then summed across stocks to calculate aggregate equilibrium spawners and catch under the assumption that all component stocks are harvested at the same fixed ER, and all are at equilibrium. This simplifying assumption allows the aggregate trade-off profiles to be calculated directly from the SR parameter estimates.  This approach is described in Section 2.3 of Walters and Martell (2004), Walters et al. (2019), Staton et al. (2020), and Connors et al. (2020).

At a fixed exploitation rate $U_q$ the equilibrium calculation for spawner abundance $S_{q}$  is:


\begin{equation}
S_{q} = S_{\mathrm{MSY}} \frac{U_{MSY} - \ln\left(\frac{1 - U_{MSY}}{1 - U_{q}}\right)}{U_{MSY}}
\end{equation}

and for equilibrium harvest ($H_{q}$) is:

\begin{equation}
H_{q} = \frac{U_{q}S_{q}}{1 - U_{q}}
\end{equation}

Appendix \@ref(EquProfFuns) shows the corresponding R code.

### Status-based Aggregate Limit Reference Points (Status) {#StatusMethods}

Canada's modernized *Fisheries Act* (2019) requires that limit reference points (LRP) are developed for stock management units (SMU). Pacific salmon present a challenge due to their complex population structure, and guidelines for developing aggregate salmon LRPs were just published [@LRPGuidelinesSAR]. The recommended approach is to assess status of the CUs in the SMU according to WSP criteria, and then determine whether an SMU meets the LRP based on the CU statuses (i.e., number of CUs in the red status zone, changes in CU statuses over time).

Status assessments under the WSP integrate multiple metrics, where available [@Holtetal2009BM]: 

* abundance relative to biological benchmarks (Sgen, 80% Smsy), where available
* absolute abundance relative to a small population threshold of 1,000 spawners, for consistency with COSEWIC criteria [@CosewicMetrics2021]
* long-term trend
* short-term trend (probability of decline)
* distribution of spawners across sites

Integrated WSP status assessments have been completed for Fraser River Sockeye [@FrSkWSPStatus2012; @FrSkWSPStatus2017], Southern BC Chinook [@SBCCkWSPStatus2012SAR], and Interior Fraser Coho [@IFCohoWSPStatus2013SAR]. Each of these status assessments was a multi-year process, culminating in a multi-day workshop where 30-40 experts reviewed available information (quality-controlled data, biological benchmarks, status metrics) and assigned a consensus status designation to each CU. This process has not been completed for Skeena and Nass Sockeye, but considering stock status in harvest management decisions is required under the WSP [@WSPImplementation]. 

Ongoing work to develop an algorithm-based rapid approximation of the expert status designations will use the data and biological benchmarks generated through the Skeena and Nass Sockeye escapement goal review process. Pending completion of these multi-criteria status assessments for Skeena and Nass Sockeye, we illustrate the building blocks of the status-based approach using one of the status metrics, but we do not attempt to complete a comprehensive status assessment here.

Specifically, we compared the running generational geometric mean of spawner abundance to the lower benchmark at Sgen and the upper benchmark at 80% Smsy, then summarized the annual proportion of stocks in the red, amber, and green status zones *on that single metric*.  We used the median Sgen and Smsy values for the long-term average productivity scenario (Section \@ref(ModelSelection)), which is consistent with the benchmarks used in past WSP status assessments (Section \@ref(BMMethods)).


### Aggregate Abundance Reference Points Based on Logistic Regression (Log Reg)

A candidate approach for developing aggregate abundance reference points is to define a success/failure criterion, plot observed success/failure vs. observed aggregate abundance, fit a logistic regression, and select a reference point based on a chosen probability threshold [@LRPGuidelinesSAR]. This approach is only applicable under certain conditions, and formal guidelines for its use in the development of limit reference points have not been finalized. 

We include an example of this approach using a criterion linked to the lower WSP benchmark for the relative abundance metric, which is Sgen. Specifically, we defined success as *"At least 80% of the stocks in the aggregate are above Sgen"*.  We used the median Sgen value for the long-term average productivity scenario, which is consistent with the benchmarks used in past WSP status assessments (Section \@ref(BMMethods)).

<!--chapter:end:02-Methods.Rmd-->


### Simulation-based Aggregate Abundance Reference Points (Sim) {#SimMethodsGeneral}


Forward simulation can be used to explore how different management actions perform over a range of alternative assumptions about future conditions, with risk quantified using the resulting escapement trajectories as part of a formal decision analysis [e.g., @HilbornPeterman1996PrecApp; @deYoungetal1999UncertainWorld; @PuntetalMSEBestPractices].

The key benefit of building forward simulation models is that they allow us to compare the expected performance of alternative strategies and identify strategies that are more robust to uncertainty [e.g., @PuntetalMSEBestPractices], which has been characterized as searching for a *"safe-fail"* strategy that avoids catastrophic outcomes even when things go wrong, rather than identifying a strategy that is optimal under very specific assumptions and conditions (Ann-Marie Huang, DFO, and Mike Staley, pers. comm, 2010).

Building a fully functional simulation model to support Skeena and Nass Sockeye planning will require that many choices be made regarding model scope, biological assumptions, management assumptions,  objectives, and performance measures. 

We include a worked example to illustrate potential benefits and expected challenges, and to initiate the development process for a more comprehensive model. We consider this an urgent first step, because  recently published guidelines [@LRPGuidelinesSAR] identify forward simulations as one of the candidate approaches for developing aggregate reference points for stock management units, and the on-going Canadian domestic consultation process has also focused on simulation results. In addition, the development of a formal management strategy evaluation (MSE) model was identified as an important future step by the two independent reviewers for the escapement goal review process. We describe the current version of the simulation model in the next section.


## IMPLEMENTATION OF FORWARD SIMULATION APPROACH {#SimMethodsImplementation}

### Model Structure


```{r num.stocks.calc, echo = FALSE, results = "asis"}

tmp.df <- read_csv("data/Sims/Generated_StockInfo_SimUsed.csv")

num.nass <- sum(tmp.df$MU == "Nass")
num.skeenawild <-  sum(tmp.df$MU == "SkeenaWild")


```

This simulation example explores the range of near-term responses to alternative harvest strategies under alternative productivity assumptions, starting from recent spawner abundances (i.e., not simulating a long time into the future to explore equilibrium conditions).

This simulation example includes only the  `r  num.nass + num.skeenawild` wild stocks for which spawner-recruit models were fitted in the current round of work (`r num.nass` Nass stocks, `r num.skeenawild` Skeena stocks). Simulations start with the last 8 years of spawner abundance (2012-2019). For a few stocks, missing estimates in this time window were infilled with the mean of available observations. 

Forward simulations generate one 20-yr trajectory for each parameter set sampled from the parameter distributions selected for each productivity scenario (Section \@ref(ModelSelection)).

For each simulated year (Figure \@ref(fig:SimModelFlowchart)):

* calculate stock run size based on recruits by age
* apply the candidate harvest strategy to each aggregate
* calculate the resulting spawner abundance by stock
* calculate the total recruits for each stock based on SR parameter set for the candidate productivity scenario (includes a random error, and a cap on recruitment set at 20% larger than the largest observed recruitment) 
* distribute the recruits across return years based on median observed age composition

Total run size $Run$ for each aggregate $agg$ in year $yr$ with parameter set $par$ is the sum across ages and stocks, with age-specific recruits for each stock from corresponding brood years (e.g., age 4 fish from 4 years ago, age 5 fish from 5 years ago):

\begin{equation} 
  Run_{agg,yr,par} =  \sum_{ages}\sum_{stocks} Rec_{age,stock,yr-age,par}
\end{equation} 

Exploitation rate for each stock is a calculated based on the aggregate run, an aggregate harvest control rule $HCR$, aggregate-level outcome uncertainty $AggOU$, and stock-specific outcome uncertainty $StkOU$:
 
\begin{equation} 
  ER_{stock,yr,par} = fn(Run_{agg,yr,arp}, HCR, AggOU, StkOU)
\end{equation}

Catch $Ct$ and spawner abundance $Spn$ for each stock are then calculated as:

\begin{equation} 
  Ct_{stock,yr,par} = Run_{stock,yr,par} * ( ER_{stock,yr,par}) 
\end{equation}

\begin{equation} 
  Spn_{stock,yr,par} = Run_{stock,yr,par} * (1 - ER_{stock,yr,par}) 
\end{equation}

Finally, total recruits $Rec$ for each stock are calculated as:

\begin{equation} 
  Rec_{stock,yr,par} =  Spn_{stock,yr,par} * exp(ln.alpha_{stock,par} -  beta_{stock,par} * Spn_{stock,yr,par}) 
\end{equation}


The same model structure and code base were used for the Recovery Potential Assessment (RPA) of Fraser River Sockeye [@Huangetal2021FraserSkRPA].  The code is designed for computing efficiency in R, using array calculations and pre-filled arrays where possible. For example, age proportions are pre-generated as a 4- dimensional array (Stocks x  MCMC parameter sets x Simulation Years x Age Classes), and for each simulated brood year the corresponding 3-dimensional sub-array  of age proportions is multiplied with a 2-dimensional slice of the recruitment array (Stock x MCMC parameter sets) to populate a subset of a 3-dimensional run size array (Stocks x  MCMC parameter sets x [Brood Year + Min Age]:[Brood Year + Max Age]). The pre-generated arrays allow maximum flexibility for exploring alternative assumptions (e.g., variable or changing age composition).

\clearpage
(ref:SimModelFlowchart) Simulation model components.  The biological submodel simulates stock-specific population dynamics to generate adult returns for each stock for each brood year and resulting aggregate runs by calendar year. The harvest submodel then determines a target ER given a harvest rule and applies it with outcome uncertainty to calculate harvest and spawner abundance. One example of a harvest rule is a fixed escapement target of 300,000 combined with a minimum ER of 10% and a maximum ER of 65%. Alternative simulations can then test the effect of changing components of the harvest rule, such as varying the maximum ER from 20% to 80% in increments of 10%.


```{r SimModelFlowchart,  fig.cap="(ref:SimModelFlowchart)"}
include_graphics("diagrams/Diagram_SimModelStructure_REV.PNG")
```



\clearpage

Three extensions of the Fraser Sockeye RPA model were implemented for Skeena and Nass Sockeye:

* *Aggregate harvest strategies*: The purpose of the RPA model was to test different levels of fixed exploitation rate, but for Skeena and Nass Sockeye the focus is on testing alternative types of harvest strategies (i.e., fixed aggregate escapement goal, abundance-based rule).
* *Outcome uncertainty*: This captures the difference between management targets and actual realized outcomes, caused by mechanisms like: (1) uncertain estimates of abundance and run timing, (2) physical and biological processes that change the availability of fish to fishing gear within a season, (3) non-compliance with fishing regulations, (4) inappropriate choices of regulations, (5) errors in their implementation [e.g., @HoltPetermanOutcomeUnc]. We model outcome uncertainty in two steps, first as the difference between the aggregate ER target and the actual aggregate ER, and then as the difference between actual aggregate ER and stock-specific ER (Appendix \@ref(OutcomeUncApp)).
* *Covariation in productivity*: This captures the observation that productivity is not independent across salmon stocks, because shared environmental factors affect their life cycle [e.g., @CkCov2017]. While it is very hard to identify the specific biological mechanisms at work on a specific group of stocks, we can identify patterns in the resulting overall productivity (Recruits/Spawner) and reflect that in the forward simulation by sampling the annual random error with covariation, such that better-than-expected recruitment for Stock A tends to happen in the same year as better-than-expected recruitment for Stock B. We model covariation in productivity based on simplified correlations of log-residuals from the Basic Ricker fit within and between groups of stocks with similar life history spawning in a shared freshwater adaptive zone (Appendix \@ref(CovarProdApp)). 

Additional mechanisms could be added to the model, such as:

* en-route or pre-spawn mortality (i.e., not all fish that escape past the fisheries spawn successfully)
* changes over time in productivity (for now, the productivity scenarios differ from each other, but each is assumed to persist over the 20-yr simulation)
* changes over time in age composition of recruits for each stock

Each of these additions has a potentially large effect on the simulation results, but addressing them properly is a complex challenge, and not purely a technical one. Participants in the planning process will have to consider which of them need to be explored, and how to bound the explorations, if the choice is made to pursue simulation-based aggregate reference points.





\clearpage

### Types of Harvest Control Rules

In the current model structure, two types of alternative harvest strategies can be specified for each of the two aggregates (Nass, SkeenaWild):

* *FixedER*: Apply a fixed exploitation rate from 0% to 90% in 10% increments to both aggregates, assuming that all component stocks are harvested at the same rate. 

* *FixedEsc*: Simple abundance-based harvest rule for each aggregate, where target exploitation rate is based on aggregate abundance above the escapement goal, with optional specifications for a minimum ER at low abundance and upper cap on ER at larger abundances. Alternative escapement targets were set at 25% to 250% of the interim escapement goal for Nass at 200,000 and the interim aggregate escapement goal for SkeenaWild at 300,000 (Table \@ref(tab:FixedEscHCR)). These escapement targets were combined with lower bounds on ER of 0%, 10%, or 20%, and upper bounds of 60% or 80%.


(ref:FixedEscHCR) Fixed Escapement Strategy: Alternative Scenarios. Scenarios were specified relative to the interim escapement goals currently in use, so that *Esc100* matches the interim goal, *Esc50* is half the interim goal, and *Esc200* is double the interim goal. Note that the interim goal for the SkeenaWild aggregate was set at 1/3 of total Skeena interim target of 900,000, based on average observed proportion of wild spawners in the total spawner abundance since 2000.


```{r FixedEscHCR, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Sims/FixedEscMin10Max80ER_Variations.csv") %>% 
	mutate(ScenarioValue = as.numeric(gsub("Esc","",Scenario))) %>%
	arrange(ScenarioValue) %>%
	select(Scenario,MU,spn.goal) %>%
	dplyr::rename(SpnGoal = spn.goal) %>%
	mutate(SpnGoal = prettyNum(round(SpnGoal*10^6,0),big.mark=",",scientific=F) )  %>% 
	pivot_wider(id_cols = Scenario,names_from = MU, values_from = SpnGoal)




table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","r","r"),
                  caption = "(ref:FixedEscHCR)")  %>%
	kableExtra::row_spec(3:4, hline_after = TRUE) %>%
	kableExtra::row_spec(4, bold = TRUE) 


```






### Scenarios {#SimScenarios}

What makes forward simulations so powerful is the ability to test many different scenarios and bring the results into a collaborative planning process where plausible outcomes under alternative assumptions can be debated. However, this flexibility also creates the biggest challenge for using simulation models in a decision support setting: how to bound the explorations?

With the current model structure, we have so far explored the following options for key model components:

* *Productivity*: six alternative scenarios (Section \@ref(ModelSelection))
* *Harvest Strategy*: Two main types (Fixed ER vs. Fixed Esc), 10 different levels for each, plus alternative combinations of ER floor and cap for the Fixed Esc strategies (0-90% ER, 10-80%, 20-80%, 20-60%), for a total of 50 alternative strategies
* *Aggregate outcome uncertainty*: Three alternatives - none, narrow, wide (Appendix \@ref(OutcomeUncApp))
* *Stock-specific outcome uncertainty*: Three alternatives - none, all years, 1995-2013 brood years (Appendix \@ref(OutcomeUncApp))
* *Covariation in productivity*: Four alternatives - none, simplified covariation 1984-2013 brood years, simplified covariation 1999-2013 brood years, detailed pairwise covariation 1984-2013 brood years  (Appendix \@ref(CovarProdApp))

Just these model components already yield 6x50x3x3x4 = 10,800 alternative scenarios. Because these components interact (e.g., the effect of the covariation assumption may differ depending on the harvest strategy), we would ideally run and compare all of the alternative scenarios, but in practice this is usually an iterative process guided by participants in a broader planning exercise [e.g., @PuntetalMSEBestPractices].

In this paper we present examples of results for 40 of the alternative scenarios, as well as a high-level summary of sensitivity tests completed so far. The intent is to illustrate the type of information that can be generated by a full-scale implementation of a management strategy evaluation and
set the stage for future rounds of model refinement and scenario exploration. The forty scenarios are:

* 10 levels of fixed ER (0-90%) under long-term average productivity
* 10 levels of fixed ER (0-90%) under recent productivity (1 generation)
* 10 levels of fixed escapement, ranging from 1/4 to 2.5 times the interim EG, with a 10% ER floor and an 80% ER cap, under long-term average productivity
* 10 levels of fixed escapement, ranging from 1/4 to 2.5 times the interim EG, with a 10% ER floor and an 80% ER cap, under recent productivity (1 generation)

We ran these scenarios for 3 generations (15 years), starting with 2020 as the first simulated year. All scenarios used the wide version of the aggregate outcome uncertainty, the 1995-2013 version of the stock-specific outcome uncertainty, and the simplified productivity covariation observed over 1999-2013 brood years. These settings were used as the base case for model explorations following the peer review process in April 2022.

### Objectives, Performance Measures, and Diagnostic Plots

To convert the simulation trajectories into meaningful summaries of expected outcomes, we need to identify aggregate-level and stock-level objectives and develop quantitative performance measures for them. For this illustration, we defined a general aggregate objective as *"most stocks should meet their stock-specific conservation objectives"*, and translated this into a quantitative objective that "*16 of the 20 modelled stocks (80%) should have at least 80% probability of spawner abundance exceeding the upper WSP benchmark for the relative abundance metric, which is set at 80% Smsy, after 3 generations (simulation years 11-15)"*. These objectives are examples selected for this illustration, and are not intended as a recommendation of which management objectives should be evaluated by upcoming planning processes. 

We used the median Smsy value for the long-term average productivity scenario (Section \@ref(ModelSelection)) in this performance measure, which is consistent with the benchmarks used in past WSP status assessments (Section \@ref(BMMethods)). Simulation trajectories based on the current productivity scenario and the high/low productivity bookend scenarios were also compared to the same benchmark, to make results comparable across scenarios and to emphasize how different the outcomes under the different scenarios are compared to expectations based long-term average properties.

As with the other components of this simple illustration, the challenging work of developing an agreed-upon suite of objectives and performance measures specifically for the current management context of Skeena and Nass Sockeye will start in the next phase of the project, which is an engagement process with rights holders and stakeholders. Once this takes shape, additional performance measures can easily be calculated and presented for the simulation trajectories (e.g., probability that aggregate catch meets some minimum level, variability in catch associated with different types of harvest strategy).






<!--chapter:end:02b-MethodsSim.Rmd-->

# RESULTS

This Research Document focuses on SR modelling for wild Sockeye stocks (16 Skeena, 4 Nass) and alternative approaches for developing management reference points for two aggregates (Skeena Wild, Nass). Enhanced Pinkut and Fulton Sockeye present a fundamentally different challenge in terms of population dynamics and management implications. We include a review of available information for Babine Sockeye enhancement facilities (Appendix \@ref(ChannelReview)) and illustrate two candidate methods for expanding a management reference point for wild Skeena Sockeye to a management reference point for the whole Skeena aggregate including enhanced Pinkut and Fulton (Section \@ref(SkeenaExpResults)). Section \@ref(AltApproachEnhanced) explains the rationale for this approach within the scope of the current project. Appendix \@ref(PinkutFultonResults) includes the available SR data and parameter estimates for Pinkut and Fulton as a reference, but resulting benchmark estimates should not be used as they are, given SR model fitting issues and management challenges.


## SINGLE-STOCK SPAWNER-RECRUIT MODEL FITS (STOCK-LEVEL AND AGGREGATE-LEVEL)  {#SingleStockSRResults}


### Convergence {#Convergence}


```{r , echo = FALSE, results = "asis"}

conv.summary.df <- read_csv("data/Convergence/FitsComparison_ConvergenceSummary.csv") 

full.set.post.df <- read_csv("data/Appendix SR Model Outputs/FullSet_Posterior_Summary.csv") 

```


We tested `r conv.summary.df %>% dplyr::filter(DataType == "All") %>% select(Total)` alternative SR model fits for `r length(unique(full.set.post.df$Stock)) -3` stock-level SR data sets (including enhanced Pinkut and Fulton) and three aggregate-level SR data sets (Nass, Skeena, SkeenaWild).

All of the candidate SR model fits met at least two of three convergence criteria, and most model fits met all three criteria (Table \@ref(tab:ConvergenceTab)). Convergence varied depending on the amount of data used, the SR model form, and form of the capacity prior (Smax). Models fit to all available brood years of data converged more reliably than model fits to truncated data sets, but note that not all model forms could be fitted to all stocks, and that the number of available brood years varies across stocks (Table \@ref(tab:DataOverviewSkeena); Figure \@ref(fig:SRDataOverview)). More complex SR model forms were run with more intensive MCMC sampling (longer burn-in, larger samples), but still had a lower rate of meeting all three convergence criteria. Alternative capacity priors had little effect on convergence for the Basic Ricker model, with 88-89% of model fits meeting all three convergence criteria across four alternative capacity priors.


### Alternative SR Model Fits

Posterior estimates of capacity (Smax) were much wider and more skewed than posterior estimates of productivity (ln.alpha) for most stocks using the Basic Ricker model (Figure \@ref(fig:FitsCompProdCap)). Four stock-level fits and one aggregate-level fit were notably more uncertain (i.e., wider posteriors) than the other fits: Lower Nass Sea and River Type, Kitwanga, Motase, and the SkeenaWild aggregate.

For the Basic Ricker model, median Bayesian estimates of Smsy were similar to simple deterministic estimates for most cases (Figure \@ref(fig:FitsCompDetFig)). Differences between Bayesian and deterministic estimates were larger for cases where the Bayesian estimate was more uncertain (i.e., wider posterior, larger SIQR). The two stocks with the largest difference between the Bayesian and deterministic Smsy estimate also had the most uncertain Bayesian estimates (Lower Nass Sea and River Type, Kitwanga).


(ref:ConvergenceTab) Summary of convergence diagnostics for single-stock SR model fits. We considered three convergence diagnostics with quantitative thresholds (Table \@ref(tab:MCMCDiagnostics)). Table shows the total number of stock-level and aggregate-level single-stock SR model fits tested (Total), the number that met each criterion (Rhat, Gelman, Geweke), the number and percent of fits that met at least two of the criteria (Met2, pMet2), and the number and percent of fits that met all three criteria (Met3, pMet3). Convergence thresholds were specified as Rhat < 1.05, Gelman within [0.99,1.01], and Geweke within [-2,2]. For each criterion, the value compared to the threshold was the most extreme value across all estimated parameters. For the TVP model fits with time-varying productivity, this captures the poorest fit across all the brood-year-specific ln.alpha posteriors.

```{r ConvergenceTab, echo = FALSE, results = "asis"}

table.in <- conv.summary.df %>% mutate(ModelType = gsub("Kalman","TVP",ModelType))
table.in$DataType[duplicated(table.in$DataType)] <- ""



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l","l","l",rep("r",8)),
                  caption = "(ref:ConvergenceTab)" ) %>%
    kableExtra::row_spec(c(1,9), hline_after = TRUE) %>%
	add_header_above(c(" " = 4, "Criteria" = 3," " = 4) ) %>%
  kableExtra::row_spec(c(5,7,13,15), extra_latex_after = "\\cmidrule(l){2-11}") 

```

For the Basic Ricker model, the effect of alternative capacity priors was generally larger for cases where capacity estimates were more uncertain (Figure \@ref(fig:FitsCompCapPrior)). Three stock-level fits and one aggregate-level fit were notably more sensitive (i.e., larger differences in median estimates) than the other fits: Lower Nass Sea and River Type, Kitwanga, Babine Late Wild, and the Skeena aggregate. Motase had the widest Smsy posterior across alternative capacity priors.


Posterior parameter distributions were similar for Basic Ricker and AR1 model fits in most cases (Figures \@ref(fig:FitsCompAR1Pars) and \@ref(fig:FitsCompAR1Smsy)). The AR1 model had slightly wider ln.alpha
posteriors for most fits and slightly narrower Smax posteriors for some fits. Estimates of ln.alpha  that were more uncertain in the Basic Ricker fit did not improve with the AR1 model form. The AR1 fit for Swan/Stephens was even poorer than the Basic Ricker fit. Among the cases where AR1 models could be fitted, the posterior capacity estimates were most uncertain for the three aggregate-level fits and for Lower Nass Sea and River Type Sockeye, which also showed the largest improvement due to the change in model form (i.e., Smax posterior much narrower for the AR1 fit than the Basic Ricker fit). Median Smsy estimates were very similar for the two alternative model forms, except for Lower Nass Sea and River Type, where the AR1 estimate has a 26% lower median and a substantially narrower posterior.

A TVP model with time-varying productivity could be fitted to 12 stocks to identify stock-specific changes over time in productivity (Figure \@ref(fig:FitsCompProd1)). Most stocks showed recent declines in productivity, but the time trends differed between stocks.  Five stocks showed a persistent decline in productivity since the 1990s, and these include the two largest wild stocks (Babine Early Wild, Babine Late Wild, Damdochax, Meziadin, and Swan/Stephens). Estimated productivity of Alastair gradually increased since the 1960s, until a drastic drop in the last few brood years. Babine Mid Wild has been mostly stable, with a slight decline. Bear has increased. Productivity is highly variable over time for three stocks (Lakelse, Lower Nass Sea and River Type, and Morice). 

The estimated productivity trends from the TVP model fit generally track the trends over time of residuals generated from a Basic Ricker fit (Figure \@ref(fig:FitsCompProd1) vs. Figure \@ref(fig:FitsCompProd2)), and the TVP residuals therefore change less over time (Figure \@ref(fig:FitsCompProd3) vs. Figure \@ref(fig:FitsCompProd2)). Note the trade-off between the time-trends in time-varying ln.alpha from the TVP model and the TVP residuals (Figure \@ref(fig:FitsCompProd1) vs. Figure \@ref(fig:FitsCompProd3)): For those stocks where the productivity parameter changes smoothly over time, the TVP residuals vary a lot from year to year (i.e., "spiky"), while for those stocks where the productivity parameter changes rapidly between years, the TVP residuals are much smoother. Depending on the stock, the observed variability in the data is allocated either to the time-varying productivity parameter or to the residual error.

For stocks where all three model forms could be fitted, the median posterior estimates of the productivity parameter ln.alpha are very similar between the basic Ricker and AR1 model, but brood-year specific ln.alpha estimates for the TVP model with time-varying productivity span a wide range, from much higher to much lower median estimates  than the basic Ricker and AR1 fits (Table \@ref(tab:ProdCompAcrossModels)).

Figures \@ref(fig:ScatterMezidian) to \@ref(fig:JoinPostBabLW) summarize the SR data and alternative model fits for the largest wild stock from each aggregate: Meziadin on the Nass and Babine Late Wild on the Skeena.

\clearpage
(ref:ProdCompAcrossModels) Comparison of median ln.alpha estimates across SR model forms. Table lists the number of available brood years with SR data (n), the median estimate for the basic Ricker model (BR), and median estimates (Med) and percent difference to the basic Ricker (pDiff) for the AR1 model fit, and the year-specific estimate for the brood years with  lowest and highest median estimate from the time-varying productivity model (TVP Min, TVP Max).

```{r ProdCompAcrossModels, echo = FALSE, results = "asis"}

table.in <- read_csv("data/FitsComparison_ReportTable_lnalpha.csv",
										 col_types = cols(.default = "c"))  %>%
							select(-contains("SIQR"),-SpnContr) %>%
							mutate_all(~ as.character(.x))
	

table.in[is.na(table.in)] <- "-"
table.in$Basin[duplicated(table.in$Basin)] <- ""


col.names.use <-  c("Basin","Stock","n","Med","Med","pDiff","Med","pDiff","Med","pDiff")



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l","l",rep("r",8)),
									col.names = col.names.use ,
                  caption = "(ref:ProdCompAcrossModels)" ) %>%
	 #kableExtra::column_spec(6, bold = T) %>%
     #kableExtra::row_spec(c(lines.idx), hline_after = TRUE) %>%
	add_header_above(c(" " = 3, "BR" = 1,"AR1" = 2,"TVP Min" = 2,"TVP Max" = 2))  

```







\clearpage

(ref:FitsCompProdCap) Comparison of Bayesian posterior distributions for productivity (ln.alpha) and capacity (Smax). Model fits shown are the Basic Ricker fits with capped uniform prior for Smax. Posterior distributions are summarized as the standardized interquartile range SIQR = (p75-p25)/p50. The reference line marks a slope of 2 (i.e., posterior for Smax is twice as wide relative to the median value as the posterior for ln.alpha). Model fits are flagged with red points if the SIQR for Smax is larger than 0.8 or the SIQR for ln.alpha is larger than 0.3. Johnston is labelled because it has the narrowest Smax posterior (lowest SIQR).

```{r FitsCompProdCap,   fig.cap="(ref:FitsCompProdCap)"}
include_graphics("data/Convergence/FitsComparison_ProdvsCapWidth.png")
```




\clearpage
(ref:FitsCompDetFig) Comparison of Bayesian and deterministic estimates for Smsy. Both estimates use the basic Ricker model form. The Bayesian estimate is the version with the capped uniform prior on capacity. Model fits are flagged with red points if the difference is larger than 25%. Panel A compares estimates on a log scale to allow comparison across stocks with very different estimates, but stocks are flagged if the difference in original (i.e., unlogged) values is larger than 25%. Bayesian and deterministic fits are mostly similar. Panel B shows the relationship between the % difference and the width of the Bayesian posterior distribution, expressed as the standardized interquartile range (SIQR). The difference between estimates increases with larger uncertainty in the Bayesian estimates (i.e., larger SIQR).

```{r FitsCompDetFig,   fig.cap="(ref:FitsCompDetFig)"}
include_graphics("data/Convergence/FitsComparison_BRcuVsDet.png")
```




\clearpage
(ref:FitsCompCapPrior) Effect of alternative capacity priors on median Smsy estimates. We tested two to four alternative priors for Smax for the stock-level and aggregate-level fits of the Basic Ricker model (wide vs. capped, uniform vs. lognormal). The effect of alternative capacity priors is larger for more uncertain SR model fits (i.e., larger SIQR). SR model fits with red points are flagged if the difference  between median estimates is larger than 25% or the standardized interquartile range for the most uncertain fit is larger than 1 (i.e., the range between the lower and upper quartiles is larger than the median value).

```{r FitsCompCapPrior,   fig.cap="(ref:FitsCompCapPrior)"}
include_graphics("data/Convergence/FitsComparison_CapPriorEffect.png")
```


\clearpage
(ref:FitsCompAR1Pars) Comparison of posterior distributions for Basic Ricker and AR1 model fits - Parameters. Both model fits used the capped uniform capacity prior. Panels compare the width of posterior distributions (SIQR) between model forms for ln.alpha (A) and Smax (B). SR model fits are flagged with red points for n.alpha if either SIQR > 0.3 and flagged for Smax if either SIQR > 0.6.


```{r FitsCompAR1Pars,   fig.cap="(ref:FitsCompAR1Pars)"}
include_graphics("data/Convergence/FitsComparison_BRcuVsAR1_Pars.png")
```


\clearpage
(ref:FitsCompAR1Smsy) Comparison of posterior distributions for Basic Ricker and AR1 model fits - Smsy. Figures compare median estimates (A) and width of the posteriors (B) of posterior Smsy distributions from the alternative model forms. Model fits are flagged with red points if the difference in median estimates is larger than 25%. Aggregate fits are also labelled in Panel B. Note that Panel A compares estimates on a log scale to allow comparison across stocks with very different estimates, but stocks are flagged if the difference in original (i.e., unlogged) values is larger than 25%.

```{r FitsCompAR1Smsy,   fig.cap="(ref:FitsCompAR1Smsy)"}
include_graphics("data/Convergence/FitsComparison_BRcuVsAR1_Smsy.png")
```



\clearpage
(ref:FitsCompProd1) Time-varying productivity for 12 stocks with complete time series. Each panel shows the median and 80% bounds of year-specific posterior distributions of ln.alpha for the TVP model fit. Reference lines show the corresponding intrinsic productivity in terms of recruits per spawner (R/S) at very low spawner abundance (technically, at 0 spawners). 

```{r FitsCompProd1,   fig.cap="(ref:FitsCompProd1)"}
include_graphics("data/Convergence/FitsComparison_PROD_3_ProdPatterns_KF_All.png")
```




\clearpage
(ref:FitsCompProd2) Log residuals from the Basic Ricker fit for 12 stocks where a time-varying productivity TVP model was also fitted. Each panel shows median and 80% bounds for annual residuals and a 4yr running average trend line.

```{r FitsCompProd2,   fig.cap="(ref:FitsCompProd2)"}
include_graphics("data/Convergence/FitsComparison_PROD_3_RickerResids_Basic_All.png")
```



\clearpage
(ref:FitsCompProd3) Log residuals from the TVP model fit with time varying productivity for 12 stocks with complete time series. Each panel shows median and 80% bounds for annual residuals and a 4yr running average trend line.

```{r FitsCompProd3,   fig.cap="(ref:FitsCompProd3)"}
include_graphics("data/Convergence/FitsComparison_PROD_3_RickerResids_KF_All.png")
```






\clearpage



(ref:ScatterMezidian) Scatterplot of log productivity ln(R/S) vs. spawner abundance - Meziadin. Observations are colour-coded, with earlier data in fainter shading. The secondary axis illustrates the corresponding raw R/S values. 

```{r ScatterMezidian,   fig.cap="(ref:ScatterMezidian)"}
include_graphics("data/StockSampleFigs/Meziadin_RpS_ScatterPlot.png")
```


(ref:JoinPostMezidian) Joint posterior distributions for productivity parameter ln(alpha) and capacity parameter Smax (1/b): Meziadin, with PR-based capacity priors. Each panel shows the scatter of MCMC samples, contour lines for the joint distribution, and two depictions of the marginal distributions for each parameter: boxplot with median, quartiles, and 80% whiskers, and a kernel density plot. Dashed reference lines show the medians of the marginal distributions. Figure includes 1 panel for the Basic Ricker model fit (top left), one panel for the Ricker AR1 fit (top right), and two panels showing the brood years with the lowest and highest productivity (median ln.alpha) for the TVP model fit (bottom panels). The parameter samples for the TVP model have 1 set of posterior samples for Smax, and 1 set of ln.alpha posterior samples for each brood year. The bottom panels show the joint distribution of Smax and ln.alpha for 2 brood years, selected to capture the brood year with the lowest median ln.alpha and the highest median ln.alpha.

```{r JoinPostMezidian,   fig.cap="(ref:JoinPostMezidian)"}
include_graphics("data/StockSampleFigs/Meziadin_SR_Par_JointPosterior_3Modelcu.png")
```



(ref:ScatterBabLW) Scatterplot of log productivity ln(R/S) vs. spawner abundance - Babine Late Wild. Observations are colour-coded, with earlier data in fainter shading. The secondary axis illustrates the corresponding raw R/S values.

```{r ScatterBabLW,   fig.cap="(ref:ScatterBabLW)"}
include_graphics("data/StockSampleFigs/BabLW_RpS_ScatterPlot.png")
```


(ref:JoinPostBabLW) Joint posterior distributions for ln.a and b: Babine Late Wild. Each panel shows the scatter of MCMC samples, contour lines for the joint distribution, and two depictions of the marginal distributions for each parameter: boxplot with median, quartiles, and 80% whiskers, and a kernel density plot. Dashed reference lines show the medians of the marginal distributions. Figure includes 1 panel for the Basic Ricker model fit (top left), one panel for the Ricker AR1 fit (top right), and two panels showing the brood years with the lowest and highest productivity (median ln.alpha) for the TVP model fit (bottom panels). The parameter samples for the TVP model have 1 set of posterior samples for Smax, and 1 set of ln.alpha posterior samples for each brood year. The bottom panels show the joint distribution of Smax and ln.alpha for 2 brood years, selected to capture the brood year with the lowest median ln.alpha and the highest median ln.alpha.

```{r JoinPostBabLW,   fig.cap="(ref:JoinPostBabLW)"}
include_graphics("data/StockSampleFigs/Bab-LW_SR_Par_JointPosterior_3Modelcu.png")
```



\clearpage
## HIERARCHICAL SPAWNER-RECRUIT MODEL FITS (STOCK-LEVEL) {#HBMResultsComp}

### Model Versions Used for Comparison

The purpose of including the HBM analyses by McAllister and Challenger in this paper is to explore the potential benefits of including the hierarchical structure and sharing information across stocks. For this comparison to single-stock fits, all other elements of the model fits should be kept as similar as possible, at least for the starting point of the comparison. McAllister and Challenger document sensitivity tests of the Hierarchical Bayesian Model (HBM) in Appendix \@ref(app:HBMFits). Here we focus on comparing results between the single-stock fits and two versions of the HBM model fits:

* *HBM Base Case*: Used all stocks with data, including enhanced Pinkut and Fulton, with a shared year effect of productivity across all stocks, and more informative lognormal priors on Smax for several of the stocks (CV = 0.3).
* *HBM Senstivity Run 26*: Excluded Pinkut and Fulton, less informative lognormal priors for Bear, Kitwanga, and Sustut (CV = 2).


### Comparison of Biological Benchmark Estimates

We compared HBM estimates of biological benchmarks to single stock fits with the Basic Ricker model (capped uniform Smax prior). This comparison covered the largest number of stocks, because single-stock AR1 and TVP model fits could only be applied to stocks with complete time series.  

For most stocks, median Smsy estimates from the HBM Base Case were similar to the single stock Basic Ricker estimates, but for six stocks the difference was more than 25% (Figure \@ref(fig:HBMComp1)). After excluding the enhanced stocks and relaxing the HBM capacity prior on three stocks (Bear, Kitwanga, Sustut), all the estimates were similar between HBM and single-stock model fits (Figure \@ref(fig:HBMComp2)). In fact, Smsy estimates for HBM Run 26 and the single-stock Basic Ricker with capped uniform prior (Model BRcu) were more similar to each other than the Bayesian single stock estimate was to the simple deterministic estimate (Figure \@ref(fig:FitsCompDetFig)). 

Widths of posterior Smsy distributions were similar between HBM and single-stock model fits for many stocks, tighter (i.e., more precise) in the HBM estimates for some stocks, and wider in the HBM estimates for a few stocks (Figure \@ref(fig:HBMComp3)). Smsy estimates for three stocks stood out in both HBM and single-stock model fits as particularly uncertain (i.e., very wide posteriors): Kitwanga, Asitka, and Motase. 

While Smsy estimates were much more similar between HBM and single-stock model fits after the capacity prior was relaxed for some stocks (HBM Run 26), median estimates of Umsy remained quite different between the two types of model (Figure \@ref(fig:HBMComp4)). Median Umsy differed by more than 5% for seven stocks. HBM Run 26 estimated higher median Umsy (i.e., higher productivity) than the single stock Basic Ricker fit for Johnston, Asitka, Kitwanga, and Swan/Stephens. HBM Run 26 estimated lower median Umsy for Sustut, Bear, and Kitsumkalum. The range of median Umsy estimates across stocks was narrower for HBM Run 26 (44%-72%) than for the single stock Basic Ricker fit (41%-79%).



\clearpage
(ref:HBMComp1) Difference in median Smsy estimates for the HBM Base Case and the single stock Basic Ricker fit with capped uniform capacity priors. Stocks with differences larger than 25% are highlighted.

```{r HBMComp1,   fig.cap="(ref:HBMComp1)"}
include_graphics("data/HBM/FitsComparisonHBM_BRcuvsBaseCase_Smsy_PercDiff.png")
```



(ref:HBMComp2) Difference in median Smsy estimates for HBM Run 26 and the single stock Basic Ricker fit with capped uniform capacity priors. Stocks with differences larger than 25% are highlighted.

```{r HBMComp2,   fig.cap="(ref:HBMComp2)"}
include_graphics("data/HBM/FitsComparisonHBM_BRcuvsSensitivityRun26_Smsy_PercDiff.png")
```

\clearpage

(ref:HBMComp3) Comparison of Smsy estimates for HBM Run 26 and the single stock Basic Ricker fit with capped uniform capacity priors. (A) Comparison of median estimates. Stocks with differences in median estimates larger than 25% are highlighted. Panel A compares estimates on a log scale to allow comparison across stocks with very different estimates, but stocks are flagged if the difference in original (i.e., unlogged) values is larger than 25%. (B) Comparison of the standardized interquartile range (SIQR), which captures half of the posterior samples. Stocks with SIQR > 0.4 in either estimate are highlighted.

```{r HBMComp3,   fig.cap="(ref:HBMComp3)"}
include_graphics("data/HBM/FitsComparisonHBM_BRcuvsSensitivityRun26_Smsy.png")
```



(ref:HBMComp4) Comparison of Umsy estimates for HBM Run 26 and the single stock Basic Ricker fit with capped uniform capacity priors. Sidebars identify the range of median estimates from each model type. Stocks are highlighted if median Umsy differ by more than 5%. Note that the difference in actual values is used here, not the relative % difference used for Smsy comparisons in previous plots.

```{r HBMComp4,   fig.cap="(ref:HBMComp4)"}
include_graphics("data/HBM/FitsComparisonHBM_BRcuvsSensitivityRun26_Umsy.png")
```


\clearpage
### Comparison of eEstimated Productivity Changes Over Time (Single-stock vs. HBM Fits)

We compared HBM estimates of productivity over time to single stock fits with the TVP model (capped uniform Smax prior) for the nine wild Skeena stocks where both estimates were available. 

The shared year effect identified by the HBM model was very similar between the HBM Base Case, which included Pinkut and Fulton, and HBM Run 26, which excluded the enhanced stocks (Figure \@ref(fig:HBMComp5), Panel A). Both versions identified a sharp productivity drop in 1994, followed by a spike in 1995. The 4-year running mean of the HBM Base Case shared year effect identified a period of higher-than-average productivity in the 1980s and early 1990s, followed by a general decline since then (Panel B). Single stock TVP fits identified a similar decline in productivity since the 1990s for Babine Late Wild, the largest wild Skeena stock,  but other stocks had very different productivity trends over time (e.g., Panel D of Figure \@ref(fig:HBMComp5), Figure \@ref(fig:HBMComp6)). The three wild Babine stocks had very similar productivity trend over time in the single stock TVP fits. 

Both the HBM shared year effect and the single-stock TVP estimates of changes in productivity over time link back to the residuals from the Basic Ricker fit. Based on the residuals, the  1994/1995 dip and spike in the HBM shared year effect was driven by the residuals from the Babine stocks (Figure \@ref(fig:HBMComp6)). In the HBM Base Case the change over time was very pronounced, because it included Pinkut and Fulton, which had the disease outbreak. In HBM Run 26, without the enhanced stocks, this was less pronounced, but it still picked up the same signal from the 3 wild Babine stock residuals, and assumed that all the non-Babine stocks experienced the same dramatic changes in those 2 years. The residual 1994/1995 signal from the three wild Babine stocks needs to be interpreted carefully, because spawner and recruit estimates for all five Babine stocks are derived together from a total weir count, based on estimated stock proportions from tagging studies in the 1970s.


(ref:HBMComp5) Comparison of estimated productivity trends over time from HBM and single stock TVP model fits. (A) Annual shared year effect from the HBM model fits. Blue line and shaded area show median and 80% of the shared year effect from the HBM Base Case, the red overlayed line shows the shared year effect from HBM Run 26. (B) 4yr running mean of the shared year effect from the HBM Base Case. (C, D) Estimates of the time-varying productivity parameter ln.alpha from a single stock TVP model fit with capped uniform capacity prior for Babine Late Wild and Alastair. Medians and 80% bounds are shown.

```{r HBMComp5,   fig.cap="(ref:HBMComp5)"}
include_graphics("data/HBM/FitsComparison_PROD_1_SinglevsHBM.png")
```


(ref:HBMComp6) Changes in productivity parameter ln.alpha over time for nine wild Skeena stocks with complete SR time series. Estimates of the time-varying productivity parameter ln.alpha from a single stock TVP model fit with capped uniform capacity prior. Medians and 80% bounds are shown. 

```{r HBMComp6,   fig.cap="(ref:HBMComp6)"}
include_graphics("data/HBM/FitsComparison_PROD_2_ProdPatterns_KF_Skeena.png")
```




\clearpage
##  PRODUCTIVITY SCENARIOS (STOCK-LEVEL AND AGGREGATE-LEVEL)

### Scenario Descriptions

We worked through the considerations and steps outlined in Section \@ref(ModelSelection) to select spawner-recruit parameter sets for four alternative productivity scenarios from the suite of candidate single-stock SR model fits.  These scenarios were used for the remaining analyses in this report (Tables \@ref(tab:SelectedModelsTab) and \@ref(tab:SelectedModelsTabAgg)). 

Key considerations were:

* We wanted to highlight differences in productivity between stocks, so focused on single-stock model fits, rather than the hierarchical model fits that start with the assumption of a similar underlying productivity across stocks.
* Given that all candidate single-stock model fits converged for at least two of the three criteria (Section \@ref(Convergence)), we did not reject any of the fits based on statistical considerations.
* We wanted to highlight the implications of alternative productivity assumptions for each stock, so we generated three alternatives to the base case with long-term average productivity. 
* Where available, we used the AR1 model fit for the long-term average scenario. Otherwise, we used the Basic Ricker fit.
* Where available, we used year-specific ln.alpha estimates from the time-varying productivity (TVP) model for the alternative productivity scenarios. Otherwise, we subsampled parameter sets from the Basic Ricker fit.
* For stocks with time-varying productivity (TVP) model fits, we used the last available generation for the *Recent* productivity scenario. However, given discussions during the peer-review meeting [@SkeenaNassSkPRO;@SkeenaNassSkSAR], we also tested the effect of expanding the number of brood-years included in the *Recent* scenario to either two generations or 3 generations.

We anticipate that future processes, such as the on-going Canadian domestic engagement process, will identify additional candidate model fits,  explore alternative parameter selection criteria, and request additional productivity scenarios.  The analysis framework is set up to rapidly respond to these requests. All of the results shown in subsequent sections can be  easily re-generated with alternative MCMC parameter sets (e.g., different productivity assumptions, Hierarchical Bayesian Model fits), but we have decided to limit the number of examples included in this Research Document. 

\clearpage


(ref:SelectedModelsTab) Stock-level SR models selected for alternative scenarios.  Model fits are BR = Basic Ricker, AR1 = Ricker with lag-1 autoregression correction, TVP = Ricker model with time-varying productivity parameter alpha. All selected model fits used the capped uniform (*cu*) capacity prior. The range of brood years from which parameter sets were sampled is listed for TVP fits. For example, TVPcu2010-2014 denotes that 1/5th of the parameter samples were taken from each year in the period 2010 to 2014. For BR and AR1 fits, alternative scenarios are identified based on the percentile used for the adjusted median of the sample distribution. For example, BRcu(0.1) denotes that half the parameter samples for that scenario were taken from below the 10th percentile of the original distribution, and half from above. For stocks without TVP fits the recent productivity scenario matched the low productivity scenario if observed R/S clearly decreased in recent years, matched the high productivity scenarios if they increased, and matched the long-term average scenario if there was no clear trend in either direction. Note that three alternative versions of *Recent* productivity scenario for stocks with TVP fits were explored. All use the model fit, generation length, and end year specified in this table, but they include either one, two, or three generations. Section \@ref(AltProdResults) compares the parameter distributions. The examples in the rest of this Research Document are based on the 1-generation version of the *Recent* productivity scenario.

```{r SelectedModelsTab, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_SelectedModels.csv") %>% arrange(Aggregate,StkSeq) %>%
									select(-StkSeq,-Stock) %>% dplyr::filter(!is.na(Aggregate)) %>%
									dplyr::rename(Stock = StkNmS,Recent = Now,MU=Aggregate) %>%
									mutate_all(funs(str_replace(., "KF","TVP")))




table.in$MU[duplicated(table.in$MU)] <- ""

table.in[is.na(table.in)] <- "-"


table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "l",
                  caption = "(ref:SelectedModelsTab)" ) %>%
    kableExtra::row_spec(c(7,9), hline_after = TRUE) %>%
	add_header_above(c(" " = 2, "Scenarios" = 4)) 

```




\clearpage

(ref:SelectedModelsTabAgg) Aggregate-level SR models selected for alternative scenarios. Layout as per Table \@ref(tab:SelectedModelsTab). 

```{r SelectedModelsTabAgg, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_SelectedModelsAgg.csv") %>%			
	dplyr::rename(Recent = Now) %>%
									mutate_all(funs(str_replace(., "KF","TVP")))

table.in[is.na(table.in)] <- "-"




table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "l",
                  caption = "(ref:SelectedModelsTabAgg)" ) %>%
    #kableExtra::row_spec(c(7,9), hline_after = TRUE) %>%
	add_header_above(c(" " = 1, "Scenarios" = 4)) 

```



### Comparison of Parameter Distributions for Productivity Scenarios {#AltProdResults}

Productivity parameters for the *recent* scenario, generated as per Table \@ref(tab:SelectedModelsTab), have lower median productivity for about 1/3 of the stocks, and wider distributions (i.e., larger uncertainty, lower confidence) for most of the stocks (Table \@ref(tab:ProdCompTab1), Figures \@ref(fig:ProdComp1) and \@ref(fig:ProdComp2)). Stocks with lower productivity in the *Recent* scenario include Meziadin, the largest Nass stock, and all three wild Babine stocks (Early, Mid, and Late Wild). Four small stocks, each accounting for less than 2% of cumulative spawner abundance since 2000, have higher *Recent* productivity: Alastair, Bear, Slamgeesh, and Johnston. Three stocks are notable for having both much lower productivity, with a % difference in median ln.alpha between scenarios larger than -35%, and a distribution that is more than twice as wide (i.e., % difference in SIQR > 100): Babine Late Wild, Babine Early Wild, Meziadin, and Kitwanga (Figure \@ref(fig:ProdComp2)).


Alternative versions of the *recent* productivity scenario, using either two or three generations instead of only the last generation, have similar median productivity and similar spread for about 1/2 of the stocks (Table \@ref(tab:ProdCompTab2), Figures \@ref(fig:ProdComp3) and \@ref(fig:ProdComp3)). This includes two of the three wild Babine stocks (Mid and Late Wild). For 1/3 of the stocks, median productivity is higher, and the distribution is narrower if additional brood years are included in the definition of the *recent* scenario. These include Meziadin, the largest Nass stock, and one of the wild Babine stocks (Babine Early Wild). For two stocks, median productivity is lower and more uncertain: Morice and Swan/Stephens. Corresponding Smsy estimates change similarly due to the alternative time windows used for the recent productivity scenario (Table \@ref(tab:ProdCompBMRatioSmsy)).






\clearpage
(ref:ProdCompTab1) Estimated productivity parameters ln.alpha for the long-term average and recent productivity scenarios. For each scenario, the table lists median (Med) and standardized inter-quartile range (SIQR, range between p25 and p75) for the Bayesian posterior parameter subsamples generated as per Table \@ref(tab:SelectedModelsTab). The last two columns show percent difference (*PercDiff*) between the scenarios. Stocks are sorted by *PercDiff* in median estimate. Horizontal lines separate the stocks into three groups based on percent difference in median ln.alpha: more than ~ 10% decrease (top), more than ~ 10% increase  (bottom), or less than ~ 10% change in either direction (middle). Note that table includes only wild stocks.

```{r ProdCompTab1, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_ProdChange.csv") %>%		
	dplyr::filter(!(Stock %in% c("Skeena","SkeenaWild","Nass","Pinkut","Fulton"))) %>%
	dplyr::select(Stock,Median.LTAvg,SIQR.LTAvg,Median.Curr,SIQR.Curr,
								Median.PercChange,	SIQR.PercChange) %>%
	arrange(Median.PercChange) %>%
	mutate_if(is.numeric, round,2)
	

lines.idx <- c( max(which(table.in$Median.PercChange < -9.5)),
								min(which(table.in$Median.PercChange > 9.5))-1)

	
table.in$Stock <- gsub("Lower Nass Sea & River Type","L Nass SRT",table.in$Stock)

table.in[is.na(table.in)] <- "-"


col.names.use <-  c("Stock",rep(c("Med","SIQR"),3))



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
									col.names = col.names.use ,
                  caption = "(ref:ProdCompTab1)" ) %>%
	 kableExtra::column_spec(6, bold = T) %>%
    kableExtra::row_spec(c(lines.idx), hline_after = TRUE) %>%
	add_header_above(c(" " = 1, "LTAvg" = 2,"Recent" = 2,"PercDiff" = 2)) 

```




\clearpage

(ref:ProdComp1) Estimated median productivity (ln.alpha) for the long-term average and recent productivity scenarios. Medians are for the Bayesian posterior parameter subsamples generated as per Table \@ref(tab:SelectedModelsTab). Stocks falling on the solid red line have the same median for both scenarios. For stocks below the solid red line, recent productivity is lower than long-term average productivity. For stocks between the dashed red lines, the difference in productivity scenarios is less than 25%. Note that figure includes only wild stocks.

```{r ProdComp1,   fig.cap="(ref:ProdComp1)"}
include_graphics("data/RecentProd_Comparison_LTavgvs1genv_Values.png")
```




\clearpage

(ref:ProdComp2) Differences in median and spread of the productivity parameter (ln.alpha) for the long-term average and recent productivity scenarios. Points show the percent change in the standardized inter-quartile range (SIQR, range between p25 and p75) vs. the change in median ln.alpha. Horizontal and vertical red lines mark "no change". Note that figure includes only wild stocks.


```{r ProdComp2,   fig.cap="(ref:ProdComp2)"}
include_graphics("data/RecentProd_Comparison_LTavgvs1gen_Diffs.png")
```




\clearpage
(ref:ProdCompTab2) Estimated productivity parameters ln.alpha for three versions of the recent productivity scenario (1,2, or 3 generations). For each scenario, the table lists median (Med) and standardized inter-quartile range (SIQR, range between p25 and p75) for the Bayesian posterior parameter subsamples generated as per Table \@ref(tab:SelectedModelsTab). Table also shows percent difference (*PercDiff*) between the alternative versions. Stocks are sorted by *PercDiff* in median estimate between the 2-generation and 1-generation versions. Horizontal lines separate the stocks into three groups based on percent difference in median ln.alpha between the 2-generation and 1-generation versions: more than ~ 5% decrease (top), more than ~ 5% increase  (bottom), or less than ~ 5% change in either direction (middle). Note that table includes only wild stocks. Table \@ref(tab:ProdCompBMRatioSmsy) compares the corresponding Smsy estimates.

```{r ProdCompTab2, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_ProdChange.csv") %>%		
	dplyr::filter(!(Stock %in% c("Skeena","SkeenaWild","Nass","Pinkut","Fulton"))) %>%
	dplyr::select(Stock,Median.CurrLS,SIQR.CurrLS,Median.CurrLS2,SIQR.CurrLS2,
								Median.PercChange2,	SIQR.PercChange2,
								Median.CurrLS3,SIQR.CurrLS3,
								Median.PercChange3,	SIQR.PercChange3) %>%
	arrange(Median.PercChange2) %>%
	mutate_if(is.numeric, round,2)
	
	
table.in$Stock <- gsub("Lower Nass Sea & River Type","L Nass SRT",table.in$Stock)

table.in[is.na(table.in)] <- "-"


lines.idx <- c( max(which(table.in$Median.PercChange2 < -4.99)),
								min(which(table.in$Median.PercChange2 > 4.99))-1)

	

col.names.use <-  c("Stock",rep(c("Med","SIQR"),5))



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
									col.names = col.names.use ,
                  caption = "(ref:ProdCompTab2)" ) %>%
	 kableExtra::column_spec(6, bold = T) %>%
     kableExtra::row_spec(c(lines.idx), hline_after = TRUE) %>%
	add_header_above(c(" " = 1, "1 Gen" = 2,"2 Gen" = 2,"PercDiff\n(2 Gen vs 1 Gen)" = 2,
										 "3 Gen" = 2,"PercDiff\n(3 Gen vs 1 Gen)" = 2)) 

```





\clearpage
(ref:ProdCompBMRatioSmsy) Comparison of Median Smsy estimates across productivity scenarios. Layout as per Table \@ref(tab:ProdCompTab2). Median Smsy estimates are in 1,000s (columns 2,4,and 8).

```{r ProdCompBMRatioSmsy, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_SmsyChange.csv") %>%		
	dplyr::filter(!(Stock %in% c("Skeena","SkeenaWild","Nass","Pinkut","Fulton"))) %>%
	dplyr::select(Stock,Median.CurrLS,SIQR.CurrLS,Median.CurrLS2,SIQR.CurrLS2,
								Median.PercChange2,	SIQR.PercChange2,
								Median.CurrLS3,SIQR.CurrLS3,
								Median.PercChange3,	SIQR.PercChange3) %>%
	arrange(Median.PercChange2) %>%
	mutate_if(is.numeric, round,2)
	
	
table.in$Stock <- gsub("Lower Nass Sea & River Type","L Nass SRT",table.in$Stock)

table.in[is.na(table.in)] <- "-"


lines.idx <- c( max(which(table.in$Median.PercChange2 < -4.99)),
								min(which(table.in$Median.PercChange2 > 4.99))-1)

	

col.names.use <-  c("Stock",rep(c("Med","SIQR"),5))



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
									col.names = col.names.use ,
                  caption = "(ref:ProdCompBMRatioSmsy)" ) %>%
	 kableExtra::column_spec(6, bold = T) %>%
     kableExtra::row_spec(c(lines.idx), hline_after = TRUE) %>%
	add_header_above(c(" " = 1, "1 Gen" = 2,"2 Gen" = 2,"PercDiff\n(2 Gen vs 1 Gen)" = 2,
										 "3 Gen" = 2,"PercDiff\n(3 Gen vs 1 Gen)" = 2)) 

```







\clearpage

(ref:ProdComp3) Estimated median productivity (ln.alpha) for the 1-generation and 2-generation versions of the recent productivity scenario. Medians are for the Bayesian posterior parameter subsamples generated as per Table \@ref(tab:SelectedModelsTab). Stocks falling on the solid red line have the same median for both scenarios. For stocks below the solid red line, the 1-generation version has lower productivity than the 2-generation version. For stocks between the dashed red lines, the difference between alternative versions is less than 25%. Note that figure includes only wild stocks.

```{r ProdComp3,   fig.cap="(ref:ProdComp3)"}
include_graphics("data/RecentProd_Comparison_1genvs2Gen_Values.png")
```



\clearpage

(ref:ProdComp4) Differences in median and spread of the productivity parameter (ln.alpha) for the  1-generation and 2-generation versions of the recent productivity scenario. Points show the percent change in the standardized inter-quartile range (SIQR, range between p25 and p75) vs. the change in median ln.alpha. Horizontal and vertical red lines mark "no change". Note that figure includes only wild stocks.

```{r ProdComp4,   fig.cap="(ref:ProdComp4)"}
include_graphics("data/RecentProd_Comparison_1genvs2Gen_Diffs.png")
```


\clearpage
##  BIOLOGICAL BENCHMARK ESTIMATES (STOCK-LEVEL AND AGGREGATE-LEVEL) {#BMResults}


### Illustration of Stock-level Results: Meziadin and Babine Late Wild

Standard biological benchmarks for the largest wild stock from each aggregate differed substantially between alternative productivity scenarios. 

Comparing the recent productivity scenario to the long-term average  productivity scenario for Meziadin (Figure \@ref(fig:BMMezidian)) and Babine Late Wild (Figure \@ref(fig:BMBabLW)):

* The recent productivity scenario had  lower and more uncertain estimates of productivity (ln.alpha) and lower estimates of capacity (Smax), which resulted in lower estimates of Smsy, Seq, and Umsy. 
* Estimates of Sgen, which is linked to both the productivity estimate and the value of Smsy, increased for Meziadin and decreased for Babine Late Wild, but note that in both cases Sgen increased as a relative proportion of Smsy (i.e., with lower productivity need more spawners to build back to Smsy in one generation, but if Smsy is much lower, then Sgen can actually drop relative to the long-term average scenario).


(ref:BMMezidian) Posterior distributions of productivity parameter ln.alpha and biological benchmark estimates: Meziadin. Each panel shows the posterior distribution (median, quartiles, 80% bounds) for two productivity scenarios: long-term average (LTAvg) and recent productivity. Two versions of the parameter estimates are shown: regular (R) and with log-normal bias correction (C) on the productivity parameter ln.alpha.

```{r BMMezidian,   fig.cap="(ref:BMMezidian)"}
include_graphics("data/StockSampleFigs/Meziadin_BM_SummaryPlot.png")
```



\clearpage
(ref:BMBabLW) Posterior distributions of biological benchmark estimates: Babine Late Wild. Layout as per Figure \@ref(fig:BMMezidian).

```{r BMBabLW,   fig.cap="(ref:BMBabLW)"}
include_graphics("data/StockSampleFigs/Bab-LW_BM_SummaryPlot.png")
```

\clearpage
### Stock-level and Aggregate-level Biological Benchmarks for Nass Sockeye  

*Abundance benchmarks*

Comparing aggregate to stock-level Smsy estimates for Nass Sockeye under long-term average productivity (Table \@ref(tab:SmsyLtAvgNass)):

* The aggregate-level Smsy estimate for Nass Sockeye was substantially larger than the sum of stock-level estimates for those stocks where SR models were fitted. The four stocks with stock-level estimates are assumed to account for most of the Sockeye production from the Nass, but see notes regarding Bowser in Section \@ref(PrioritiesFuture). 

Comparing aggregate to stock-level Smsy estimates for Nass Sockeye under recent productivity (Table \@ref(tab:SmsyRecentNass)):

* The aggregate-level Smsy estimate was larger than the sum of stock-level estimates for four modelled stocks, but the difference was smaller than under the long-term average productivity scenario.

Comparing aggregate and stock-level benchmark estimates under recent productivity to the long-term average productivity scenario  (Table \@ref(tab:SmsyLtAvgNass) vs. Table \@ref(tab:SmsyRecentNass), Table \@ref(tab:SmaxLtAvgNass) vs. Table \@ref(tab:SmaxRecentNass), Table \@ref(tab:SgenLtAvgNass) vs. Table \@ref(tab:SgenRecentNass)):

* The aggregate estimate, and the Meziadin Smsy estimate are much lower under recent productivity. 
* The Smsy estimate for the second-largest stock, Lower Nass Sea and River Type Sockeye, also dropped under recent productivity, but much less.  
* The same general differences were observed for estimates of Smax. 
* Under the recent productivity scenario, Sgen estimates increased for Meziadin and Kwinageese and decreased for the other two stocks with SR data, so that there was little change in the sum of Sgen estimates due to alternative productivity assumptions. 


Sgen is not applicable for stock aggregates, so we did not include a comparison between aggregate and stock-level Sgen estimates. 

*Umsy*

Aggregate-level estimates of Umsy closely matched the median of stock-level median estimates under both productivity scenarios, but differed from the median Umsy estimate for Meziadin, the largest stock (Tables \@ref(tab:UmsyLtAvgNass) and \@ref(tab:UmsyRecentNass)). Under the recent productivity scenario, Umsy for Meziadin is the lowest among the 4 modelled Nass stocks and about 10% lower than the aggregate fit (55% for the aggregate, 45% for Meziadin).


\clearpage

(ref:SmsyLtAvgNass) Comparison of aggregate and stock-level Smsy estimates: Nass / Long-term average productivity. Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit. 

```{r SmsyLtAvgNass, echo = FALSE, results = "asis"}

abd.bm.tab.src <- read_csv("data/SummaryTables_AbundanceBM.csv") %>% select(-Stock)

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Smsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyLtAvgNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SmsyRecentNass) Comparison of aggregate and stock-level Smsy estimates: Nass / Recent productivity. Stocks are sorted based on median estimate. 

```{r SmsyRecentNass, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Smsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyRecentNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```




\clearpage

(ref:SmaxLtAvgNass) Comparison of aggregate and stock-level Smax estimates: Nass / Long-term average productivity.  Stocks are sorted based on median estimate. 

```{r SmaxLtAvgNass, echo = FALSE, results = "asis"}



table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Smax") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmaxLtAvgNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SmaxRecentNass) Comparison of aggregate and stock-level Smax estimates: Nass / Recent productivity.  Stocks are sorted based on median estimate. 

```{r SmaxRecentNass, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Smax") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmaxRecentNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```






\clearpage

(ref:SgenLtAvgNass) Comparison of aggregate and stock-level Sgen estimates: Nass / Long-term average productivity.  Stocks are sorted based on median estimate. 

```{r SgenLtAvgNass, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Sgen") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenLtAvgNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SgenRecentNass) Comparison of aggregate and stock-level Sgen estimates: Nass / Recent productivity.  Stocks are sorted based on median estimate. 

```{r SgenRecentNass, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Sgen") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenRecentNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



\clearpage

(ref:UmsyLtAvgNass) Comparison of aggregate and stock-level Umsy estimates: Nass / Long-term average productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyLtAvgNass, echo = FALSE, results = "asis"}

umsy.bm.tab.src <- read_csv("data/SummaryTables_UMSY.csv") %>% select(-Stock)

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Umsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"


table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyLtAvgNass)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```



(ref:UmsyRecentNass) Comparison of aggregate and stock-level Umsy estimates: Nass / Recent productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyRecentNass, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Umsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyRecentNass)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```





\clearpage
### Stock-level and Aggregate-level Biological Benchmarks for Skeena Wild Sockeye

*Abundance benchmarks*

Comparing aggregate to stock-level Smsy estimates for Skeena Sockeye under long-term average productivity (Table \@ref(tab:SmsyLtAvgSkeenaWild)):

* The aggregate-level Smsy estimate for Skeena Sockeye was lower than the sum of stock-level estimates for those stocks where SR models were fitted. The 16 stocks with stock-level estimates are assumed to account for most of the Sockeye production from the Skeena. 

Comparing aggregate to stock-level Smsy estimates for Skeena Sockeye under recent productivity (Table \@ref(tab:SmsyRecentNass)):

* The aggregate-level Smsy estimate was much lower than the sum of stock-level estimates for 16 modelled stocks, and the difference was larger than under the long-term average productivity scenario.

Comparing aggregate and stock-level benchmark estimates under recent productivity to the long-term average productivity scenario  (Table \@ref(tab:SmsyLtAvgSkeenaWild) vs. Table \@ref(tab:SmsyRecentSkeenaWild), Table \@ref(tab:SmaxLtAvgSkeenaWild) vs. Table \@ref(tab:SmaxRecentSkeenaWild), Table \@ref(tab:SgenLtAvgSkeenaWild) vs. Table \@ref(tab:SgenRecentSkeenaWild)):

* The aggregate estimate, and the Babine Late Wild Smsy estimate, are much lower under recent productivity. 
* The same general differences were observed for estimates of Smax.
* Under the recent productivity scenario, Sgen estimates decreased for most of the stocks, and the sum of Sgen estimates decreased substantially. 

Sgen is not applicable for stock aggregates, so we did not include a comparison between aggregate and stock-level Sgen estimates. 

*Umsy*

Aggregate-level estimates of Umsy differed substantially from the median of stock-level median estimates under both productivity scenarios (Tables \@ref(tab:UmsyLtAvgSkeenaWild) and \@ref(tab:UmsyRecentSkeenaWild)). The median of stock-level estimates was much higher, because large stocks and small stocks were weighted equally in the median calculcation. Aggregate Umsy estimates were closer to the Umsy estimates for the largest stocks (Babine Early, Mid, and Late Wild).



\clearpage

(ref:SmsyLtAvgSkeenaWild) Comparison of aggregate and stock-level Smsy estimates: SkeenaWild / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyLtAvgSkeenaWild, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Smsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyLtAvgSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
(ref:SmsyRecentSkeenaWild) Comparison of aggregate and stock-level Smsy estimates: SkeenaWild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyRecentSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Smsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyRecentSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```




\clearpage

(ref:SmaxLtAvgSkeenaWild) Comparison of aggregate and stock-level Smax estimates: SkeenaWild / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmaxLtAvgSkeenaWild, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Smax") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmaxLtAvgSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
(ref:SmaxRecentSkeenaWild) Comparison of aggregate and stock-level Smax estimates: SkeenaWild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmaxRecentSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Smax") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmaxRecentSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```






\clearpage

(ref:SgenLtAvgSkeenaWild) Comparison of aggregate and stock-level Sgen estimates: SkeenaWild / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SgenLtAvgSkeenaWild, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Sgen") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenLtAvgSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```

\clearpage

(ref:SgenRecentSkeenaWild) Comparison of aggregate and stock-level Sgen estimates: SkeenaWild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added.  

```{r SgenRecentSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Sgen") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenRecentSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```




\clearpage

(ref:UmsyLtAvgSkeenaWild) Comparison of aggregate and stock-level Umsy estimates: Skeena Wild / Long-term average productivity.  Table also lists the range and median across stock-level estimates. 

```{r UmsyLtAvgSkeenaWild, echo = FALSE, results = "asis"}


table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Umsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyLtAvgSkeenaWild)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```

\clearpage

(ref:UmsyRecentSkeenaWild) Comparison of aggregate and stock-level Umsy estimates: Skeena Wild / Recent productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyRecentSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Umsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyRecentSkeenaWild)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```




\clearpage
## COMPARISON OF SUSTAINABLE EXPLOITATION RATE ACROSS STOCKS AND SCENARIOS {#UmsyComp}

Observed differences in estimated productivity (ln.alpha) between stocks and between scenarios translate into large differences in estimates of Umsy, defined as the sustainable harvest mortality rate at MSY (Figure \@ref(fig:AggUmsy)). Aggregate-level Umsy estimates were higher for the Nass aggregate than for the Skeena Wild aggregate, and Umsy under recent productivity was much lower than under long-term average productivity for both aggregates. Umsy estimates varied substantially between the component stocks for each aggregate. The SkeenaWild aggregate includes more stocks than the Nass aggregate, and median stock-level Umsy estimates spanned a wider range. Umsy under recent productivity was lower  and more uncertain for most stocks. The ranking of stocks by productictivity also differed between the long-term average and recent productivity scenario. Tables \@ref(tab:UmsyLtAvgNass), \@ref(tab:UmsyRecentNass), \@ref(tab:UmsyLtAvgSkeenaWild), and \@ref(tab:UmsyRecentSkeenaWild) above list the corresponding estimates. Appendix \@ref(BiasCorrectedBM) lists the bias-corrected versions of the estimates.

Umsy distributions can be considered relative to different levels of target exploitation rate for a stock aggregate:

* Figure \@ref(fig:UmsyProfiles1) shows the Prop(ER < Umsy), the proportion of the posterior Umsy distribution for each stock which exceeds ERs from 0 to 100% (i.e., the probability that a particular aggregate ER is sustainable over the long run for each component stock). This is the same information as Figure \@ref(fig:AggUmsy), just expressed in a different way: this version shows how much of the boxplots in Figure \@ref(fig:AggUmsy) falls to the right of each ER level. At 40% ER applied over the long run, two wild stocks have less than 50% probability of being harvested sustainably (two lines below the horizontal red line) under the long-term average productivity scenario, which increases to four wild stocks under the recent productivity scenario.
* Figure \@ref(fig:UmsyProfiles2) summarizes this information across stocks. It shows how many stocks have at least 50% probability of being sustainably harvested at each ER. At 40% ER, two wild stocks don't meet this objective under the long-term average productivity scenario (18 of 20 stocks do meet it), which increases to four wild stocks under the recent productivity scenario (16 of 20 wild stocks = 80%  of stocks meet the objective).
* Figures \@ref(fig:UmsyHeatmapLTAvg) and \@ref(fig:UmsyHeatmapRecent) show the details by stock. At 40% ER, the two stocks with less than 50% probability of meeting this objective under the long-term average productivity scenario are Kitwanga and Swan/Stephens. Under the recent productivity scenario, the list of stocks not meeting this objective also includes Babine Early Wild and Babine Late Wild.
* Figure  \@ref(fig:UmsyHisto) shows the frequency distribution of stock-specific median Umsy estimates. This type of plot was presented by @Waltersetal2008ISRP in their review of Skeena Sockeye (their Figure 14), so we've included it here for comparison. For Skeena Wild,  the spread of median Umsy estimates across component stocks is wider under the recent productivity scenario (i.e., the mixed-stock fishery challenge is more pronounced).  For Nass, there is an overall shift to lower median Umsy for the component stocks.

The different types of plots in Figures \@ref(fig:AggUmsy) to \@ref(fig:UmsyHisto) all show the same underlying information, just presented differently. However, these alternative displays capture alternative perspectives, and no single version will be informative for all participants in a planning process.

\clearpage
(ref:AggUmsy) Comparison of aggregate and stock-level Umsy estimates across stocks and aggregates for two alternative productivity assumptions. Stocks within an aggregate are sorted based on median Umsy. The largest stock in each aggregate is highlighted with red horizontal lines.

```{r AggUmsy,   fig.cap="(ref:AggUmsy)"}
include_graphics("data/UmsyPlots/Agg_Umsy_Comparison.png")
```


\clearpage
(ref:UmsyProfiles1) Probability profiles of sustainable ER. Proportion of Bayesian posterior Umsy estimates that exceeds different levels of ER for 20 wild stocks. Red horizontal line marks 50% probability. The ER where a line for a stock crosses the red line corresponds to the median estimate of Umsy. The largest stock from each aggregate is highlighted. 

```{r UmsyProfiles1,   fig.cap="(ref:UmsyProfiles1)"}
include_graphics("data/UmsyPlots/ResDoc_Umsy_Profile_Figure.png")
```


\clearpage
(ref:UmsyProfiles2) Summary of sustainable ER across 20 wild stocks. Number of stocks with at least 50% probability of being harvested sustainably.

```{r UmsyProfiles2,   fig.cap="(ref:UmsyProfiles2)"}
include_graphics("data/UmsyPlots/ResDoc_Umsy_Profile_Figure2.png")
```



\clearpage
(ref:UmsyHeatmapLTAvg) Umsy comparison across stocks - Long-term average productivity. Each column shows estimates for a fixed aggregate ER rate. Each cell in the table shows the probability of that ER being sustainable over the long run (i.e., ER < Umsy). Probabilities are categorized using the Intergovernmental Panel on Climate Change (IPCC) Likelihood Scale to facilitate discussion of results (Table \@ref(tab:IPPCLikelihoodTab)). Note that this figure shows the same information as the top panel of Figure \@ref(fig:UmsyProfiles1), just in more detail. Stocks are grouped by aggregate, and roughly sorted within aggregate from mouth of the river upstream. Grey shading indicates stocks that either lack SR data (e.g., Oweegee, Sicintine) or are enhanced (Pinkut, Fulton).


```{r UmsyHeatmapLTAvg,   fig.cap="(ref:UmsyHeatmapLTAvg)"}
include_graphics("data/UmsyPlots/Agg_Heatmap_FixedER_UmsyComparison_LTAvgProd.png")
```



\clearpage
(ref:IPCCscale) IPCC Likelihood scale from @IPCCscale and colour coding used in this paper.


```{r IPPCLikelihoodTab, echo = FALSE, results = "asis"}

ipcc.df <- read.csv("data/Reference Tables/IPCC_LikelihoodScale.csv",stringsAsFactors = FALSE)


ipcc.df  %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)})%>%
   mutate_all(function(x){gsub("NA", "", x)})%>%
   csas_table(format = "latex", escape = FALSE, font_size = 9,
                  caption = "(ref:IPCCscale)") %>%
   #kableExtra::column_spec(1, width = "15em") %>%
   kableExtra::row_spec(1:(dim(ipcc.df)[1] -1), hline_after = TRUE)
   #kableExtra::column_spec(2, width = "8em") %>%
   #kableExtra::column_spec(3, width = "5em") %>%
   #kableExtra::column_spec(4, width = "7em") %>%
   #kableExtra::column_spec(5, width = "4em") %>%
   #kableExtra::column_spec(6, width = "14em") 


```





\clearpage
(ref:UmsyHeatmapRecent) Umsy comparison across stocks - Recent productivity. Layout as per Figure \@ref(fig:UmsyHeatmapLTAvg).

```{r UmsyHeatmapRecent,   fig.cap="(ref:UmsyHeatmapRecent)"}
include_graphics("data/UmsyPlots/Agg_Heatmap_FixedER_UmsyComparison_RecentProd.png")
```



\clearpage
(ref:UmsyHisto) Umsy frequency plot. Each panel shows the distribution of stock-specific median Umsy estimates, rounded to the nearest 5%, with the 16 modelled stock in the Skeena Wild aggregate  in the top row, and the 4 modelled stocks in the Nass aggregate in the bottom row. Adapted from Figure 14 in @Waltersetal2008ISRP.

```{r UmsyHisto,   fig.cap="(ref:UmsyHisto)"}
include_graphics("data/UmsyPlots/Umsy_Histograms.png")
```









\clearpage
##  EQUILIBRIUM PROFILES (STOCK-LEVEL AND AGGREGATE-LEVEL)  {#ProfileResults}

### Examples of Spawner-based Equilibrium Profiles

Equilibrium profiles of expected yield or recruitment at different levels of spawner abundance (assumed to be a fixed escapement target) have been used as a building block for setting escapement goals in Alaskan and northern transboundary salmon fisheries (Section \@ref(EqProfilesMethods)). 

We illustrate the approach for the largest stock in each aggregate, using three alternative versions of a yield profile, and then summarize the aggregate-level and stock-level results for one commonly used version, the "80-60 range", which captures the range of spawner abundances with an 80% probability of achieving at least 60% of median MSY on average over the long term, if the stock were managed to a fixed escapement goal in that range.

*Meziadin*: The all-year median spawner abundance is very close to the median estimate of Smsy under the long-term average productivity scenario, but most years since 2000 were at or below the long-term average Smsy (Figure \@ref(fig:ProfileMezidian), Panel A). Under long-term average productivity, spawner abundances near the median Smsy estimate had a roughly 80% probability of achieving at least 80% of long-term average MSY, but under the recent productivity scenario no spawner abundance was likely to achieve that objective (Panel B). For objectives with lower targets (achieve at least 60% of MSY, achieve an equilibrium yield of 100,000 fish), the ranges of spawner abundances with an 80% probability of meeting the objective under long-term average productivity was wider (Panels C, D).  Probabilities of achieving these lower objectives were higher under recent productivity, but still didn't meet the 80% threshold we used for illustration.

*Babine Late Wild*: The all-year median spawner abundance is below the median estimate of Smsy under the long-term average productivity scenario, and most years since 2000 were below the long-term average Smsy (Figure \@ref(fig:ProfileBabineLW), Panel A). The observed differences in yield profiles across alternative productivity scenarios and objectives for Babine Late Wild were similar to the Meziadin profiles (Panels B-D).

 "80-60" yield ranges could be calculated for both aggregates and most of the stocks under the long-term average productivity, but only a few stocks met the 80% threshold under the recent productivity scenario (Tables \@ref(tab:ProfTab8060Nass) and \@ref(tab:ProfTab8060SkeenaWild)).


(ref:ProfileMezidian) Sample yield profiles: Meziadin (Largest Nass stock). Observed spawner abundances (A) and three alternative equlibrium yield profiles (B, C, D). Yield profiles are shown for the long-term average productivity scenario (LtAvg) and the recent productivity scenario (Recent). Boxplots in panel A show distributions for either all years or by decade, with number of observations in brackets. Each boxplot shows median (vertical line), lower and upper quartiles (box) and the range between smallest and largest observations (x).  

```{r ProfileMezidian,   fig.cap="(ref:ProfileMezidian)"}
include_graphics("data/StockSampleFigs/Meziadin_EqProbProfiles.png")
```





\clearpage
(ref:ProfileBabineLW) Sample Yield Profiles: Babine Late Wild (Largest wild Skeena stock). Layout as per Figure \@ref(fig:ProfileMezidian).

```{r ProfileBabineLW,   fig.cap="(ref:ProfileBabineLW)"}
include_graphics("data/StockSampleFigs/Bab-LW_EqProbProfiles.png")
```



\clearpage



(ref:ProfTab8060Nass) Summary of "80-60" yield profiles - Nass. For the long-term average productivity scenario, the table lists median Smsy (Smsy), median yield at Smsy (MSY), 60% of MSY, the range of spawner abundances with 80% probability of achieving 60% MSY (Lower, Upper), as well as median recruits and median productivity in that spawner range (Rec, RpS). For the recent productivity scenario, spawner range, recruits, and productivity are also listed for those cases where there is at least an 80% of achieving 60% of the long-term average MSY.

```{r ProfTab8060Nass, echo = FALSE, results = "asis"}

prof.tab.src <- read_csv("data/SummaryTables_Profiles8060.csv")

table.in <-  prof.tab.src %>% dplyr::filter(Basin=="Nass") %>% arrange(StkSeq) %>% select(-StkSeq,-Basin,-Stock) %>%
														mutate_at(c(2:7,9:11),function(x){prettyNum(round(x), big.mark=",")}) %>%
														mutate_at(c(8,12),function(x){format(round(x,1),n.small =1)}) 
	
col.names.use <- c("Stock","Smsy","MSY","60\\% MSY","Lower","Upper","Rec","RpS","Lower","Upper","Rec","RpS")
table.in[table.in == "NA"] <- "-"
table.in[table.in == " NA"] <- "-"
	

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",11)),
                  caption = "(ref:ProfTab8060Nass)", col.names = col.names.use ) %>%
    kableExtra::row_spec(c(1), hline_after = TRUE) %>%
	add_header_above(c(" ", "LtAvg" = 7, "Recent" = 4)) 

```



(ref:ProfTab8060SkeenaWild) Summary of sample yield profiles - SkeenaWild.  Layout as per Table \@ref(tab:ProfTab8060Nass).

```{r ProfTab8060SkeenaWild, echo = FALSE, results = "asis"}

table.in <-  prof.tab.src %>% dplyr::filter(Basin=="Skeena",!(Stock %in% c("Pinkut", "Fulton","Skeena"))) %>% arrange(StkSeq) %>% select(-StkSeq,-Basin,-Stock) %>%
							mutate_at(c(2:7,9:11),function(x){prettyNum(round(x), big.mark=",")}) %>%
														mutate_at(c(8,12),function(x){format(round(x,1),n.small =1)}) 
	
col.names.use <- c("Stock","Smsy","MSY","60\\% MSY","Lower","Upper","Rec","RpS","Lower","Upper","Rec","RpS")
table.in[table.in == "NA"] <- "-"
table.in[table.in == " NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",11)),
                  caption = "(ref:ProfTab8060SkeenaWild)", col.names = col.names.use ) %>%
    kableExtra::row_spec(c(1), hline_after = TRUE) %>%
	add_header_above(c(" ", "LtAvg" = 7, "Recent" = 4)) 

```




\clearpage
### Examples of ER-based Equilibrium Profiles


Equilibrium yield and spawner abundance at different levels of exploitation rate (assumed to be a fixed ER target) have been used to develop aggregate trade-off plots for various salmon stock aggregates and mixed-stock fisheries, including Skeena Sockeye (Section \@ref(EqProfilesMethods)). 

We illustrate the approach for the 16 modelled wild stocks in the Skeena aggregate  and the 4 modelled wild stocks in the Nass aggregate.


*Skeena Wild* (Figure \@ref(fig:ERBasedProfileSkeenaWild), Table \@ref(tab:ProfTabERbasedNass))


Aggregate equilibrium catch is largest for exploitation rate around 50% under the long-term average productivity scenario, and around 40% under the recent productivity scenario. The number of spawners and catch at equilibrium under these exploitation rates are very different depending on the productivity assumption. Under long-term average productivity and 50% ER every year, the aggregate is assumed to settle into a stable state of around 390,000 spawners and around 390,000 catch, with 6 stocks harvested above their sustainable harvest rate (Umsy), and one of these stocks extirpated. Under recent productivity and 41% ER every year, the aggregate is assumed to settle into a stable state of around 226,000 spawners and around 157,000 catch, with two stocks harvested above their stock-specific Umsy, and two of those extirpated.

If all stocks are at equilibrium, then managing to a fixed ER of 50% under long-term average productivity is the same as managing, on average, to an aggregate spawning target of 390,000.  In practice, however, aggregate run sizes and stock composition vary from year to year, and parameter estimates are uncertain.  In any given year, these two strategies can therefore have very different implications, both in terms of aggregate outcomes and stock-level outcomes. For example, consider a year with an aggregate run of 400,000. Though mathematically equivalent at equilibrium, in that year a fixed ER strategy of 50% implies a spawning target of 200,000 and a catch target of 200,000, while a fixed escapement strategy of 390,000 implies a spawning target of 390,000 and a catch target of 10,000 (ER = 0.25%). Now imagine another year with an aggregate run of 1 million. In that year a fixed ER strategy of 50% implies a spawning target of 500,000 and a catch target of 500,000, while a fixed escapement strategy of 390,000 implies a spawning target of 390,000 and a catch target of 610,000 (ER = 61%).  At 61% ER over many years, about 10 of the 16 stocks would be overfished or extirpated. In that specific year, however, the stock-level implications of a 61% ER depend on stock composition (i.e., similar to equilibrium stock composition, or disproportionate contribution of a few stocks?). Forward simulations can be used to explore the expected long-term effect of alternative strategies for these types of contingencies (Section \@ref(ProjBesdResults)).  


*Nass* (Figure \@ref(fig:ERBasedProfileNass), Table \@ref(tab:ProfTabERbasedNass))

Aggregate equilibrium catch is largest for exploitation rate around 60% under the long-term average productivity scenario, and around 50% under the recent productivity scenario. The number of spawners and catch at equilibrium under these exploitations are very different depending on the productivity assumption. Under long-term average productivity and 60% ER every year, the aggregate is assumed to settle into a stable state of around 220,000 spawners and around 330,000 catch, with 2 stocks harvested above their sustainable harvest rate (Umsy), and none of the stocks extirpated. Under recent productivity and 50% ER every year, the aggregate is assumed to settle into a stable state of around 125,000 spawners and around 125,000 catch, with two stocks harvested above their stock-specific Umsy, and none of the stocks extirpated.




\clearpage
(ref:ERBasedProfileSkeenaWild) Example of aggregate equilibrium trade off plots for the SkeenaWild aggregate with 16 modelled stocks. For 5% increments of aggregate exploitation rate (ER; top axis), the figure shows median estimates (points) and interquartile range along the vertical axes (shaded area, p25 to p75) for aggregate spawner abundance (bottom axis), aggregate catch (left axis), and number of stocks where aggregate ER exceeds stock-specific median estimates of Umsy, the exploitation rate at maximum sustainable yield. Note  that the ranges of spawner abundances and catch levels differ substantially between long-term average productivity (Panel A) and recent productivity  (Panel B), but the ranges of ER and number of stocks are the same in both panels.

```{r ERBasedProfileSkeenaWild,   fig.cap="(ref:ERBasedProfileSkeenaWild)"}
include_graphics("data/AggProfiles_SkeenaWild_Dual_2Panel.png")
```



\clearpage
(ref:ProfTabERbasedSkeenaWild) Summary of aggregate equilibrium trade-offs under alternative exploitation rates - Skeena Wild. Table shows values from Figure \@ref(fig:ERBasedProfileSkeenaWild) at 10% increments of fixed harvest rate *(U)*.

```{r ProfTabERbasedSkeenaWild, echo = FALSE, results = "asis"}

er.based.prof.tab.src <- read_csv("data/Agg_Tradeoff_SummaryByAgg.csv") %>% mutate(U2= round(U*100))

table.in <-  er.based.prof.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild", U2 %in% seq(0,100,by=10)) %>%
							select(Prod,U2, -Aggregate,
										 NumStksOverfished_Med,	NumStksBelowSgen_Med, NumStksExtirpated_Med, 	
										 EqSpn_p25,	EqSpn_Med,	EqSpn_p75,	EqCt_p25,	EqCt_Med,	EqCt_p75) %>%
							mutate_at(c(6:11),function(x){format(round(x/1000,1), big.mark=",",n.small =1)}) 
	
col.names.use <- c("Prod","U (\\%)", "Above\nUmsy","Below\nSgen","Ext","p25","Med","p75","p25","Med","p75")


table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",11)),
                  caption = "(ref:ProfTabERbasedSkeenaWild)", col.names = linebreak(col.names.use) ) %>%
    kableExtra::row_spec(c(11), hline_after = TRUE) %>%
	add_header_above(c(" ", " ", "Median Num Stocks" = 3, "Spawners (1000s)" = 3, "Catch (1000s)" = 3)) 

```




\clearpage
(ref:ERBasedProfileNass) Example of aggregate equilibrium trade off plots for the Nass aggregate with 16 modelled stocks. For 5% increments of aggregate exploitation rate (ER; top axis), the figure shows median estimates (points) and interquartile range along the vertical axes (shaded area, p25 to p75) for aggregate spawner abundance (bottom axis), aggregate catch (left axis), and number of stocks where aggregate ER exceeds stock-specific median estimates of Umsy, the exploitation rate at maximum sustainable yield. Note  that the ranges of spawner abundances and catch levels differ substantially between long-term average productivity (Panel A) and recent productivity  (Panel B), but the ranges of ER and number of stocks are the same in both panels.

```{r ERBasedProfileNass,   fig.cap="(ref:ERBasedProfileNass)"}
include_graphics("data/AggProfiles_Nass_Dual_2Panel.png")
```


\clearpage
(ref:ProfTabERbasedNass)  Summary of aggregate equilibrium trade-offs under alternative exploitation rates - Skeena Wild. Table shows values from Figure \@ref(fig:ERBasedProfileSkeenaWild) at 10% increments of fixed harvest rate *(U)*.

```{r ProfTabERbasedNass, echo = FALSE, results = "asis"}

table.in <-  er.based.prof.tab.src %>% dplyr::filter(Aggregate=="Nass", U2 %in% seq(0,100,by=10)) %>%
							select(Prod,U2, -Aggregate,
										 NumStksOverfished_Med,	NumStksBelowSgen_Med, NumStksExtirpated_Med, 	
										 EqSpn_p25,	EqSpn_Med,	EqSpn_p75,	EqCt_p25,	EqCt_Med,	EqCt_p75) %>%
							mutate_at(c(6:11),function(x){format(round(x/1000,1), big.mark=",",n.small =1)}) 
	
col.names.use <- c("Prod","U (\\%)", "Above\nUmsy","Below\nSgen","Ext","p25","Med","p75","p25","Med","p75")


table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",11)),
                  caption = "(ref:ProfTabERbasedNass)", col.names = linebreak(col.names.use) ) %>%
    kableExtra::row_spec(c(11), hline_after = TRUE) %>%
	add_header_above(c(" ", " ", "Median Num Stocks" = 3, "Spawners (1000s)" = 3, "Catch (1000s)" = 3)) 

```




\clearpage
##  STATUS-BASED AGGREGATE LIMIT REFERENCE POINTS {#StatusRPResults}

```{r , echo = FALSE, results = "asis"}

rel.abd.metric.all.df  <- read_csv("data/SummaryTables_RelAbdMetric_AllYears.csv") 

rel.abd.status.2019 <- rel.abd.metric.all.df %>% 
	dplyr::filter(UpTo == 2019) 

```


Status assessments under Canada's Wild Salmon Policy combine multiple considerations into a single integrated status, including absolute abundance, relative abundance, long-term trend, short-term trend, probability of decline, and spatial distribution (Section \@ref(StatusMethods)). Relative abundance is assessed by comparing the generational average of spawner abundance (geometric mean) to a lower benchmark at Sgen and an upper benchmark at 80% Smsy. If abundance falls below Sgen, status *on this one metric* is Red, above 80% Smsy it is Green, and in-between it is Amber. WSP status has been recommended as the primary consideration for evaluating aggregate limit reference points under the 2019 update of the *Fisheries Act* [@LRPGuidelinesSAR]. 

In a full integrated WSP status assessment [e.g., @FrSkWSPStatus2017], a group of experts would work through a case-by-case review of observed abundance against the full posterior distributions of alternative benchmark estimates, then decide how much weight to give this metric relative to other information, such as trends. A rapid approximation of the integrated assessments has been developed, using an algorithm derived from completed integrated expert assessments to combine median benchmark estimates, where deemed appropriate, with the other metrics [@RapidStatusTechRep1]. Integrated or rapid status assessments have not been completed for Nass and Skeena Sockeye, but we illustrate potential uses of the status information with a single metric. We used benchmark estimates without lognormal bias correction for the long-term average productivity scenario to calculate the relative abundance metric for wild Nass and Skeena stocks.

With data up to 2019, spawner abundances for the largest stock from each aggregate fell into the Amber status zone for the relative abundance metric (Figures \@ref(fig:RelAbdMetricMeziadian) and \@ref(fig:RelAbdMetricBabLW)). For Meziadin, annual abundances and running generational averages have been above 80% of the median long-term Smsy for most years since 1980, but dipped into the Amber zone in the last few years. Babine Late Wild abundances and running generational averages have been in the Amber zone for most years since the late 1990s, and even dipped into the Red zone for some years.

Looking at the average abundance for the generation ending in 2019, `r sum(rel.abd.status.2019$MetricStatus == "Red")` stocks were in the Red zone,  `r sum(rel.abd.status.2019$MetricStatus == "Amber")` stocks were in the Amber zone, `r sum(rel.abd.status.2019$MetricStatus == "Green")` stocks were in the Green zone, and `r sum(rel.abd.status.2019$MetricStatus == "None")` stocks could not be assessed for this metric (Table \@ref(tab:RelAbd2019)). Generational average for some stocks falls near the benchmark value (i.e., lower or upper ratio is near 1), and in these cases a small change in the median benchmark estimate could result in a switch of the status category (e.g., Meziadin).  For other stocks, the generational average is so clearly in the Red zone (e.g., Kitwanga at less than 20% of Sgen) or in the Green zone (e.g., Mcdonell at almost 4 times the upper benchmark), that the metric status wouldn't change for any of the alternative benchmark estimates generated by the various candidate SR models. 

The proportion of stocks from each aggregate that fall into the Red, Amber, or Green status zone on the relative abundance metric has varied over time (Tables \@ref(tab:RelAbdTabNass) and \@ref(tab:RelAbdTabSkeenaWild), Figure \@ref(fig:RelAbdPattern)). Most of the modelled stocks for both aggregates were in the Amber or Green zones for the relative abundance metric for most years since the 1980s, but note the several of the largest stocks fell into the Amber zone in recent years (with data up to 2019), and that incorporating low returns in 2020 and 2021 may push these stocks deeper into the Amber zone or even into the Red zone (e.g., Meziadin, Babine Late Wild, Babine Early Wild). If integrated status assessments were to result in a similar picture, then the proposed aggregate limit reference points would be triggered for most years on the Nass aggregate for both of the illustrated objectives (No Red, < 20% Red), and for most years on the Skeena aggregate for the stricter objective (No Red).


\clearpage
(ref:RelAbdMetricMeziadian) WSP metric for relative abundance: Meziadin (Largest Nass stock). Figure shows estimated spawner abundances (blue line with points) and running generational average (red line) compared to lower and upper benchmarks (boxplots). Each boxplot shows median (horizontal line), half of the posterior distribution (box, p25-p75), and 80% of the posterior distribution (whiskers, p10-p90). Benchmark estimates are shown without (R) and with (C) lognormal bias correction. Horizontal reference lines mark the median benchmark estimate without bias correction, which are the values used in Tables \@ref(tab:RelAbd2019) to \@ref(tab:RelAbdTabSkeenaWild).

```{r RelAbdMetricMeziadian,   fig.cap="(ref:RelAbdMetricMeziadian)"}
include_graphics("data/StockSampleFigs/Meziadin_RelAbd_Plot.png")
```





\clearpage
(ref:RelAbdMetricBabLW) WSP metric for relative abundance: Babine Late Wild (Largest Skeena stock). Layout as per Figure \@ref(fig:RelAbdMetricMeziadian).

```{r RelAbdMetricBabLW,   fig.cap="(ref:RelAbdMetricBabLW)"}
include_graphics("data/StockSampleFigs/Bab-LW_RelAbd_Plot.png")
```




\clearpage

(ref:RelAbd2019) Single-metric statuses using data up to 2019: Relative abundance.  Table lists the average generation (main age class, Gen), the number of observations in the generation ending in 2019 (Obs). The generational avg (geometric mean) is compared to median estimates of Sgen (Lower) and 80% Smsy (Upper) under the long-term average productivity scenario. Note that the resulting status category is only for the relative abundance metric, and a comprehensive status assessment would also consider absolute abundance (i.e., is it less than 1,000 adults?), short-term and long-term trends, probability of decline, and spatial distribution (Section \@ref(StatusMethods)).

```{r RelAbd2019, echo = FALSE, results = "asis"}

table.in <-  rel.abd.metric.all.df %>%
								dplyr::filter(UpTo == 2019) %>% arrange(RatioUBM) %>%
								select(-UpTo,-MetricScore) %>% 
								mutate_at(5:7,function(x){prettyNum(round(x),big.mark=",")}) %>%
								mutate_at(8:9,function(x){format(round(x,2),n.small =2)}) %>%
								mutate(MetricStatus = recode(MetricStatus,"None" = "Unk"))


#table.in[is.na(table.in)] <- "NA"
table.in[table.in == "NA"] <- "-"


col.names.use <- c("Aggregate","Stock","Gen","Obs","GenAvg","Lower","Upper","Lower","Upper","Status\nCategory")


table.in %>%
   mutate_at(2,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(2,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c(rep("l",2),rep("r",8)),
                  caption = "(ref:RelAbd2019)",col.names = linebreak(col.names.use) ) %>%
	add_header_above(c(" " = 5, "BM Value" = 2, "Ratio" = 2," " = 1 )) 

```









\clearpage


(ref:RelAbdTabNass) Retrospective changes over time in annual metric statuses for Nass: Relative abundance.  Table summarizes status categories for the relative abundance metric as described in Table \@ref(tab:RelAbd2019), listing the total number of stock (n), the number and proportion of stocks for which the metric could not be calculated (Unk, pUnk), the number of stock for which the metric could be calculated (nStatus), the number of stocks in each status ctegory (Red, Amber, Green), the proportion of Red or Green among the assessed stocks (pRed, pGreen), and proportion of all stocks not assessed as Green (pNotGreen; includes Red, Amber, and Unk).

```{r RelAbdTabNass, echo = FALSE, results = "asis"}

table.in <-  read_csv("data/SummaryTables_RelAbdMetric_AnnualSummary_Nass.csv")

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
                  caption = "(ref:RelAbdTabNass)") 

```


\clearpage
(ref:RelAbdTabSkeenaWild) Retrospective changes over time in annual metric statuses for Skeena Wild: Relative abundance.  Layout as per Table \@ref(tab:RelAbdTabNass).

```{r RelAbdTabSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  read_csv("data/SummaryTables_RelAbdMetric_AnnualSummary_SkeenaWild.csv")

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
                  caption = "(ref:RelAbdTabSkeenaWild)") 

```


\clearpage
(ref:RelAbdPattern) Changes over time of single-metric statuses: Relative abundance. The first three panels show the retrospective changes over time in status categories for the relative abundance benchmark, which are listed in Tables  \@ref(tab:RelAbdTabNass) and \@ref(tab:RelAbdTabSkeenaWild). The fourth panel shows a timeline of years for which either none or only a few of the assessed stocks were in the Red zone on this metric.

```{r RelAbdPattern,   fig.cap="(ref:RelAbdPattern)"}
include_graphics("data/RelAbdMetric_Patterns_MultiPanel.png")
```




\clearpage
## LOGISTIC-REGRESSION-BASED AGGREGATE ABUNDANCE REFERENCE POINTS {#LogRegResults}

We explored whether the approach of developing aggregate abundance reference points based on logistic regressions would be potentially applicable to Nass or Skeena Sockeye. For the illustration, we defined a "success" as *"At least 80% of the stocks in aggregate are above Sgen"*, checked which past years met the criterion, and plotted success/failure vs. aggregate abundance.

For the Skeena Wild aggregate, there was relationship between aggregate abundance and success (Figure \@ref(fig:LogRegSkeenaWild)). All past years where aggregate abundance exceeded about 350,000 spawners met the criterion, but many years with lower abundance also met the criterion. A logistic regression could be fitted to the resulting data, but the shape was highly sensitive to alternative definitions of the success criterion we tested (variations not included in this paper).

For the Skeena aggregate, any results from the logistic regression approach would also need to be carefully framed in the context of (1) wild stocks, (2) total effective spawners combining wild spawners and  effective spawners from the channel stocks, and (3) total escapement including non-spawning biological surplus on the enhanced stocks (Figure \@ref(fig:LogRegSkeenaWild2)).


For the Nass aggregate, there was no link between aggregate abundance and success (Figure \@ref(fig:LogRegNass)). The years with the two largest spawner abundances did not meet the criterion (1992 and 1993, both with more than 500,000 spawners). Most of the remaining years did not meet the criterion, and the success years spanned a wide range of abundances, from less than 200,000 to more than 400,000. 



\clearpage
(ref:LogRegSkeenaWild) Illustration of log-regression approach for deriving aggregate abundance reference points: Skeena Wild. Panels show aggregate spawner abundance (A), the proportion of stocks in the aggregate that met the criterion "annual spawner abundance larger than than the lower benchmark set at Sgen" (pAnnualSpnLgLBM)  by year (B) and by aggregate abundance (C), and overall success/failure on the aggregate criterion of "80% of stocks above the benchmark" (D). The interim escapement goal for the Skeena aggregate is 900,000 spawners, and the corresponding interim escapement goal for the Skeena Wild aggregate is 300,000 based on average stock composition (Section \@ref(Background)), which is marked in the plots. Wild spawner abundance has been at or above the interim EG for most years since the 1980s (A). The proportion of stocks meeting the success criterion has ranged from about 60%-100% (B). For most years where aggregate spawner abundance was at or above the interim goal, 80% or more of the stocks met the success criterion (C). The data points in panel C are then simplified to whether they meet the 80% threshold (Yes/No) to fit a logistic regression (D). The fitted red regression line shows the increasing probability that at least 80% of stocks meet the success criterion as aggregate spawner abundance increases, with aggregate spawner abundance larger than 500,000 resulting in very little incremental increase in the probability of success for this specific example of a success criterion. 

```{r LogRegSkeenaWild,   fig.cap="(ref:LogRegSkeenaWild)"}
include_graphics("data/PropBasedTargetPlots_SkeenaWild_pAnnualSpnLgLBM.png")
```



\clearpage


(ref:LogRegSkeenaWild2) Illustration of log-regression approach for deriving aggregate abundance reference points: Alternative aggregations. Skeena Wild are only a part of the total annual returns that are currently managed based on aggregate abundance. This figure shows three versions of the aggregate spawner abundance (A): wild spawners (including only wild stocks), total effective spawners (wild plus channel loading plus Pinkut and Fulton spawners below the fence), and total escapement including biological surplus (i.e., run kept out of channels but beyond the capacity of spawning grounds below the fences). The time series of success/failure from Figure \@ref(fig:LogRegSkeenaWild) can be plotted against either one of these aggregate abundance (B-D), and would lead to very different results for an aggregate abundance reference point. Interim EG of 900,000 for the total aggregate and 300,000 for the wild aggregate are marked. 

```{r LogRegSkeenaWild2,   fig.cap="(ref:LogRegSkeenaWild2)"}
include_graphics("data/PropBasedTargetPlots_COMPARE_SkeenaWild_pAnnualSpnLgLBM.png")
```





\clearpage
(ref:LogRegNass) Illustration of log-regression approach for deriving aggregate abundance reference points: Nass. Layout as per Figure \@ref(fig:LogRegSkeenaWild). The interim escapement goal for the Nass aggregate at 200,000 spawners is marked. Aggregate spawner abundance has been at or above the interim EG for most years since the 1980s (A). The proportion of stocks meeting the success criterion has ranged from about 25%-100%, but for most years it was less than 80% (B). For most years where aggregate spawner abundance was at or above the interim goal, less than 80% of the stocks met the success criterion (C). The data points in panel C are then simplified to whether they meet the 80% threshold (Yes/No), but the observed scatter of points does not allow for a logistic regression fit, because even the largest aggregate spawner abundances failed to meet the success criterion (D).

```{r LogRegNass,   fig.cap="(ref:LogRegNass)"}
include_graphics("data/PropBasedTargetPlots_Nass_pAnnualSpnLgLBM.png")
```




\clearpage
## SIMULATION-BASED AGGREGATE ABUNDANCE REFERENCE POINTS {#ProjBesdResults}

### Example Results

The example results shown here are for a small and very specific subset of potential scenarios, as defined in Section \@ref(SimScenarios), and for two specific versions of more general objectives.

We include two types of summaries for the simulation results: 

* *Stock-specific probabilities* (Figures \@ref(fig:HeatmapAggFixedERLtAvg) to \@ref(fig:HeatmapAggFixedEscRecent)): These plots compare 10 different levels of a harvest strategy, showing for each modelled stock the probability of achieving one specific objective.
* *Trade-off plots* (Figures \@ref(fig:TradeoffPlotSkeenaWildLTAvg) to \@ref(fig:TradeoffPlotNassRecent)):  These plots compare two objectives across 10 different levels of  a harvest strategy.


#### Probabilities - Fixed Exploitation Rates

Under long-term productivity, almost all modelled stocks (19/20) met the objective when fixed ER was 10% or less, but over half the modelled stocks (11/20) met the objective for fixed ER greater than 50%. Most modelled stocks failed to meet the objective at fixed ER at 60% or higher (Figure \@ref(fig:HeatmapAggFixedERLtAvg)). These proportions shifted dramatically under the recent productivity scenario: Even under 10% fixed ER a quarter (5/20) of the modelled stocks did not meet the objective (Figure \@ref(fig:HeatmapAggFixedERRecent)). 

#### Probabilities -  Fixed Escapement

Under long-term productivity about half (11/20) of the modelled stocks met the objective for aggregate escapement goals set at least 75% above the current interim escapement goals, or target of 350,000 for Nass Sockeye compared to the interim EG of 200,000, and a 525,000 target for Skeena Wild compared to the assumed interim EG of 300,000 (Figure \@ref(fig:HeatmapAggFixedEscLtAvg)). Performance degraded rapidly for lower escapement goals, with only 6 of 20 modelled stock meeting the objective at the interim EG (4th column). The effect of lower fixed escapement goals was less pronounced under the recent productivity scenario, with twice the number of stocks (12/20) meeting the objective at the interim EG (Figure \@ref(fig:HeatmapAggFixedEscLtAvg); 4th column). This is due to the interaction between aggregate abundance and target exploitation rate: under the recent productivity scenario, run sizes for the largest stocks are lower, leading to a lower aggregate abundance, and a lower target ER with a fixed escapement strategy. Under long-term average productivity, aggregate abundances and resulting target ERs are higher, and the component stocks are less likely to meet conservation objectives if they have lower productivity than the largest stocks. A key benefit of the forward simulation approach is that it allows us to identify and investigate these types of counter-intuitive interactions.



\clearpage
(ref:HeatmapAggFixedERLtAvg) Simulation summary - Alternative fixed ER and long-term average productivity.  Simulation results are summarized for different levels of fixed ER (0% to 90%; columns) and one productivity scenario across all stocks. The numbers in each cell of the grid show the probability of spawner abundance in the 3rd generation exceeding a benchmark set at 80% of median Smsy under long-term average productivity for one stock under one specific level of fixed ER. Probabilities are categorized using the Intergovernmental Panel on Climate Change (IPCC) Likelihood Scale to facilitate discussion of results (Table \@ref(tab:IPPCLikelihoodTab)). Stocks are grouped by aggregate, and roughly sorted within aggregate from mouth of the river upstream. Grey shading indicates stocks that were not modelled in the current project. Bolded blue numbers above the grid show the number of stocks in each column with probability larger than 80%.


```{r HeatmapAggFixedERLtAvg,   fig.cap="(ref:HeatmapAggFixedERLtAvg)"}
include_graphics("data/Sims/Agg_Heatmap_SpnLgSmsyBM_LTAvg_FixedER.png")
```





\clearpage
(ref:HeatmapAggFixedERRecent) Simulation summary - Alternative fixed ER and recent productivity. Layout as per Figure \@ref(fig:HeatmapAggFixedERLtAvg).

```{r HeatmapAggFixedERRecent,   fig.cap="(ref:HeatmapAggFixedERRecent)"}
include_graphics("data/Sims/Agg_Heatmap_SpnLgSmsyBM_Recent_FixedER.png")
```






\clearpage
(ref:HeatmapAggFixedEscLtAvg) Simulation summary - Alternative fixed escapement targets with 10% ER floor and 80% ER cap under long-term average productivity.  Layout as per Figure \@ref(fig:HeatmapAggFixedERLtAvg), except that columns correspond to different levels of fixed escapement, set at increments of the interim escapement goal for each aggregate. The fourth column corresponds to the interim goal (200,000 for Nass, 300,000 for Skeena Wild), the first column to 1/4 of the interim goal, and the last column to 2.5 times the interim goal.

```{r HeatmapAggFixedEscLtAvg,   fig.cap="(ref:HeatmapAggFixedEscLtAvg)"}
include_graphics("data/Sims/Agg_Heatmap_SpnLgSmsyBM_LTAvg_FixedEsc10to80.png")
```






\clearpage
(ref:HeatmapAggFixedEscRecent) Simulation summary - Alternative fixed escapement targets with 10% ER floor and 80% ER cap under recent productivity.  Layout as per Figure \@ref(fig:HeatmapAggFixedEscLtAvg).

```{r HeatmapAggFixedEscRecent,   fig.cap="(ref:HeatmapAggFixedEscRecent)"}
include_graphics("data/Sims/Agg_Heatmap_SpnLgSmsyBM_Recent_FixedEsc10to80.png")
```


\clearpage
#### Trade-off plots

The basic trade-off is the same for both aggregates under both productivity assumptions: The number of stocks meeting the biological objective increases as the spawning target increases, up to a point,  while the average annual catch  over 3 generations peaks at some spawning level and then starts declining with further increases in spawning target. 

The spawning target with peak average catch is much lower under recent productivity (Figures \@ref(fig:TradeoffPlotSkeenaWildRecent),\@ref(fig:TradeoffPlotNassRecent)) than under recent productivity (Figures \@ref(fig:TradeoffPlotSkeenaWildLTAvg), \@ref(fig:TradeoffPlotNassLTAvg)), and the amount of catch is much lower as well. 

The effect of the productivity assumption on the number of stocks meeting the biological objective differs by spawning target:

* At larger spawning targets, more stocks meet the biological objective under long-term average productivity than under recent productivity, because more stocks are sufficiently productive to withstand higher aggregate exploitation rates, even though those rates are higher than under recent productivity. Under recent productivity, several stocks fail to meet the biological objective even with a large spawning target, because their productivity is so low that even with the lower exploitation rates associated with larger spawning targets, they don't reach their stock-specific benchmark within 3 generations from recently observed spawner abundances.

* At spawning targets around the interim EG, more stocks meet the biological objective under recent productivity than under long-term average productivity, because aggregate abundances are lower, resulting in lower exploitation rates for the same spawning target. This is another example of the counter-intuitive interactions discussed above for the probability plots.


\clearpage
(ref:TradeoffPlotSkeenaWildLTAvg) Example of Trade off Plot - SkeenaWild - Long-term average productivity. Compares change for two different performance measures as aggregate spawning target is increased from 1/4 of the current escapement goal (left-most point) to 2.5 times the current goal (right-most point). Performance measures were selected to show the trade-off between an example biological objective (number of stocks for which the 3rd simulated generation exceeds 80% of Smsy with more than 80% probability; blue line with solid points, left axis) and an example harvest objective (trimmed average annual catch over 3 generations, orange line with open circles, right axis). Both performance measures improve as as the aggregate spawning target increases up to around 500,000, but average catch peaks around 500,000 spawning target (much above the interim EG of 300,000), while the number of stocks meeting 80% of Smsy continues to increase.

```{r TradeoffPlotSkeenaWildLTAvg,   fig.cap="(ref:TradeoffPlotSkeenaWildLTAvg)"}
include_graphics("data/Sims/TradeOffPlot_2_ProbSmsyVSAvgCtTr_Skeena_LTAvg.png")
```



\clearpage
(ref:TradeoffPlotSkeenaWildRecent) Example of Trade off Plot - SkeenaWild - Recent productivity.  Layout as per Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg). The basic trade-off is the same as in Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg) for long-term average productivity, with average catch peaking at some spawning level while the number of stocks meeting the biological objective continues to increase. However, with this recent productivity scenario, the average catch peaks at a lower spawning target (around the interim goal of 300,000 vs. 500,000) and peak catch is much lower (around 175,000 vs. almost 500,000). The number of stocks meeting the biological objective is higher at lower spawning targets (because large stock have reduced productivity under the recent scenario, so total run sizes and resulting aggregate ER are lower), but lower at higher spawner targets (because under recent productivity more stocks do not reach the biological objective in 3 generations).

```{r TradeoffPlotSkeenaWildRecent,   fig.cap="(ref:TradeoffPlotSkeenaWildRecent)"}
include_graphics("data/Sims/TradeOffPlot_1_ProbSmsyVSAvgCtTr_Skeena_Recent.png")
```




\clearpage
(ref:TradeoffPlotNassLTAvg) Example of Trade off Plot - Nass - Long-term average productivity.  Layout as per Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg). The basic trade-off is the same as in Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg) for Skeena Wild, with average catch peaking at some spawning level while the number of stocks meeting the biological objective continues to increase. For Nass Sockeye, under long-term productivity, the average catch peaks around the interim goal of 200,000 and peak catch is around 325,000. The number of stocks meeting the biological objective ranges from 0 for spawning targets below the interim EG of 200,000 to all 4 modelled stocks for spawning targets
above about 400,000. 

```{r TradeoffPlotNassLTAvg,   fig.cap="(ref:TradeoffPlotNassLTAvg)"}
include_graphics("data/Sims/TradeOffPlot_4_ProbSmsyVSAvgCtTr_Nass_LTAvg.png")
```




\clearpage
(ref:TradeoffPlotNassRecent) Example of Trade off Plot - Nass - Recent productivity.  Layout as per Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg). Observed differences between recent and long-term average productivity for Nass are similar to the observed differences for Skeena Wild: average catch peaks at a lower spawning target (around 125,000 vs. 200,000) and reaches a lower peak (about 110,000 vs. 325,000). Only 2 of the 4 stocks reach the biological objective under recent productivity over 3 generations, even for spawning targets more than double the interim EG.

```{r TradeoffPlotNassRecent,   fig.cap="(ref:TradeoffPlotNassRecent)"}
include_graphics("data/Sims/TradeOffPlot_3_ProbSmsyVSAvgCtTr_Nass_Recent.png")
```




\clearpage
### Summary of Sensitivity Tests

The example results in the previous section are for a small, and very specific subset, of potential scenarios, as defined in Section \@ref(SimScenarios), and for two very specific versions of more general objectives. To support scoping discussions for future simulation work and collaborative planning processes, we summarize general observations from sensitivity tests described in Table \@ref(tab:SimSensTestTable).  Figure \@ref(fig:FixedEscVar) illustrates the comment regarding ER floor and cap.



(ref:SimSensTestTable) Observed effects of key model components. 

```{r SimSensTestTable, echo = FALSE, results = "asis"}


sims.obs.df <- read.csv("data/Sims/SimSensTest_Summary.csv",stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM") %>%
								arrange(Seq) %>% select(-Seq)


colnames(sims.obs.df) <- linebreak(c("Model\nComponent","Observations"))



sims.obs.df %>%
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "l",
                  caption = "(ref:SimSensTestTable)") %>%
  kableExtra::column_spec(1, width = "7em") %>%
  kableExtra::column_spec(2, width = "43em") %>%
   kableExtra::row_spec(1:  (dim(sims.obs.df)[1] -1), hline_after = TRUE)


```


\clearpage
(ref:FixedEscVar) Variations of a fixed escapement goal strategy. Both panels compare a 300,000 escapement goal (A,C) and a 600,000 escapement goal (B,D), with the resulting ER at different aggregate run sizes shown in grey for strict fixed escapement policy (i.e., harvest every fish above the goal, harvest is 0 below the goal). Top panel shows the corresponding harvest control rules (HCR) if ER floor is 10% (i.e., harvest 10% of run regardless of run size, increase ER as run size increases) and ER cap is 80% (i.e., never harvest more than 80%, even at very large aggregate run size). Bottom panel shows the HCR for the same two escapement goals, but with ER floor at 25% and ER cap at 60%. Even though escapement goals of 300,000 and 600,000 are very different, with the higher floor and lower cap, the target ER for the two strategies is the same for a wide range of run sizes. Differences in simulated outcomes between the two strategies are even less pronounced if aggregate-level outcome uncertainty is added to the model (e.g., a difference of 5% in the ER target has little effect if the actual ER is modelled as target ± 15%).



```{r FixedEscVar,   fig.cap="(ref:FixedEscVar)"}
include_graphics("data/Sims/FixedEscRule_Illustration.png")
```


\clearpage
## EXPANDING AGGREGATE REFERENCE POINTS FOR WILD SKEENA SOCKEYE TO ACCOUNT FOR ENHANCED CONTRIBUTION {#SkeenaExpResults}

### Context

Enhanced Pinkut and Fulton present two distinct challenges for managing the total Skeena stock aggregate: 

* *Enhanced contribution to returns* (Figure \@ref(fig:ChannelContribution)): The contributions of Sockeye originating from the enhanced Babine tributaries to the aggregate returns of Babine and Skeena Sockeye have increased over time. Pinkut and Fulton Sockeye together accounted for about 30% of Babine returns in the 1950s and 1960s, before the start of BLDP enhancement, but consistently contribute 80% or more in recent years. Enhanced contribution to total Skeena returns has ranged from about 40% to more than 80% since the 1980s, with a median of 66%. 

* *Biological surplus* (Figure \@ref(fig:SurplusProduction)): Returning Pinkut and Fulton fish that exceed the capacity of natural spawning habitat below the fence and are locked out of the channels are considered a non-spawning surplus. The method for estimating the annual surplus is described in Section \@ref(SurplusEst). The surplus amount has declined from over 400,000 fish in the 1980s and 1990s to under 200,000 in the 2000s and 2010s. However, the proportion of the surplus relative to the total Skeena Sockeye abundance has increased with decreasing returns of wild Skeena Sockeye. From 2016-2019, the surplus represented around 20 percent of the total Sockeye return and up to 50% of total escapements for the Skeena aggregate. In 2 recent years, the surplus matched or exceeded the total harvest.  

(ref:ChannelContribution) Contribution of BLDP-enhanced Sockeye stocks (Pinkut and Fulton). Figure shows % contribution of Pinkut and Fulton to Babine returns (A) and total Skeena returns (B). Vertical red line indicates start of construction of BLDP enhancement facilities in 1965. Trend line shows the running 4-yr average. 

```{r ChannelContribution,  fig.cap="(ref:ChannelContribution)" }
include_graphics("data/ChannelReview/ChannelContribution.PNG")
```


(ref:SurplusProduction) Estimates of channel surplus over time - Total Skeena. Top left panel shows the time series of estimated surplus spawners from both channel-enhanced stocks, excluding ESSR harvests. The remaining panels show the magnitude of the surplus relative to total Skeena Sockeye run size, total escapement (i.e., effective spawner abundance), and total catch including ESSR harvest. 


```{r SurplusProduction,  fig.cap="(ref:SurplusProduction)" }
include_graphics("data/ChannelReview/ChannelSurplus_TotalSkeena_TimeSeries.PNG")
```

\clearpage


### Alternative Expansion Approaches 

We tested two alternative approaches for expanding a wild Skeena management reference point to a total Skeena management reference point. Both used the observed composition of the total Skeena returns from 1982 to 2019, specifically the % of wild spawners in the terminal return, and they both assume that future stock composition is similar to the range of stock compositions observed since the 1980s. 

However, they differ in how those data are used:

* *Simple expansion* (Figure \@ref(fig:SkeenaExp1)): Use the median or upper/lower quartiles for the % wild spawners as a direct scalar on the wild reference point. For example, with a median scalar of 3.58, a terminal run of a bit more than 1 million had a 50:50 chance of resulting in about 300,000 wild spawners. To increase the probability of meeting 300,000 spawners to 75% (3/4 chance), the terminal run would need to be around 1.4 million. Similarly, for a 50:50 chance of meeting a wild spawning goal of 400,000, the terminal run would need to be around 1.5 million. 
* *Logistic regression* (Figure \@ref(fig:SkeenaExp2)): For each candidate terminal run target, calculate the probability of meeting alternative wild spawning goals. The logistic regression approach classifies the annual observations as a success (i.e., wild spawning goal was met) or failure (i.e., wild spawning goal was not met), then calculates the probability of success for different combinations of terminal return target and wild spawning goal. For example, in 1982 the terminal return was 1,447,330 and the wild spawner abundance was	303,954, so 1982 is classified as a success for a wild goal of 300,000, and as a failure for a wild goal of 400,000. The current terminal run target of 1.05 million is as likely as not to meet the current interim EG for wild Skeena Sockeye. This result is similar to the 50:50 chance identified with simple expansion in Figure \@ref(fig:SkeenaExp1). The current terminal run target of 1.05 million is unlikely to meet a wild spawning goal of 350,000 or larger.


Both of these approaches could be applied using forecasted stock composition for a specific year. Given pre-season forecasts for the enhanced stocks and largest wild stocks, in combination with target harvests, an in-season scalar could be approximated. However, this  information is not currently part of the annual in-season planning process.

Note that the % of wild *spawners* used for Figures \@ref(fig:SkeenaExp1) and \@ref(fig:SkeenaExp2) differs slightly from the % of wild *returns* plotted in Panel B of Figure \@ref(fig:ChannelContribution), due to differential in-river harvest of wild and enhanced Sockeye (e.g., timing and location of in-river fisheries below Babine fence, ESSR fisheries targeting Pinkut and Fulton in Babine Lake).


(ref:SkeenaExp1) Simple expansion of wild Skeena management reference points. Plot shows three alternative scalars applied to wild spawning goals ranging from 250,000 to 600,000. Current interim escapement goal and terminal run management target are shown for reference. Note that the scalars are estimated from observed historical stock composition, independent of alternative SR model fits or productivity assumptions.

```{r SkeenaExp1,   fig.cap="(ref:SkeenaExp1)"}
include_graphics("data/SkeenaExpansions/SkeenaExpansion_ScalarPlot_For_DiscPaper_v1.png")
```



\clearpage
(ref:SkeenaExp2) Probability of meeting wild spawning goals at different management targets for terminal run. Logistic regression was used to estimate the probability of success (i.e., wild spawning goal is met) for different combinations of terminal run target and wild spawning goal. Probabilities are categorized using the Intergovernmental Panel on Climate Change (IPCC) Likelihood Scale to facilitate interpretation of results (Table \@ref(tab:IPPCLikelihoodTab)). 

```{r SkeenaExp2,   fig.cap="(ref:SkeenaExp1)"}
include_graphics("data/SkeenaExpansions/SkeenaExpansion_ProbSuccessPlot_For_DiscPaper.png")
```



<!--chapter:end:03-Results.Rmd-->

\clearpage
# DISCUSSION

This chapter is organized into three sections. In Section \@ref(KeyConsiderations) we discuss key considerations for the three analytical priorities that shaped the work presented in this paper: (1) enhanced production, (2) salmon population diversity, and (3) time-varying productivity. In Section \@ref(KeyConclusions) we highlight key conclusions from (1) spawner-recruit analyses, and (2) comparison of alternative approaches for developing aggregate management reference points. Section \@ref(PrioritiesFuture) discusses priorities for future work.

## KEY CONSIDERATIONS {#KeyConsiderations}


Key  considerations for developing management targets for Skeena and Nass Sockeye include the high proportion of enhanced Sockeye within the Skeena aggregate, the large number of component stocks with different production dynamics within each aggregate, and the question of how to address temporal shifts in productivity which is becoming increasingly common for salmon stocks throughout the North Pacific.


###  Enhanced Production

Enhanced Skeena Sockeye now account for a large proportion of the total Skeena Sockeye return in most years. From 1970-2020, the enhanced stocks (Pinkut and Fulton) accounted for an average of 67% of the total Skeena return (range 33-83%). We considered the wild (non-Pinkut and Fulton) and enhanced Skeena Sockeye stocks separately in our analyses, which focused on developing biological benchmarks at the stock and aggregate level for wild Skeena and Nass Sockeye stocks. While total Skeena Sockeye returns have increased considerably since the implementation of the BLDP, the realized benefits to fisheries were less than predicted (Hilborn 1992). 

One fundamental challenge for managing Skeena Sockeye is the tradeoff between reducing mixed stock fisheries to  protect smaller Skeena Sockeye stocks and increasing abundances of surplus enhanced fish arriving in Babine Lake to be locked out of the spawning channels. The number of enhanced Sockeye that exceed spawning capacity has decreased since 2000 but still represents a sizable proportion of the annual Skeena Sockeye return (Figure \@ref(fig:SurplusProduction)). The estimated surplus in 2020 exceeded the total number of Skeena Sockeye harvested in all fisheries. A portion of the surplus may be harvested in commercial ESSR (Excess Sockeye to spawning requirements) fisheries that occur in Babine Lake, but these fisheries do not take place every year. 

Sockeye escapement to the spawning channels, Pinkut Creek and Fulton Rivers have been relatively constant since the start of the BLDP, while total returns for both wild and enhanced Babine stocks (including catch, escapement, and surplus production), and the number of recruits produced per spawner, have decreased in recent decades, raising concerns about density dependence in freshwater and/or marine environments. Density dependence can affect productivity at multiple life history stages in both freshwater and marine environments, and at different scales. Some effects may be unique to a single stock or shared between many stocks within a region. For example, freshwater rearing capacity in Babine Lake controls the production of wild and enhanced Babine Sockeye, while density dependence in the marine environment can affect recruitment for Skeena, Nass, and other Sockeye stocks [@Peterman1982]. 

Babine smolt size has decreased continually over time, with the biggest decreases observed in the decades before the BLDP (Appendix \@ref(ChannelReview)). Smolt size is positively correlated with smolt to adult survival (SAS) [@HendersonCass], and size selective survival has been observed for Babine Sockeye [@WestLarkin]. There is a weak positive relationship between SAS and the mean length of smolts exiting Babine Lake, and a decreasing trend for smolt weight with increasing abundance, which suggests that freshwater density dependence may affect the size and survival of Babine smolts, but we do not have sufficient information to assess this at the stock level. While previous studies have concluded that lake rearing capacity is not a limiting factor for Babine Sockeye [@Shortreedetal2000PRModel], updated limnological studies have not taken place since 2013, and no reported data since 2000. 

The HBM results support a common shared year effect among Skeena Sockeye stocks, which suggests that limitations to recruitment occur on an aggregate scale. Covariation in productivity occurs at regional scales for different species and populations of Pacific salmon throughout the Northeast Pacific, [i.e., @PyperetalSpatialCov; @DorneretalCov]. Declines in Sockeye salmon populations in the Northeast Pacific region have been linked to the unprecedented abundance of Pacific salmon currently rearing in the Northeast Pacific following decades of large-scale enhancement of Pink and Chum Salmon originating from Asia and North America [@RuggeroneConnorsSxcomp].

Enhanced Babine stocks represent the largest component of Skeena Sockeye and are a key consideration for developing management targets. While wild Skeena and Nass stocks were the  focus of the biological benchmark analyses presented here, management reference points for Skeena Sockeye will need to consider the large contribution of enhanced Sockeye to the aggregate. From 1970-2020, the enhanced stocks (Pinkut and Fulton) accounted for an average of 67% of the total Skeena return (range 33-83%). Because loading targets for the spawning channels and managed sections of Pinkut Creek and Fulton River are set to achieve constant spawner densities that are maintained to maximize fry production, mathematical spawner-recruitment models such as the Ricker model, which require a range of spawner escapements (i.e., contrast in the data) may not produce useful parameter estimates. For stocks where spawning escapement is fixed, spawner recruit models and are not recommended for developing biological benchmarks or management targets for the enhanced stocks. Although the CSAS review committee generally acknowledged the challenges of conducting spawner recruit analyses for enhanced stocks with low contrast in the spawner escapement data, they recommended including spawner-recruit modelling results for the enhanced stocks in this Research Document. These results are presented in Appendix \@ref(PinkutFultonResults).

Our analyses considered the wild (non-Pinkut and Fulton) of Skeena Sockeye separately. In Sections \@ref(SpnEst) to \@ref(RunReconEst), we summarized estimation methods and run-reconstructions specific to Babine Sockeye, and assessed trends in surplus production. In  Section \@ref(SkeenaExpResults), we examined the ratio of wild and enhanced Skeena Sockeye to develop advice for expanding abundance-based reference points for the Skeena Wild aggregate to account for the enhanced contribution. 


### Salmon Population Diversity

The different Skeena and Nass stocks considered here include dozens of distinct Conservation Units which spawn in tributaries throughout both watersheds. For Skeena Sockeye, non-Babine Sockeye, with run sizes ranging from hundreds to tens of thousands of spawners, represent a comparatively small proportion of the aggregate Skeena Sockeye return but account for most of the genetic diversity of the aggregate. From a conservation perspective, maintaining the evolutionary adaptive potential within a metapopulation is important for conserving resilience to future environmental changes [@Kardos2021GeneticVariation]. Within a metapopulation, asynchronous population dynamics among component populations confer resilience and stability of associated fisheries [e.g., @Hilborn2003Biocomplexity; @Schindleretal2010Portfolio], while more synchronous population dynamics increase variability in returns and for fisheries [@Freshwateretal2015WeakenedPortfolio]. For Skeena Sockeye, the loss of biodiversity since the start of the directed commercial fishery, characterized by decreased wild Skeena Sockeye returns combined with increased proportions of enhanced BLDP-origin Sockeye has been characterized as portfolio simplification [@Priceetal2021PortfolioSimpl].     

Existing genetic variation may allow populations to adapt more quickly to rapid environmental change [@Barrett2008]. Among Skeena and Nass Sockeye stocks, there are numerous examples where stock-level variation in population characteristics is measurable and likely contributes to asynchronous population dynamics among stocks. These Sockeye populations exhibit stock-specific diversity in life history patterns, smolt size, run timing, freshwater age and age-at-return. Diversity in freshwater age structure can buffer a population from poor marine conditions at different life history stages [@Moore2014LifeHistDiv]. Diversity in adult run timing may protect some stocks with different vulnerabilities to large-scale fisheries, while diversity in life history and smolt migration timing may attenuate year-class failures in years of reduced ocean survival [@Beamish2012], or reduce the risk of mismatch between timing of ocean entry and prey availability [i.e., @Satterthwaite2014; @CarrHarrisetal2018SmoltMigration]. 

Direct links between population characteristics and conservation and/or fisheries benefits are difficult to quantify, but there are numerous recent examples of population level changes that have helped to buffer Skeena and Nass Sockeye from low returns. In the Nass, the early-timed sea-type population accounted for up to 30% of the Nass Sockeye aggregate return in 2018 and 2019, when Sockeye returns to Meziadin fell below its escapement goal. Within the Meziadin stock complex, a relatively new spawning population (Strohn Creek), which has appeared in recent decades, now accounts for a substantial proportion of the Sockeye return (M. Cleveland, Gitanyow Fisheries Authority, pers. comm., 2021). For the Skeena, extremely low Sockeye returns in 2013, 2017 and 2019, which were driven by poor returns to Pinkut and Fulton, were buffered to an extent by returns to non-Babine Skeena Sockeye systems, which accounted for up to 25% of the aggregate Sockeye return, compared to just 10% in a typical year (unpublished DFO data).

If the relative abundance, or evenness of the different component populations is taken into account, there has been a substantial decrease in Sockeye diversity among Skeena and Nass Sockeye salmon since the start of large scale directed commercial fisheries at the beginning of the 20th century [@Priceetal2019Scales]. For many Skeena and Nass Sockeye populations, Indigenous history, early settler accounts and recent reconstructed historical abundances provide evidence of much larger returns than recent time series of escapements for these stocks. For instance, Sockeye escapements to the Kitwanga River, which likely exceeded tens of thousands at the start of the 20th century [@Priceetal2019Scales], have recently seen returns as low as 230 spawners in 2018. Kitwanga Sockeye are now the focus of an intensive recovery effort [@Cleveland2019KitwangaRecovery], and recovery planshave been initiated for other Skeena populations that have been considered  conservation concerns at different times, including Lakelse Sockeye and Morice Sockeye. 

An aggregate escapement goal that assumes long-term average productivity and stable stock composition may not protect less productive populations from overexploitation. Reduced diversity and increased synchrony may introduce additional management challenges [@Freshwateretal2020Selectivity]. Maintaining genetic and within-population diversity for aggregate salmon populations may protect fish populations and associated fisheries that depend on them may increase their resilience to environmental change [@Andersonetal2015Portfolio; @Kardos2021GeneticVariation].  


The diversity of Sockeye populations has been a key consideration throughout the Skeena and Nass Sockeye escapement goal review, and was identified as an important priority to address by the bilaterally agreed-upon Terms of Reference and by the TWG and independent reviewers. Some of the aggregation approaches described in Section 2.5, including status-based methods and forward simulation modelling, are more suitable than others, such as estimating the aggregate-level MSY, for incorporating stock-level diversity into management targets. The ongoing management engagement process has focused on using forward simulation modelling, to explore tradeoffs between harvests and biological risks, and the simulation outputs can provide information about what spawner abundances are associated with the highest number of stocks achieving biological objectives. The simulations show that under recent productivity, harvests will peak at lower aggregate spawner targets than would maximize the number of healthy stocks.


### Time-varying Productivity {#ImportantTimeVar}

Sockeye salmon populations are changing rapidly with the cumulative effects of stressors including fishing pressure and climate change. Skeena and Nass Sockeye stocks have seen declining productivity, together with increasing variability and increased frequency of low returns since 2000. Skeena and Nass Sockeye are now among a growing list of major British Columbia Sockeye salmon populations (along with Rivers Inlet, Smith Inlet, and Fraser Sockeye), which once supported large scale Canadian commercial fisheries, that are now constrained by low returns and associated conservation efforts. The four lowest Nass Sockeye returns were recorded from 2017-2022. For Skeena Sockeye, the lowest escapements since the catastrophic Babine landslide in the 1950s occurred in 2013, 2017, and 2019. 

Although the temporal patterns of variation in productivity vary by stock, a general pattern of decline is evident across stocks:

* Patterns in the spawner-recruit data for individual stocks as well as the Skeena and Nass aggregates (i.e., spawners, observed recruits per spawner).
* Ricker residuals (i.e., observed productivity compared to productivity predicted by fitted models).
* decline in the productivity parameter (alpha) for many of the stocks where sufficient data are available to fit stock-specific Ricker models with time-varying productivity. 
* Consistent shared-year effect in the HBM model results across the Skeena Sockeye stocks, finding a similar pattern of decline in productivity as for single-stock Ricker fit for the Skeena aggregate data set.

Further, there is evidence that size-at-age, and fecundity for Skeena and Nass Sockeye have decreased in recent decades. Anecdotally, long time commercial fishermen targeting Skeena Sockeye report switching to smaller-mesh nets (from 5 1/2” to 4 ¾” beginning in the 1980s. Population-level changes in body size, age composition, or size-at-age have been observed in all species of salmon in different regions throughout North America [e.g., @Oke2020RecentDeclinesBodySize; @Ohlbergeretal2020CkEscQual; @SchaulGeiger2016ClimateCoho]. The patterns of decline in overall length and length-at-age for Skeena and Nass Sockeye are consistent with decreases of similar magnitudes that have been observed for Sockeye salmon populations in Southeast Alaska [e.g., @Oke2020RecentDeclinesBodySize]. 

For Skeena Sockeye sampled at the Tyee Test Fishery, length at age decreased by 2-3% for 5, 6 and 7 year old fish and remained constant for 4 year old fish between the 1980s and 2010s. The overall length of Nass Sockeye sampled at Meziadin Fishway since 2010 is substantially less than the historic average, indicating shifts in age composition combined with decreases in body length of sampled fish. The observed decreases in overall size for sampled populations of Skeena and Nass Sockeye, together with observed decreases in fecundity of approximately 13% for fish sampled at the Babine spawning channels, indicate a trend toward decreased reproductive with implications for both wild and enhanced Sockeye populations. Escapement goals that assume constant egg production over time may not account for these patterns of decline in escapement quality.

Together with low returns and apparent declines in reproductive potential, Skeena and Nass Sockeye are facing increased frequencies of extreme environmental conditions. For example, Skeena Sockeye from the 2013 brood year, which was itself the lowest Skeena Sockeye return since the years immediately following the Babine slide, experienced extreme environmental conditions throughout their life history. The spawning channels did not meet their loading targets in 2017, 2017 and 2019, and smolts which migrated to sea in 2015 encountered a marine heatwave that persisted from 2014 to 2016 that was extreme in intensity, geographic range, and the unusual depth of anomalous temperatures [@Rossetal2021Heatwaves]. 4- and 5-year old Sockeye that returned in 2017 and 2018 encountered drought conditions and extreme temperatures during the return migration and on the spawning grounds. While each of these events are system specific, and anecdotal examples of what were previously thought to be rare events, there is no question that extreme events are occurring at an increasing frequency.

Maintaining healthy and diverse Sockeye salmon populations for Skeena and Nass Sockeye will require planning for these extreme events, which may include developing escapement goals and management strategies that can adapt and rapidly respond to changing conditions, such as mitigating for extreme temperatures as has been practiced for Fraser Sockeye [@FraserSKEnroute; @FraseSkMgmtAdjFAO2021]. Maintaining population diversity will also help to buffer for aggregate populations from the likelihood of future catastrophic events, which should be considered in fisheries management, including the development of escapement goals. While fisheries managers cannot predict climate-related or other catastrophic events on a year-to-year basis, the probability of bad outcomes can be reduced by introducing buffers to mitigate risk, and by maintaining stock-level diversity within the Skeena and Nass metapopulations [@Andersonetal2015Portfolio].

A key finding of the data review was that many Skeena and Nass Sockeye stocks and both aggregates had dramatically lower recruitment productivity in recent years compared with the long-term average. Our subsequent analyses focused on exploring the performance of stocks under different productivity scenarios, and specifically compared results generated using spawner recruitment parameters developed using long-term average productivity with recent productivity (Tables \@ref(tab:ProdCompTab2) and  \@ref(tab:ProdCompBMRatioSmsy)). While decision makers need to consider that lower productivities are likely to continue in the future, we included results based on long-term average productivity to illustrate the contrast and magnitudes of difference based on different productivity assumptions. More work needs to be done to incorporate different productivity variations, including different definitions of “recent” productivity, into the current framework.  Potential future work on alternative scenarios may include environment considerations and bound likely futures based on known relationships (potentially stock by stock relationship differences may be leveraged based on existing/ongoing work).


## KEY CONCLUSIONS {#KeyConclusions}


### Alternative SR Model Fits and Productivity Scenarios

#### Alternative SR Model Forms

We explored three alternative SR model forms, which was informative because (1) the comparison between them helped with understanding the properties of each stock-specific SR data set, and (2) parameter estimates from different model forms could be used for different purposes.

For stocks with complete time series, where all three SR model forms could be fitted, the following observations are noteworthy:

* given the strong temporal trends in residuals, the AR1 Ricker fit improved the statistical properties of the fit compared to the Basic Ricker fit. Therefore, we chose parameters from the AR1 fit to generate the long-term-average productivity scenario, where available.
* productivity patterns identified through the TVP model fit with time-varying productivity differed between stocks, and in some cases the productivity patterns tracked the residuals from the Basic Ricker fit very closely (i.e., did not identify a smooth underlying pattern). These highly variable productivity patterns need to be interpreted with caution, but can still serve as a useful source of parameter estimates for alternative productivity scenarios (i.e., give a high contrast to the high and low productivity bookend scenarios). Therefore, we used the TVP parameter estimates for different time periods to generate alternative productivity scenarios, where available. TVP fits generally resulted in more uncertain estimates of the productivity parameter (i.e., wider ln.alpha posteriors), but in more precise estimates of capacity (i.e., narrower Smax posteriors).

#### Biological Benchmarks

The Nass Sockeye aggregate includes 9 different CUs which were combined into 7 stocks for our analyses, while the Skeena aggregate includes 30 extant CUs, which were combined into 24 stocks for our analyses. For both aggregates, spawner-recruitment based biological benchmarks were developed for each of the major contributing stocks. Productivity for the different Nass and Skeena stocks varies across time, with the aggregate stocks, and their largest components (Meziadin and Babine wild stocks) exhibiting near-continual declines in recruits per spawner, and the productivity parameter from single stock fits, since 2000 (Figures \@ref(fig:FitsCompProd1) and \@ref(fig:FitsCompProd2)). Plausible alternative productivity scenarios were developed to characterize high, low, long-term average, and recent productivity scenarios by sampling from the posterior distributions for the Ricker-alpha parameter from the most appropriate available models for each scenario.

Biological benchmarks were estimated for the different Skeena and Nass Sockeye stocks using the parameter distributions generated from the alternative productivity scenarios. These were used to build illustrations of equilibrium probability profiles and aggregate reference points that can be used to inform choices for aggregate escapement goals, once management objectives have been clearly defined.

Extensive testing showed that benchmark estimates for some stocks were highly sensitive to one or more of the following:

* *Data treatments* (Appendix \@ref(AltSRTest)): Filtering out brood years with R/S > 45 made a big difference in benchmark estimates for some stocks, and we consider this a necessary quality-control step. If this results in very different benchmark estimates, it indicates that 1 or 2 extreme values had a big effect, and should be investigated carefully. In contrast, infilling missing values generally had little effect on benchmark estimates, because we infilled  using average values, and the infilling did not generate any extreme points. For stocks where the infilling made a difference in the Basic Ricker estimate, this is due to the age structure of Sockeye salmon. A single missing year of spawner and run size estimates can exclude 3-5 brood years from the SR data set. For example, infilling two return year data points for Kitsumkalum added 8 brood years to the SR data set, but only changed the median Smsy estimate by 6%. Infilling five return year data points for Asitka more than doubled the available SR data from 11 brood years to 24 brood years, but also changed resulting estimates substantially (Smsy: -21%, ln.alpha: -41%).
* *Benchmark calculations* (Appendix \@ref(BMCalcTest)): Estimates of biological benchmarks were insensitive to alternative formulations (<2%), but the success rate varied for alternative implementations of the Sgen optimizer. We used the @Scheuerell2016 method to calculate Smsy, because it is the only exact solution, and the @Connorsetal2022 version of the Sgen optimizer, because it was the only non-brute-force method that did not crash for any of the tested parameter combinations.
* *Bayesian estimation*: Median Bayesian benchmark estimates from the Basic Ricker model fits were similar to the simple deterministic estimates for most stocks. Those stocks that were flagged as more than 25% different from the deterministic estimate (Figure \@ref(fig:FitsCompDetFig)) also had more uncertain Bayesian estimates (i.e., very wide posteriors), and were highly sensitive to alternative capacity priors (Figure \@ref(fig:FitsCompCapPrior)).

While estimates for smaller stocks with noisier and incomplete SR data were generally more sensitive than larger stocks with higher quality data, it was not always the same stocks that were flagged in different sensitivity tests. SR fits  for the enhanced stocks and for aggregate-level SR data, in particular, were highly sensitive to alternative SR model assumptions, even though the quality of each individual spawner and recruit estimate was high.


#### Comparing Single-stock and Aggregate Model Fits

Although calculating SR model fits for aggregate data is computationally simple, the resulting parameters are not appropriate for developing management targets. 

For the Skeena aggregate, the majority of effective spawners, and most of the adult returns, are from the BLDP enhancement facilities on Pinkut and Fulton. We explored model fits for a SkeenaWild aggregate that excluded the enhanced stocks, but this level of analysis masked the diversity of stock-specific productivity patterns identified in the stock-level analyses, particularly the substantial recent decline in productivity for several of the wild Skeena stocks with complete (or infilled complete) time series. 

For the Nass aggregate, two stocks with very different life histories and production dynamics account for most of the abundance. Historically, the Nass run was mostly from Meziadin lake-type Sockeye, but in recent years Lower Nass Sea and River Type Sockeye have increased both in absolute abundance and relative contribution. We consider it more appropriate, therefore, to model these two stocks separately, rather than in a combined SR data set.


#### Comparing Single-stock and Hierarchical SR Model Fits

At the beginning of their WinBUGS manual, @WinBUGSManual remind users in bold, red font: *"Beware: MCMC sampling can be dangerous"*. Computing power continues to increase and MCMC software continues to evolve (e.g., from WinBUGS to JAGS to STAN), but the basic challenge remains the same: If estimates are highly sensitive to model structure and prior assumptions, which version is the most “correct”? 

Extensive sensitivity testing  is standard practice in Bayesian estimation. However, in this project we had the rare opportunity of comparing two completely separate and fundamentally different implementations of the same type of SR model fit to the exact same original data set. Extensive sensitivity testing was conducted for both model implementations, but it was only through direct comparison between the two implementations that we were able to identify sources for the discrepancies. Most of the initial large differences in benchmark estimates were not due to the hierarchical structure, but resulted from alternative specifications of the capacity prior (Smax). Differences were more pronounced for stocks where the data didn't have a strong density-dependent signal (i.e., "noisy" scatterplot), which were flagged as highly sensitive in both sets of analyses. 

The ability to compare independent model implementations served as an important reminder that Bayesian estimation can be very sensitive to prior assumptions and structural differences. Resulting estimates need to be ground-truthed based on biological expertise, and the stock-specific uncertainty of the estimates needs to be considered in subsequent uses of the results. 

The implications of observed differences between estimates will differ depending on how the information is used. For example, if a subsequent decision process focuses on the sum of stock-level Smsy estimates, then large relative differences in the Smsy estimates for a few small stocks will not substantially affect the decision. If, however, a decision process focuses on status assessments that incorporate relative abundance benchmarks, such as Sgen and 80% Smsy, then large differences in benchmark estimates can translate into a very different overall status picture, which could strongly influence decisions that are made based on these assessments.

Similarly, stock-specific estimated patterns in productivity differed between single-stock and hierarchical model fits. Both sets of results can be useful, and  a direct comparison is informative. The shared-year effect in the hierarchical model looks for a common pattern across stocks, and two types of  sensitivity tests can be used to investigate the estimated patterns: 

* *top-down*: McAllister and Challenger (Appendix \@ref(app:HBMFits)) started with a hierarchical model form that included 18 modelled Skeena stocks, and then dropped one or more stocks from the analysis. In these tests, the common shared year effect was not sensitive to alternative combinations of  16-17 stocks.
* *bottom-up*: An alternative approach might be to begin by fitting hierarchical models with smaller groups of stocks and explore how the shared year effect differs for these smaller groups. For example, hierarchical model fits could be tested for (1) just the three wild Babine stocks, (2) all five Babine stocks, (3) all wild  middle Skeena lake-type Sockeye stocks with SR data, (4) all lower Skeena lake-type stocks with SR data, and (5) all upper Skeena lake-type stocks with SR data. Based on these explorations, a hierarchical model with a more nuanced spatial structure could then be developed.


### Alternative Approaches for Developing Aggregate Management Reference Points {#AggregationComparison}

This Research Document presents alternative biological benchmarks for Skeena and Nass Sockeye stocks along with a number of different approaches for developing aggregate reference points. Specific choices about how to use the information presented here, including the eventual choices for updated escapement goals for Skeena and Nass Sockeye will be made once management objectives are defined in subsequent processes, including a multistakeholder evaluation process to select an escapement goal for Skeena and Nass Sockeye which is currently underway.  

While these processes will involve different individuals and will unfold at different timelines, they all rely on the same fundamental information: (1) agreed-upon spawner-recruit data, and (2) agreed-upon approach for fitting spawner-recruit models, and (3) agreed-upon approach for calculating the resulting biological benchmarks. How the data, model fits, and biological benchmarks are considered will also differ between the processes, so we focused on the fundamental information, but also illustrate how it could be used. The approaches presented here are intended to be flexible to incorporating new information if required by the decision-making process, for example it is expected that model fits and biological benchmarks will change as data sets are updated in the future. The data sets and tools, or building blocks, presented here, are intended to provide a starting point for the decision-making process, rather than prescriptive recommendations for the choices that will eventually be made.

#### Requirements for the Escapement Goal Review

Revised escapement goals for Skeena and Nass Sockeye are required for implementing Pacific Salmon Treaty provisions. Following the evaluation of alternative approaches against selection criteria developed by a group of CSAS meeting participants, subsequent work focused on developing a refined version of the forward simulation model, along with equilibrium tradeoff profiles to evaluate tradeoffs during the management engagement process.

* *Equilibrium tradeoff plots*: Equilibrium probability profiles estimate expected yield for a given stock under different productivity scenarios. For example, the equilibrium profiles for Meziadin Sockeye  illustrate the probabilities of attaining 60 or 80% of long-term average MSY, or an equilibrium yield greater than 100,000 under long-term average or recent productivity scenarios (Figure \@ref(fig:ProfileMezidian)). These results suggest that for Meziadin Sockeye, there is a very low likelihood of attaining any of these objectives under current conditions.
* *Simulation-based reference points*: Simulation models can be used to explore tradeoffs between aggregate catch and biological risks to individual populations within each river system, across a range of aggregate escapement goals, and identify aggregate spawning goals that maximize catch, stock health, or other specified performance metrics. For the initial version of this Research Document, we built a simple simulation model to illustrate the type of information that can be incorporated in a management strategy evaluation. The simulation model was then refined following discussion at the CSAS peer review meeting and recommendations from some participants and independent reviewers to incorporate outcome uncertainty and covariation in productivity. These modifications are described in Appendix \@ref(ModelExt).

In this paper we demonstrate the potential benefits of forward simulations that generate expected trajectories under alternative assumptions. A key finding was that the responses to different harvest strategies were highly sensitive to the productivity assumption. Not surprisingly, projected abundances are lower under recent productivity scenarios than under long-term average productivity for most Skeena and Nass Sockeye stocks. The simulation also identified counter-intuitive interactions between productivity scenarios and alternative harvest strategies. For escapement goals below the current interim escapement goals, more stocks achieve the 3-generation objective under the recent productivity scenario than under the long-term average productivity scenario. Under long-term average productivity, aggregate run sizes are larger due to productive large stocks, and these run sizes translate into larger target ER, which in turn impacts less productive, smaller stocks (Figures \@ref(fig:TradeoffPlotSkeenaWildLTAvg) to \@ref(fig:TradeoffPlotNassRecent)). 

Future model extensions could address other considerations, such as area and time-specific differences in harvest impacts, and future productivity. Such expansions need to be carefully bounded to focus on mechanisms that are relevant to the objectives of the analysis.


#### Requirements for Regional Salmon Initiatives

Regional salmon initiatives can also build on the examples of alternative aggregation approaches presented in this paper:

* *Aggregate limit reference points (LRP)*: New guidance [@LRPGuidelinesSAR] identifies two of the building blocks presented in this Research Document as candidate approaches for developing LRPs under the modernized *Fisheries Act* (2019). Specifically, the logistic regression results and illustration of multi-criteria status assessments included here lay the groundwork for a formal LRP process for Skeena and Nass Sockeye.
* *Multi-criteria status assessment*:  We illustrated status considerations for a single metric, abundance relative to biological benchmarks (Sgen, 80% Smsy), but actual status assessments under the  Wild Salmon Policy are based on a combination of multiple criteria (absolute abundance, long-term and short-term trends, distribution). Completing multi-criteria status assessments for Pacific salmon conservation units is a key deliverable of Wild Salmon Policy implementation.


## PRIORITIES FOR FUTURE WORK {#PrioritiesFuture}

Through the CSAS review process in April 2022, the follow-up process on comparing aggregation approaches, discussions with the independent reviewers, and feedback from the ongoing Canadian domestic engagement process, we have identified several critical areas for future work.

Each of these tasks presents considerable work, both in terms of analyses and process, and they cannot all be addressed at the same time, given their interconnectedness and requirement for input from  many of the same people. For example:

* Development of an aggregate LRP consistent with new guidelines requires multi-criteria status assessments of individual conservation units [@LRPGuidelinesSAR]. 
* Exploring alternative strategies for managing the the non-spawning surplus of enhanced Sockeye would require developing an improved and expanded simulation model. 
* Both of these analyses would require substantial participation from the same core group of DFO staff, and would need to be completed while they continue supporting annual operational requirements (e.g., PSC technical committees).

We anticipate that the ongoing Canadian domestic engagement process and bilateral Canada/U.S. discussions through the PSC will provide the next opportunities to develop guidance on priorities and scope for the next phases of work. We summarize potential tasks below to support the scoping discussions.


### Objectives

Regardless of which approach is chosen for developing aggregate management reference points for Skeena and Nass Sockeye, the application of that approach requires clearly defined quantitative objectives. We used various examples of quantitative objectives throughout this Research Document, but the larger escapement goal review process needs to develop four distinct types of agreed-upon objectives:

* *Quantitative objectives for individual wild stocks*: We focused our analyses on simple stock-level objectives linked to the lower and upper WSP benchmarks for relative abundance (Sgen, 80%Smy). However, other biological stock-level objectives should be explored, and stock-specific harvest objectives should be identified where possible (e.g., for specific in-river fisheries) 
* *Quantitative aggregate-level objectives*: Different aggregate-level objectives are either implicitly or explicitly required for applying the alternative approaches for developing aggregate management reference points. Several require an explicit definition of what constitutes success for each component stock, and then an explicit definition of what constitutes success for the aggregate. Focusing on wild stocks only, one example we used was *"At least 80% of stocks in the aggregate should have at least 80% probability of spawner abundance exceeding the upper WSP benchmark  for the relative abundance metric (80% Smsy under long-term average productivity) after 3 generations (simulation years 11-15)"*. However, many variations of objectives like this could be considered and tested, and an agreed-upon shortlist should be developed.
* *Quantitative objectives for individual enhanced stocks*: BLDP enhancement activities are currently managed under the general objective of maximizing smolt production under historical conditions, but this general goal should be reviewed given observed changes in the environment, in wild stocks, and in population dynamics of enhanced stocks. 
* *Quantitative objectives for the total Skeena aggregate, including wild and enhanced stocks*: There is currently no explicit policy guidance regarding the desired balance between biological objectives for wild stocks, production objectives for enhanced stocks, harvest objectives for combined wild and enhanced returns, and the unharvested, non-spawning surplus of enhanced returns. A formal management strategy evaluation (MSE), combining a simulation model and a structured participatory process, offers a comprehensive framework for identifying general objectives and key mechanisms, building a custom simulation model to test alternative scenarios, and refine both the analyses and objectives over time.

### Data

The natural and social system in which we are collectively trying to manage Skeena and Nass Sockeye is rapidly changing and annual assessment programs are adapting to changes in stocks, fisheries, and available tools. As a result, the data foundation available for the Skeena and Nass Sockeye escapement goal review is continuously evolving, which affects the priorities for both analyses and process. For example, emerging DNA-based stock identification may provide better estimates of stock-specific harvest impacts. Similarly, new information is being developed about the most appropriate methods for estimating spawner escapements for two stocks (Bowser, Bear). 

The analyses presented in this Research Document used SR data up to the 2019 returns, after an in-depth review of source data [@SkeenaNassSkDataRep]. Returns in 2020, 2021, and 2022, which are not incorporated in the current analysis, were unusual for many of the stocks, and may substantially affect parameter estimates. As data are updated  and SR parameter estimates revisited, the specific values of biological benchmarks, and the results of any chosen aggregation approach will also change. However, regular updates of run reconstructions, SR fits, status assessments, and simulation reruns are a substantial task. Planning for regular updates needs to fit this task in the context of all the prioritized tasks.     

### Spawner-recruit Modelling and Productivity Scenarios

The SR analyses presented in this paper include extensive sensitivity testing (3 single-stock model forms, 4 alternative capacity priors, alternative data treatments, alternative benchmark estimation approaches, single-stock vs. hierarchical Bayesian fits). Discussions during the peer review process identified several areas for further exploration:

* *Alternative capacity priors*:	We tested four combinations with either uniform or lognormal distributions and either wide or capped bounds on the distribution. The bounds were selected based on available information for each stock, including 
lake capacity estimates based on photosynthetic rate, largest observed spawner abundance, and Bayesian posterior distributions from an initial round of SR model fits. Alternative implementations of the log-normal capacity prior and alternative bounds on the capacity prior could be tested to further explore the sensitivity of resulting SR parameter estimates for stocks with highly uncertain capacity estimates. This was identified as a priority, because large initial differences between the single-stock and hierarchical Bayesian model fits were traced to differences in how the capacity priors were set up.
* *Stochastic simulations of bias*:	Potential biases in SR parameter estimates could arise from different implementation details (e.g., data treatment, model forms, capacity priors, Bayesian MCMC implementation). While we did extensive sensitivity testing, a more formal exploration of potential biases could generate many random data sets for hypothetical stocks with known population dynamics designed to be similar to the different Skeena and Nass Sockeye stocks, then apply the alternative SR model fitting variations to test which version produces estimates most similar to the "true" values.
* *Model Form switching*: For stocks with complete SR data sets (i.e., no missing years) the current implementation of alternative productivity scenarios sampled parameters from the AR1 Ricker fit for the long-term average productivity scenario and from the most recent generation of the time-varying productivity (TVP) Ricker fit for the recent productivity scenario. Some peer review participants expressed concern over potential implications of using different model forms for the productivity scenarios. Alternative versions of the long-term average productivity scenario, based on the TVP model fit could be explored (e.g., sample from all brood years instead of just the most recent generations, or calculate the average ln.alpha across brood years for each MCMC parameter set).
* *Alternative hierarchical structure for the HBM model fit*: As discussed in Section \@ref(KeyConclusions) above, different assumptions about the hierarchical structure of Skeena Sockeye could be tested within the estimation framework developed by McAllister and Challenger (Appendix \@ref(app:HBMFits)).

### Enhanced Pinkut and Fulton

Priorities for future work relate to stock assessment, channel management, interactions with wild stocks, and alternative approaches for developing a combined enhanced-wild harvest strategy. Specific tasks include:

* *Review of channel loading targets*: We included an overview of egg, fry and smolt production data and trends in this report.  An in-depth analysis of enhanced Babine Sockeye production, which is part of a larger review of the effectiveness of salmon hatcheries in the Pacific Region, led by Pacific Salmon Foundation, is ongoing (Cam West, pers. comm., DFO Salmonid Enhancement Program (retired), 2021). However, this work only addresses technical components of channel production, with a focus on enhanced Babine Sockeye. A broader integrated review is needed to determine how channel management should respond to these observed biological changes. 
* *Investigate biological interactions between enhanced and wild populations*: Although egg, fry and smolt production data for Babine Lake suggest freshwater density dependence with reductions in smolt size associated with higher fry production, more work is needed to understand the longer term effects of BLDP enhancement on wild Babine and other Skeena Sockeye populations, including updated information about changes in freshwater rearing capacity for Babine Lake, and a detailed analysis of size selective marine survival and recruitment across a range of ocean conditions. Improvements to genetic resolution between the different Babine stocks may inform a better understanding of the potential for straying between populations.    
* *Investigate indirect interactions between enhanced and wild populations through aggregate abundance, aggregate harvest rules, and mixed-stock fisheries*: This would likely involve an expanded simulation model with a distinct component for population dynamics of the enhanced stocks and a finer resolution of simulated harvest (e.g., different wild and enhanced harvests based on fishery timing, location, and gear).
* *Review of surplus management*: Our SR analyses and resulting building blocks for aggregate management reference points focused on the wild stocks. However, harvest management of Skeena Sockeye needs to find a balance between wild stock considerations, operational considerations for the enhanced stocks, and the interaction between biological surplus from the channel stocks with aggregate harvest rates on the combined returns of wild and enhanced stocks. These challenges, which are not new to Skeena Sockeye fisheries management, apply regardless of the chosen approach for setting aggregate targets (e.g., probability profiles vs. forward simulation).


### Incorporating Biological Considerations in Aggregate Management Reference Points

Changing productivity was identified as the highest analytical priority by scoping workshop participants, technical WG members, and independent reviewers for the Skeena and Nass Sockeye escapement goal review. In this paper we present an approach for selecting SR model parameters that describe alternative productivity scenarios, and we show the implications for resulting estimates of biological benchmarks and any considerations based on these benchmarks. The importance of considering time-varying productivity is discussed in Section \@ref(ImportantTimeVar) above. Specific priorities for future work include:

* *Identifying productivity regimes*: Our analyses included an approach for generating alternative productivity scenarios (i.e., long-term average vs. recent), but we did not analyze observed time trends in productivity to look for evidence with discrete shifts in productivity (i.e., regimes). Analyses like @Rodionov2005Regimes can identify shifting productivity regimes, and assist with bounding future productivity scenarios (e.g., for forward simulation). 
* *Guidance for considering productivity changes in biological benchmarks and management reference points*: Identifying past changes in productivity is an important step, but there are many conceptual challenges with incorporating this information into a management context. For example, if recent productivity is lower than average, then Smsy estimates under a recent productivity assumption will generally also be lower. Should escapement goals be lowered in years of poor productivity, or should they stay the same to avoid ratcheting down abundance over the long-term, or should they increase to speed up rebuilding? A multi-year research initiative started in the summer of 2022 to develop formal guidance  for management considerations under changing productivity (Carrie Holt and Brendan Connors, pers. comm., DFO, 2022).
* *Incorporating demographic changes into escapement goals*: For populations that are undergoing changes in escapement quality such as reduction in body size, fecundity or sex ratio, escapement goals that assume constant egg production may underestimate the number of spawners required to achieve objectives such as maximum sustained yield, or targets for fry production [@Staton2021]. Some of these changes have been observed in Skeena and Nass Sockeye, and a more detailed analysis is needed to assess whether these changes are likely to represent significant shifts in recruitment that will need to be considered in management targets for these stocks. Alternative approaches for incorporating demographic changes into escapement may need to be considered, such as considering escapement goals based on egg production rather than spawner abundance, or incorporating demographic change explicitly into spawner recruitment modeling, like the alternative escapement goal adjustments explored  in @Connorsetal2022 for Yukon Chinook.














<!--chapter:end:04-Discussion.Rmd-->

<!-- In general, you shouldn't need to edit this file with the exception of
the following French/English translation. For a French document, set the following header to: # RÉFÉRENCES CITÉES {-} -->

\clearpage

# REFERENCES {-}


<!--chapter:end:05-references.Rmd-->


<!-- The following code should appear at the beginning of the first appendix.
After that, all subsequent sections will be turned into appendices. -->

`r if(knitr:::is_latex_output()) '\\Appendices'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'`



# TECHNICAL PROCESS PARTICIPANTS  {#app:TWG}


The TWG consists of members from Fisheries and Oceans Canada, North Coast Area First Nations, Pacific Salmon Foundation, and consulting organizations (Table \@ref(tab:TWGTable)). Two independent reviewers were appointed by Canada and Alaska (Table \@ref(tab:ReviewersTable)).



```{r TWGTable, echo = FALSE, results = "asis"}


particpants.src <- read.csv("data/Reference Tables/TWG_Members.csv",stringsAsFactors = FALSE,
														fileEncoding = "UTF-8-BOM") %>% arrange(Type, Name)
  
twg.df <- particpants.src %>%
							dplyr::filter(Type == "TWG") %>%
							select(-Type)

  
csasdown::csas_table(twg.df,
  caption = "Members of the Technical Working Group (TWG).",
  format = "latex",
  #landscape = FALSE,
  font_size = 10) %>%
  kableExtra::column_spec(1, width = "12em") %>%
  kableExtra::column_spec(2, width = "35em") %>%
  kableExtra::row_spec(1:(dim(twg.df)[1]-1), hline_after = TRUE) 



```




```{r ReviewersTable, echo = FALSE, results = "asis"}



reviewers.df <- particpants.src %>%
							dplyr::filter(Type == "Reviewer") %>%
							select(-Type)

  
csasdown::csas_table(reviewers.df ,
  caption = "Independent reviewers for the escapement goal review.",
  format = "latex",
  #landscape = FALSE,
  font_size = 10) %>%
  kableExtra::column_spec(1, width = "12em") %>%
  kableExtra::column_spec(2, width = "35em") %>%
  kableExtra::row_spec(1:(dim(reviewers.df)[1]-1), hline_after = TRUE) 



```



\clearpage
# CHARACTERISTICS of ALTERNATIVE APPROACHES FOR DEVELOPING AGGREGATE MANAGEMENT REFERENCE POINTS {#AggregationAppendix}

This appendix includes one table for each of the alternative aggregation methods. All tables have the same structure: for each criterion, there is a single rating in all capitals (YES/NO/MAYBE), followed by a brief rationale. Table  \@ref(tab:TableAltApproaches) describes the aggregation methods. Table \@ref(tab:TableCriteria) describes the criteria. Table \@ref(tab:TableSummary) summarizes the results across aggregation methods.



\clearpage
(ref:TableCriteriaAggSmsy) Rationale for criteria ratings – Aggregate Smsy estimate. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaAggSmsy, echo = FALSE, results = "asis"}



criteria.table.src <- read_csv("data/AggregationApproachTables/AggregationTable_AppendixDetails.csv")

table.in <- criteria.table.src %>% dplyr::filter(Method == "Agg Smsy") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaAggSmsy)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```




\clearpage
(ref:TableCriteriaSumSmsy) Rationale for criteria ratings – Sum of stock-level Smsy estimates. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaSumSmsy, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Sum of Smsy") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaSumSmsy)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage
(ref:TableCriteriaUmsyComp) Rationale for criteria ratings – Comparison of stock-level Umsy estimates. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaUmsyComp, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Umsy Comp") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaUmsyComp)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```




\clearpage
(ref:TableCriteriaSpnStkEquProf) Rationale for criteria ratings – Stock-level equilibrium profiles based on fixed escapement targets. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaSpnStkEquProf, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Spn Equ Prof") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaSpnStkEquProf)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage
(ref:TableCriteriaERAggEquProf) Rationale for criteria ratings – Aggregate-level equilibrium profiles based on fixed exploitation rate targets. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaERAggEquProf, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "ER Equ Prof") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaERAggEquProf)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```





\clearpage
(ref:TableCriteriaStockStatus) Rationale for criteria ratings – Stock-level status considerations. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaStockStatus, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Stock Status") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaStockStatus)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```







\clearpage
(ref:TableCriteriaLogReg) Rationale for criteria ratings – Logistic Regression. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaLogReg, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Log Reg") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaLogReg)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```











\clearpage
(ref:TableCriteriaSim) Rationale for criteria ratings – Forward Simulation. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaSim, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Sim") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaSim)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "7em") %>%
     kableExtra::column_spec(2, width = "40em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```







<!--chapter:end:06_01_TWG.Rmd-->

\clearpage
# CUSTOM R PACKAGES AND FUNCTIONS {#SingleStockSRFits}


## *RapidRicker* Package

### Purpose

As part of this project, we started developing the R package *RapidRicker*, which runs spawner-recruit data quality checks, tests the sensitivity of standard biological benchmarks using simple Ricker fits, and implements Bayesian SR fits using the JAGS sampling engine [@Plummer03jags] via the *jags()* function from the *R2jags* package [@R2jags]. 

The motivation for building this package was the large number of stocks covered by the Skeena and Nass Sockeye escapement goal review. Routine aspects of data review, such as checking for potential outliers or concerns regarding contrast, presented a non-trivial challenge in an analysis covering dozens of stocks within 2 aggregates, with data continuously being updated as the data reviews progressed. With the large number of stocks, we also faced the challenge of being consistent across stocks with data treatment choices (e.g., criteria for identifying outliers).

Most of the analyses in this report were implemented using the *RapidRicker* package. A basic worked example and the JAGS code follow. Package functions are available on [Github](https://github.com/SOLV-Code/RapidRicker).

### Setting Up


```{r, eval=F, echo=T}

# Install
install.packages("devtools") # Install the devtools package
library(devtools) # Load the devtools package.
install_github("SOLV-Code/RapidRicker", dependencies = TRUE,
               build_vignettes = FALSE)

# Load
library(RapidRicker)	
library(tidyverse)	

# check the built in data set
?SR_Sample # opens help file
head(SR_Sample) # shows the first few rows

# check the function help files
?checkSRData
?calcDetModelFit
?calcDetRickerBM
?calcMCMCModelFit
?calcMCMCRickerBM 
```


### Run the Data Check

```{r, eval=F, echo=T}

# look at the default criteria for the data check 
flags_default

# apply the data check to data for 1 stock
data.chk <- checkSRData(SR_Sample[SR_Sample$Stock == "Stock1",])
names(data.chk)
print(data.chk$Summary)
print(head(data.chk$Data))

```

### Test the Simple Deterministic Ricker Fits

```{r, eval=F, echo=T}

det.fit <- calcDetModelFit(sr_obj = SR_Sample[SR_Sample$Stock == "Stock1",],
                              sr.scale = 10^6, min.obs=15)
det.fit

det.bm  <- calcDetRickerBM(fit_obj = det.fit,sr.scale = 10^6, 
                    Smsy.method = "Scheuerell2016",
					     Sgen.method = "Connorsetal2022")
det.bm

```


### Test the Bayesian Fits


```{r, eval=F, echo=T}

sr.use <- SR_Sample[SR_Sample$Stock == "Stock1",] %>% select(Year, Spn, Rec,logRpS)
head(sr.use)
sr.scale.use <- 10^6

#default priors and inits
priors.up <- generatePriors(sr_obj = sr.use , sr.scale=10^6, model_type = "Basic",
                            capacity.prior.type = "uniform")
inits.up <- generateInits(priors.up)

test.basic.up <- calcMCMCModelFit(
  sr_obj = sr.use, sr.scale = sr.scale.use  ,
  model.type = "Basic",
  model.file = "BUILT_IN_MODEL_Ricker_UniformCapPrior.txt",
  min.obs = 15,
  mcmc.settings = list(n.chains = 2, n.burnin = 20000, 
  										 n.thin = 60, n.samples = 80000),
  mcmc.inits = inits.up,
  mcmc.priors = priors.up,
  mcmc.output = "post",
  mcmc.out.path = "MCMC_Out",
  mcmc.out.label = "MCMC",
  mcmc.seed = "default",
  tracing = FALSE
)
names(test.basic.up)
head(test.basic.up$Summary)

```



\clearpage
## JAGS Code for single-stock SR model fits


### JAGS Code for Basic Ricker Model

```{r, eval=F, echo=T}
model{
	# adapted from code originally developed by Catherine Michielsens, Sue Grant, 
	# and Bronwyn MacDonald. Modifications based on comments and code samples 
	# from Ann-Marie Huang, Brendan Connors, Charmaine Carr-Harris, and 
	# Wendell Challenger.

    for (i in 1:N) {                #loop over N sample points
      R_Obs[i] ~ dlnorm(logR[i],tau_R)    #likelihood 
      logR[i] <- RS[i] +log(S[i])         # calc log(R) fitted values
       RS[i] <- ln.alpha - beta * S[i]     # ricker model
	  log.resid[i] <-  log(R_Obs[i]) - logR[i] 
   }

    ln.alpha ~ dnorm(p.alpha,tau_alpha)  #prior for ln.alpha 
    beta <- 1/ S.max				     # prior for beta

   # capacity prior: uniform OR lognormal. Use only one!!!!!
   S.max ~ dunif(1/10^6, max.scalar * smax.in )  # data is in millions
   
   S.max ~ dlnorm(log(smax.in), tau_smax) T(0,smax.limit)
	smax.limit <- max.scalar * smax.in # typical default  3 * (Max Obs)
    
    
	# non-updating samples (so can plot priors)
	S.max.prior ~ dunif(1/10^6, max.scalar * smax.in)
	ln.alpha.prior ~ dnorm(p.alpha,tau_alpha)
	
	#prior for precision parameter
    tau_R ~ dgamma(shape.tau_R,lambda_tau_R)  
    sigma <- 1/sqrt(tau_R) 			
    
	# bias correction for lognormal skewness
	ln.alpha.c <- ln.alpha + (sigma * sigma / 2) 

}
```

\clearpage

### JAGS Code for AR1 Ricker Model


```{r, eval=F, echo=T}
model{

# This is a JAGS version of the Ricker model fit with lag-1 autoregression 
# correction (AR1). Adapted from code originally developed by Catherine 
# Michielsens, Sue Grant, and Bronwyn MacDonald. Modifications based on comments
# and code samples from Ann-Marie Huang, Brendan Connors, Charmaine Carr-Harris, 
# and Wendell Challenger. The code is expanded for AR1 based on Eq21 and 22 of  
# Fleischman and Evenson (2010; ADFG FMS10-04). 

# do first year    
R_Obs[1] ~ dlnorm(logR[1],tau_R)    
logR[1] <- log(S[1]) + RS[1]    
RS[1] <- ln.alpha - beta * S[1] + phi * log.resid.0    

# do second year    
R_Obs[2] ~ dlnorm(logR[2],tau_R)    
logR[2] <- log(S[2]) + RS[2]     
RS[2] <- ln.alpha - beta * S[2] + phi * log.resid[1]    
log.resid[1] <-  log(R_Obs[1]) - logR[1]    

#loop over rext of N sample points (starting with the third)    
for (i in 2:N) { 
log.resid[i] <-  log(R_Obs[i]) - logR[i] 
}

for (i in 3:N) {       
R_Obs[i] ~ dlnorm(logR[i],tau_R)  # likelihood 
logR[i] <- log(S[i]) + RS[i]      
RS[i] <- ln.alpha - beta * S[i] + phi * log.resid[i-1] 
} 

ln.alpha ~ dnorm(p.alpha,tau_alpha)            #prior for ln.alpha     
beta <-1/S.max    # prior for beta     

# capacity prior: uniform OR lognormal. Use only one!!!!!
S.max ~ dunif(1/10^6, max.scalar * smax.in )  # data is in millions

S.max ~ dlnorm(smax.in, tau_smax) T(0,smax.limit)
smax.limit <- max.scalar * smax.in # typical default  3 * (Max Obs)
	
# non-updating samples (so can plot priors)
S.max.prior ~ dunif(1/10^6, max.scalar * smax.in)
ln.alpha.prior ~ dnorm(p.alpha,tau_alpha)

tau_R ~ dgamma(shape.tau_R,lambda_tau_R)    #prior for precision parameter     
sigma <- 1/sqrt(tau_R)   # based on Fleishman and Evenson (2010; ADFG FMS10-04)

phi ~ dnorm(0.5,0.0001) #T(0.0001,0.9999)
log.resid.0 ~ dnorm(0,tau.red) #T(-3,3)  
tau.red <- tau.white * (1-phi*phi)     
tau.white ~ dgamma(shape.tauw,lambda_tauw)    

# bias correction for lognormal skewness
ln.alpha.c <- ln.alpha + ((sigma * sigma) / (2 * (1-phi*phi)) ) 
}
```


### JAGS Code for Recursive Bayes Ricker Model with Time-varying Productivity

```{r, eval=F, echo=T}
model{
	# adapted from code by Ann-Marie Huang, which was originally contributed by 
	# Catherine Michielsens. Modifications based on comments and code samples 
	# from Ann-Marie Huang, Brendan Connors, Charmaine Carr-Harris, and Wendell 
	# Challenger.

    for (i in 1:N) {       #loop over N sample points
      R_Obs[i] ~ dlnorm(logR[i],tau_R)       #likelihood 
      logR[i] <- RS[i] +log(S[i])            # calc log(R) 
      RS[i] <- ln.alpha[i] - beta * S[i]     # ricker model
	  year[i]<-i
	  log.resid[i] <-  log(R_Obs[i]) - logR[i]  
	}

    for (i in 2:N){
          ln.alpha[i] <- ln.alpha[i-1] + w[i]
          w[i]~ dnorm(0,tauw)
    }

    #prior for alpha (actually ln.alpha!)
    
    ln.alpha[1] ~ dnorm(p.alpha,tau_alpha)    
    
    # prior for beta
    beta <-1/ S.max					   
	
	# capacity prior: uniform OR lognormal. Use only one!!!!!
   S.max ~ dunif(1/10^6, max.scalar * smax.in )  # data is in millions
	
	# non-updating samples (so can plot priors)
	S.max.prior ~ dunif(1/10^6, max.scalar * smax.in)
	ln.alpha.prior ~ dnorm(p.alpha,tau_alpha)
	
	S.max ~ dlnorm(log(smax.in), tau_smax) T(0,smax.limit)
	smax.limit <- max.scalar * smax.in # typical default  3 * (Max Obs)
	
    tau_R ~ dgamma(shape.tau_R,lambda_tau_R)    #prior for precision parameter
    sigma <- 1/sqrt(tau_R) 			# based on Fleishman and Evenson

	tauw~ dgamma(shape.tauw,lambda_tauw)
    varw<- 1/tauw
	sigw<- 1/sqrt(tauw)

   # bias correction for lognormal skewness
    for (i in 1:N) {  
			ln.alpha.c[i] <- ln.alpha[i] + (sigma * sigma / 2) 
	}
	
}

```



## Benchmark calculation functions {#BMFuns}

### R Code for Smsy Calculation  {#BMFunsSmsy}

*Rapid Ricker* includes four alternative options for calculating Smsy: (1) approximation from @Hilborn1985Proxies, (2) approximation from @PetermanPyperGrout2000ParEst, (3)
explicit solution from @Scheuerell2016, using code from the [samSim Package](https://github.com/Pacific-salmon-assess/samSim), and (4) a brute force approximation.

The main function handles inputs and specifications, and includes three of the four alternative calculation approaches:

```{r, eval=F, echo=T}

#' calcRickerSmsy
#'
#' This function calculates Smsy for a Ricker a,b parameters. Note: This function 
#' DOES NOT apply bias correction on alpha. Whether the output is bias-corrected 
#' estimates or not depends on the par set provided by the user. This keeps the 
#' parameter estimation and benchmark calculation steps clearly separated.
#' 
#' @param X  a data frame with columns ln.alpha, beta
#' @param method  one of "Hilborn1985","Petermanetal2000","Scheuerell2016", or
#'                        "BruteForce"
#' @param sr.scale scalar applied to SR data in the model fitting step, 
#'                 need it here to scale up the Sgen values
#' @param out.type either "BMOnly" or "Full"
#' @keywords Smsy
#' @export

calcRickerSmsy <- function(X, method = "Scheuerell2016",sr.scale =1, 
													 out.type = "Full"){
  
if(!(method %in% c("Hilborn1985","Petermanetal2000","Scheuerell2016",
									 "BruteForce") )){
  warning("Method must be one of Hilborn1985,Petermanetal2000,
  				Scheuerell2016, BruteForce")
  stop()}

X.orig <- X

# check for negative ln.a or b pars
X$ln.alpha[X$ln.alpha < 0] <- NA
X$beta[X$beta < 0] <- NA

do.idx <- !is.na(X$ln.alpha) & !is.na(X$beta)

smsy.est <- rep(NA, dim(X)[1] )

if(sum(do.idx)>0){

if(method == "Hilborn1985") {
  smsy.est[do.idx] <-  X$ln.alpha[do.idx]/X$beta[do.idx] *
  	                       (0.5-0.07*X$ln.alpha[do.idx]) * sr.scale  }

if(method == "Petermanetal2000") {   
  peterman.approx <- (0.5 - 0.65 * X$ln.alpha[do.idx]^1.27 / 
                          (8.7 + X$ln.alpha[do.idx]^1.27))
  smsy.est[do.idx] <- X$ln.alpha[do.idx] * peterman.approx[do.idx] / 
                            X$beta[do.idx]  * sr.scale } 

if(method == "Scheuerell2016") { 
# adapted from samSim package (https://github.com/Pacific-salmon-assess/samSim)
  smsy.est[do.idx] <- (1 - gsl::lambert_W0(exp(1 - X$ln.alpha[do.idx]))) / 
  	                        X$beta[do.idx] * sr.scale  } 
  
if(method == "BruteForce") { 
    smsy.est[do.idx] <-   mapply(smsy.proxy, ln.a = X$ln.alpha[do.idx] ,
                                b = X$beta[do.idx], sr.scale = sr.scale )}   
} # end if any do.idx 

umsy.est <- X$beta * smsy.est/sr.scale

if(out.type == "Full"){return(bind_cols(X.orig,SmsyCalc = method, 
                      	Smsy = smsy.est, Umsy = umsy.est)) }
if(out.type == "BMOnly"){return(bind_cols(Smsy = smsy.est, Umsy = umsy.est))  }

} # end calcRickerSmsy 
```

\clearpage
The brute-force approximation is implemented as a sub-routine:

```{r, eval=F, echo=T}

smsy.proxy <- function(ln.a,b,sr.scale){

if(!is.na(ln.a) & !is.na(b)){
spn.check <- seq((1/sr.scale), 1/b ,length.out = 3000)  
rec.check <-  ricker.rec(S = spn.check,ricker.lna = ln.a, ricker.b = b)
test.df <- data.frame(Spn = spn.check, Rec = rec.check) %>% 
		mutate(Yield = Rec-Spn) %>% arrange(-Rec)
s.msy <- spn.check[which.max(rec.check - spn.check) ]  * sr.scale
}

if(is.na(ln.a) | is.na(b)){s.msy <- NA}

return(s.msy)
}

```


### R Code for Sgen Calculation   {#BMFunsSgen}

*Rapid Ricker* includes four alternative options for calculating Sgen: (1) solver function extracted from @HoltOgden2013, (2) solver function extracted from the [samSim R package](https://github.com/Pacific-salmon-assess/samSim), (3) solver function used in @Connorsetal2022 and generously shared by the lead author, and (4) a brute force approximation.

The main function handles inputs and specifications, and includes three of the four alternative calculation approaches:

```{r, eval=F, echo=T}

#' calcRickerSgen
#'
#' This function calculates Sgen for a set of Ricker ln.a,b,sigma parameters, 
#' and optionally Smsy. NOTE: If method is "HoltOgden2013", then Smsy is always
#'  calculated based on Hilborn (1985) approximation, and if Smsy is provided, 
#'  it will give a warning that it was ignored. Note: This function DOES NOT 
#'  apply bias correction on alpha. Whether the output is bias-corrected 
#'  estimates or not depends on the par set provided by the user. This keeps 
#'  the parameter estimation and benchark calculation steps clearly separated.
#'
#' @param X  a data frame with columns ln.alpha, beta, sigma, and optionally Smsy
#' @param method  one of "HoltOgden2013", "samSim", "Connorsetal2022","BruteForce"
#' @param sr.scale scalar applied to SR data in the model fitting step, 
#'                    need it here to scale up the Sgen values
#' @param out.type either "BMOnly" or "Full"
#' @keywords Sgen
#' @export

calcRickerSgen <- function(X, method = "Connorsetal2022",sr.scale = 1, 
                               out.type = "Full",tracing = FALSE){

if(!(method %in% c("HoltOgden2013", "samSim", "Connorsetal2022",
									 "BruteForce") )){
  warning("Method must be one of HoltOgden2013, SamSim, Connorsetal2022, 
  				BruteForce")
  stop()}

X.orig <- X

# check for negative ln.a or b pars
X$ln.alpha[X$ln.alpha < 0] <- NA
X$beta[X$beta < 0] <- NA

do.idx <- !is.na(X$ln.alpha) & !is.na(X$beta) 
sgen.est <- rep(NA, dim(X)[1] )

if(sum(do.idx)>0){

#---------------------------------------------
if(method == "HoltOgden2013") {

  if(!is.null(X$Smsy[do.idx]) & sum(is.na(X$Smsy[do.idx])) == 0){
  	warning("Smsy provided as input, but not used for this method! ")}
  if(is.null(X$sigma)){X$sigma <- 1}
   sgen.est[do.idx] <- unlist(mapply(Sgen.solver.HO, 
                                a = exp(X$ln.alpha[do.idx]), 
                                b = X$beta[do.idx], 
                                sig = X$sigma[do.idx]))  * sr.scale
} # end if HoltOgden2013

#--------------------------------------------
if(method == "samSim") {

if(is.null(X$Smsy[do.idx]) | sum(is.na(X$Smsy[do.idx])) > 0){
	warning("Need to provide Smsy column in input data frame for this method! ")
	stop()}

     if(is.null(X$sigma)){X$sigma <- 1}


  samsim.out <-  mapply(sGenSolver.samSim.wrapper, ln.a = X$ln.alpha[do.idx], 
                                b = X$beta[do.idx], 
                                sigma = X$sigma[do.idx],
                                SMSY = X$Smsy[do.idx])
   sgen.est[do.idx] <- samsim.out  * sr.scale
} # end if samSim

#---------------------------------------------
if(method == "Connorsetal2022") {

  if(is.null(X$Smsy[do.idx]) | sum(is.na(X$Smsy[do.idx])) > 0){
  	warning("Need to provide Smsy column in input data frame for this method! ")
  	stop()}
  # https://stackoverflow.com/questions/38961221/uniroot-solution-in-r
  bc.out<-   mapply(get_Sgen.bc, a = exp(X$ln.alpha[do.idx]),b = X$beta[do.idx],
  									int_lower = -1, int_upper =  1/X$b[do.idx]*2, 
  									SMSY = X$Smsy[do.idx]/sr.scale)
    sgen.est[do.idx] <- bc.out * sr.scale
}  # end if "Connorsetal2022"

if(method == "BruteForce") {

  if(is.null(X$Smsy[do.idx]) | sum(is.na(X$Smsy[do.idx])) > 0){
  	warning("Need to provide Smsy column in input data frame for this method! ")
  	stop()}

  sgen.est[do.idx] <-   mapply(sgen.proxy, ln.a = X$ln.alpha[do.idx] ,
									b = X$beta[do.idx], 
									Smsy = X$Smsy[do.idx], 
									sr.scale = sr.scale )

  }
} # end if any do.idx

if(out.type == "Full"){
	      return(bind_cols(X.orig,SgenCalc = method,Sgen = sgen.est) %>% 
	      			 	mutate(Ratio = round(Smsy/Sgen,2) )) }
if(out.type == "BMOnly"){return(sgen.est)  }

} # end calcRickerSgen
```


Solver subroutine for the @HoltOgden2013 implementation

```{r, eval=F, echo=T}

Sgen.model.HO <-function(S,a,b,sig,trace = FALSE){
  PR<-a*S*exp(-b*S)
  SMSY<-(log(a)/b)*(0.5-0.07*log(a))
  epsilon.wna=log(SMSY)-log(PR)	#residuals
  epsilon=as.numeric(na.omit(epsilon.wna))
  nloglike=sum(dnorm(epsilon,0,sig, log=T))
  if(is.na(sum(dnorm(epsilon,0,sig, log=T)))==TRUE) print(c(a,b,sig))
  return(list(PR=PR, epsilon=epsilon, nloglike=nloglike))
  #actually returns postive loglikelihood (CH note)
}

Sgen.fn.HO <- function(S,a,b,sig){ -1.0*Sgen.model.HO(S,a,b,sig)$nloglike}	
#gives the min Ricker LL

Sgen.solver.HO <- function(a,b,sig) {
  SMSY<-(log(a)/b)*(0.5-0.07*log(a))

  SRfit=optimize(f=Sgen.fn.HO,interval=c(0, SMSY), a=a, b=b, sig=sig)	 
  # nb: not optim() !!
  return(list(SRfit=SRfit$minimum))  # returns the minimum S
}
```

Solver subroutines for the samSim implementation

```{r, eval=F, echo=T}
sGenSolver.samSim.wrapper <- function(ln.a, b, sigma,SMSY){
  sgen.val <- sGenSolver.samSim( theta = c(ln.a, b, sigma), sMSY = SMSY)
  sgen.out <- as.numeric(sgen.val)
  return(sgen.out)
}

sGenOptimum.samSim <- function(S, theta, sMSY) {
  a = theta[1]
  b = theta[2]
  sig = exp(theta[3])
  prt <- S * exp(a - b * S)
  epsilon <- log(sMSY) - log(prt)
  nLogLike <- sum(dnorm(epsilon, 0, sig, log = T))

  return(list(prt = prt, epsilon = epsilon, nLogLike = nLogLike, S = S))
}

sGenSolver.samSim <- function(theta, sMSY) {
  #gives the min Ricker log-likelihood
  fnSGen <- function(S, theta, sMSY) -1.0 * 
                         sGenOptimum.samSim(S, theta, sMSY)$nLogLike
  fit <- optimize(f = fnSGen, interval = c(0, ((theta[1] / theta[2]) * 
                                                (0.5 - 0.07 * theta[1]))),
                  theta = theta, sMSY = sMSY)
  return(list(fit = fit$minimum))
}
```


Solver subroutines for the @Connorsetal2022 implementation
 
```{r, eval=F, echo=T}
get_Sgen.bc <- function(a, b, int_lower, int_upper, SMSY) {
  fun_Sgen.bc <- function(Sgen, a, b, SMSY) {Sgen * a * exp( - b* Sgen) - SMSY}
  Sgen <- uniroot(fun_Sgen.bc, interval=c(int_lower, int_upper), 
  								a=a, b=b, SMSY=SMSY)$root
  }
```


The brute-force approximation is implemented as a sub-routine:

```{r, eval=F, echo=T}
ricker.rec  <- function(S,ricker.lna,ricker.b) {
	                            exp( (ricker.lna - ricker.b * S) + log(S) )}

sgen.proxy <- function(ln.a,b,Smsy, sr.scale){

if(!is.na(ln.a) & !is.na(b)){

spn.check <- seq((1/sr.scale),1.5*Smsy/sr.scale,length.out = 3000)
rec.check <-  ricker.rec(S = spn.check,ricker.lna = ln.a, ricker.b = b)
s.gen <- min(spn.check[rec.check > Smsy/sr.scale],na.rm=TRUE) *sr.scale
return(s.gen)

}}

```


## ER-based equlibrium profile functions {#EquProfFuns}

This function calculates equilibrium spawner abundance and equilibrium catch under the following assumptions: (1) There is no recruitment or age-at-return process variability (all v_{y,j} are 0); (2)  All populations are fished at equal exploitation rates; (3) The same exploitation rate is used year after year; (4) Productivity of component stocks is stable over time; (5) there are no other significant sources of mortality (e.g., no en-route mortality, no pre-spawn mortality). Function developed from code shared by Brendan Connors (DFO), implementing the approach from @SchnuteKronlund1996Equprof.

The main functions handles inputs, settings for the calculations, and outputs:

```{r, eval=F, echo=T}
#' calcAggEqProf
#'
#' This function calculates equilibrium profiles using eq_ricker_us() for 
#' each MCMC sample in the input file, then generates stock-level and 
#' aggregate-level summaries across MCMC samples.
#' #' @param data.use a data frame with columns SampleID, Aggregate, StkID, 
#'                       Stock , Umsy,Smsy, and Sgen
#' @export

calcAggEqProf <- function(data.use){

for(u.do in seq(0, 1, 0.01)){

print(paste("doing U =",u.do))

  #u.do <- 0.7

  out.raw <- bind_cols(
    data.use %>% select(SampleID,Aggregate,StkID,Stock),
    eq_ricker_us(U_msy = data.use$Umsy, S_msy = data.use$Smsy, 
                       S_gen = data.use$Sgen , U.check = u.do)
  )

  tmp.out.bystk <- out.raw %>% group_by(Aggregate, StkID, Stock) %>%
    summarize(U = median(U),
              NumSamples =  n(),
              NumNA = sum(is.na(S)),
              ProbOverfished = sum(overfished,na.rm=TRUE)/n(),
              ProbExtirpated = sum(extirpated,na.rm=TRUE)/n(),
              ProbBelowSgen = sum(belowSgen,na.rm=TRUE)/n(),
              EqSpn_p10 = quantile(S,probs=0.1,na.rm=TRUE),
              EqSpn_p25 = quantile(S,probs=0.25,na.rm=TRUE),
              EqSpn_Med = median(S,na.rm=TRUE),
              EqSpn_p75 = quantile(S,probs=0.75,na.rm=TRUE),
              EqSpn_p90 = quantile(S,probs=0.9,na.rm=TRUE),
              EqCt_p10 = quantile(C,probs=0.1,na.rm=TRUE),
              EqCt_p25 = quantile(C,probs=0.25,na.rm=TRUE),
              EqCt_Med = median(C,na.rm=TRUE),
              EqCt_p75 = quantile(C,probs=0.75,na.rm=TRUE),
              EqCt_p90 = quantile(C,probs=0.9,na.rm=TRUE),
              .groups = "keep"
    )

  tmp.agg.sums <- out.raw %>% group_by(Aggregate,SampleID) %>%
    summarize(U = median(U),AggSpn = sum(S,na.rm=TRUE), 
              AggCt = sum(C,na.rm=TRUE),
              NumStks = n(),
              NumStksOverfished = sum(overfished,na.rm=TRUE),
              NumStksExtirpated = sum(extirpated,na.rm=TRUE),
              NumStksBelowSgen = sum(belowSgen,na.rm=TRUE),
              .groups = "keep"
    )

  tmp.out.byagg <- tmp.agg.sums %>% group_by(Aggregate) %>%
    summarize(U = median(U),
              NumSamples =  n(),
              NumNA = sum(is.na(AggSpn)),
              NumStksOverfished_p10 = quantile(NumStksOverfished,
                       probs=0.1,na.rm=TRUE),
              NumStksOverfished_p25 = quantile(NumStksOverfished,
                       probs=0.25,na.rm=TRUE),
              NumStksOverfished_Med = median(NumStksOverfished,na.rm=TRUE),
              NumStksOverfished_p75 = quantile(NumStksOverfished,
                       probs=0.75,na.rm=TRUE),
              NumStksOverfished_p90 = quantile(NumStksOverfished,
                       probs=0.9,na.rm=TRUE),
              NumStksExtirpated_p10 = quantile(NumStksExtirpated,
                       probs=0.1,na.rm=TRUE),
              NumStksExtirpated_p25 = quantile(NumStksExtirpated,
                        probs=0.25,na.rm=TRUE),
              NumStksExtirpated_Med = median(NumStksExtirpated,na.rm=TRUE),
              NumStksExtirpated_p75 = quantile(NumStksExtirpated,
                       probs=0.75,na.rm=TRUE),
              NumStksExtirpated_p90 = quantile(NumStksExtirpated,
                       probs=0.9,na.rm=TRUE),
              NumStksBelowSgen_p10 = quantile(NumStksBelowSgen,
                       probs=0.1,na.rm=TRUE),
              NumStksBelowSgen_p25 = quantile(NumStksBelowSgen,
                       probs=0.25,na.rm=TRUE),
              NumStksBelowSgen_Med = median(NumStksBelowSgen,na.rm=TRUE),
              NumStksBelowSgen_p75 = quantile(NumStksBelowSgen,
                       probs=0.75,na.rm=TRUE),
              NumStksBelowSgen_p90 = quantile(NumStksBelowSgen,
                       probs=0.9,na.rm=TRUE),
              EqSpn_p10 = quantile(AggSpn,probs=0.1,na.rm=TRUE),
              EqSpn_p25 = quantile(AggSpn,probs=0.25,na.rm=TRUE),
              EqSpn_Med = median(AggSpn,na.rm=TRUE),
              EqSpn_p75 = quantile(AggSpn,probs=0.75,na.rm=TRUE),
              EqSpn_p90 = quantile(AggSpn,probs=0.9,na.rm=TRUE),
              EqCt_p10 = quantile(AggCt,probs=0.1,na.rm=TRUE),
              EqCt_p25 = quantile(AggCt,probs=0.25,na.rm=TRUE),
              EqCt_Med = median(AggCt,na.rm=TRUE),
              EqCt_p75 = quantile(AggCt,probs=0.75,na.rm=TRUE),
              EqCt_p90 = quantile(AggCt,probs=0.9,na.rm=TRUE),
              .groups = "keep"
    )

    if(exists("out.summary.stk")){ out.summary.stk <- 
                       bind_rows(out.summary.stk,tmp.out.bystk) }
  if(!exists("out.summary.stk")){ out.summary.stk <- tmp.out.bystk }

  if(exists("out.summary.agg")){ out.summary.agg <- 
                       bind_rows(out.summary.agg,tmp.out.byagg) }
  if(!exists("out.summary.agg")){ out.summary.agg <- tmp.out.byagg }

  out.summary.stk <- out.summary.stk %>% arrange(Aggregate, StkID)
  out.summary.agg <- out.summary.agg %>% arrange(Aggregate)

} # end looping through U values

# alternate aggregation calc
out.summary.agg.stk.pm <- out.summary.stk %>% group_by(Aggregate,U) %>%
  summarize(NumStksOverfishedv2p20 = sum(ProbOverfished >= 0.2),
            NumStksExtirpatedv2p20 = sum(ProbExtirpated >= 0.2),
            NumStksBelowSgenv2p20 = sum(ProbBelowSgen >= 0.2),
            NumStksOverfishedv2p40 = sum(ProbOverfished >= 0.4),
            NumStksExtirpatedv2p40 = sum(ProbExtirpated >= 0.4),
            NumStksBelowSgenv2p40 = sum(ProbBelowSgen >= 0.4),
            .groups = "keep"
  )

out.summary.agg <- out.summary.agg %>% left_join(out.summary.agg.stk.pm, 
                       by = c("Aggregate","U"))

return(list(summary.stk = out.summary.stk, summary.agg = out.summary.agg))

}
```

The core calculations are done in  subroutine:

```{r, eval=F, echo=T}

#' eq_ricker_us
#'

#' @param U_msy  point estimate of exploitation rate at MSY, typically 
#'          one MCMC sample. This version uses S_msy and U_msy as inputs. 
#'          eq_ricker_ab() does the comparative calculation using ln.alpha and 
#'          beta inputs. 
#' @param S_msy point estimate of exploitation rate at MSY, typically one 
#'                 MCMC sample
#' @param S_gen point estimate of Sgen, the spawner abundance that allows 
#'                  rebuilding to Smsy in 1 generation in absence of fishing
#' @param U.check vector with ER increments to evaluate, default is 0.5
#' @export
#' @examples

eq_ricker_us <- function(U_msy, S_msy, S_gen, U.check = 0.5) {
  Seq <- ((U_msy - log((1 - U_msy)/(1 - U.check)))/U_msy) * S_msy
  Seq[Seq < 0] <- 0

  Ceq <- (U.check * Seq)/(1 - U.check)
  Ceq[is.na(Ceq)] <- 0
  Ceq[Ceq < 0] <- 0

  overfished <- ifelse(U.check > U_msy, 1, 0)
  extirpated <- ifelse(Seq == 0, 1, 0)
  belowSgen <- ifelse(Seq < S_gen, 1, 0)

  return(data.frame(U = U.check, S = Seq, C = Ceq, 
                       overfished = overfished, extirpated= extirpated,
                       belowSgen = belowSgen))
}
```





<!--chapter:end:06_03-appendix_SingleStockFits.Rmd-->


\clearpage
# HIERARCHICAL BAYESIAN MODEL (HBM) OF SOCKEYE SALMON STOCK-RECRUIT DATA IN THE SKEENA RIVER SYSTEMS {#app:HBMFits}

<!-- 
**FINAL REVISIONS NEEDED: (1) FIX EQUATION THAT HAD TO BE TAKEN OUT, (2) CHECK THROUGH REVIEWS FOR EDITS** 
-->



```{r,echo=FALSE, message=FALSE}
library(tidyverse)
library(csasdown)

HBM.results <- readRDS("data/HBM/HBM_appendix_tables.rds")

pasteList <- function(x, suffix=NULL) {
  n <- length(x)
  if (!is.null(suffix)) x <- paste0(x, suffix)
  if(n == 1) return(x)
  a <- paste(head(x, n=(n-1)), collapse = ", ")
  paste(c(a, tail(x, 1)), collapse = ", and ")
}
# str(HBM.results)

# TASKS
# - Standardize terms: lognormal or log-normal,  capitalize Normal?
# - methods - include JAGS
# - [ ] prettyNum
# -[ ] intro to sensitivity
# - [ ] Citations
# - [ ] shrinkage 
# - [ ] Umsy definition
```

<!-- 
Questions

Normal draws were defined in terms of precision, does the remainder of the document

See equation \@ref(eq:HBMLinRick)
-->

**This appendix contributed by Murdoch McAllister and Wendell Challenger**

*Note: This appendix is a stand-alone contribution to the Research Document. It has been included
as a cross-check on key results (e.g., productivity scenarios), and to establish a link between
the single-stock SR analyses present in the main report and previous work by Korman and English (2013) which used a hierarchical Bayesian approach. This appendix applies the approach of Korman and English (2013) to the updated SR data and includes extensive sensitivity testing to allow more direct comparisons with the single-stock model fits.*

A hierarchical Bayesian model (HBM) that was developed for the estimation of Ricker stock-recruit model parameters for Sockeye salmon stocks in the Skeena river system. There was insufficient time to complete an HBM for the Nass River basin.  This structuring was used because it was assumed the exchangeability in the stock productivity parameter was restricted to stocks within each basin.  Differences in the frequency distribution of life history attributes of stocks such as age composition of spawners, the relative frequency of lake rearing and non-lake rearing life history types and differences in run timings and marine migration routes exist between Sockeye salmon stocks in these two watersheds.  



## Mathematical and Statistical Formulation of the HBM {#app:HBMFits-1}

The Skeena HBM models use a linearized form of the Ricker stock-recruit model:


\begin{equation}
\log{\left(\frac{R_{s,y}}{S_{s,y}}\right)=a_s-}\beta_s\cdot\ S_{s,y}+\epsilon_{s,y}
(\#eq:HBMLinRick)
\end{equation}

where $R_{s,y}$ is the observed abundance of recruits produced in stock $s$ in brood year $y$ by the corresponding abundance of spawners brood year $S_{s,y}$. The parameter $a_s$ is the natural logarithm of the maximum rate of population increase, $\beta_s$ is the stock-specific coefficient for the density-dependent effect of spawner abundance on stock productivity and $\epsilon_{s,y}$ represents stock by  year error term that is assumed to be Normally distributed. 


The HBM framework presented is based on that formulated by @KormanEnglish2013 who estimated stock-recruit parameters for numerous salmon stocks in the Skeena River system.  The HBM presumes, as @KormanEnglish2013 did, that the Ricker $a$ parameter is exchangeable between stocks within a river system with a hyper prior mean and hyper prior standard deviation:

\begin{equation}
  a_s \sim  \ln \textrm{Normal}\left(\mu_a,\ \tau_a\right) 
\end{equation}

where $\mu_a$ is the natural logarithm of the hyper prior median of the Ricker $a$ parameter, and $\tau_a$ is the hyperprior precision of the Ricker $a$ parameter.  The following hyperpriors were assigned for $\mu_a$ and $\tau_a$:
  
\begin{equation}
  \mu_a \sim \textrm{Normal}\left({0.5,\ 10^{-6}}\right)
  \textrm{, and }
  \tau_a \sim  \textrm{Gamma} \left(0.5,\ 0.5 \right).
\end{equation}

@KormanEnglish2013 choose these priors as vague priors and the same hyperpriors were retained in the current formulation.




A informative log-normal prior was also assumed for density-dependency parameter  $\beta_s$ which follows the formulation used in @KormanEnglish2013:  

\begin{equation}
  \beta_s \sim \ln \textrm{Normal}\left(\mu_{\beta,s},\ \tau_{\beta,s}\right)
\end{equation}

where $\mu_{\beta,s}$  was obtained based on results from previous limnological lake productivity analyses that assessed the spawner abundance (i.e., $S_{\textrm{max},s})$ which could generate on average the maximum number of recruits for each stock. The values $\mu_{\beta,s}$  and $\tau_{\beta,s}$ were determined as:

  
\begin{equation}
  \mu_{\beta,s}= \log \left(  \frac{1}{S_{\textrm{max},s}} \right)
  \textrm{,  and  } 
  \tau_{\beta,s}=\left(\frac{1}{\sigma_{\textrm{max},s}}\right)^2.
\end{equation}
  
where $\sigma_{\textrm{max},s}$  is the prior standard deviation in the natural logarithm of $S_{\textrm{max},s}$.  Note that values for $\sigma_{\textrm{max},s}$  are nearly identical to the coefficient of variation (CV) for smaller values of $\sigma_{\textrm{max},s}$ (i.e., about 0.5 and lower) and as such we use CV and $\sigma_{\textrm{max},s}$ interchangeably when describing precision of the prior. That said, the ratio of CV to $\sigma_{\textrm{max},s}$ increases to noticeably larger than 1 (i.e., CV is larger than $\sigma_{\textrm{max},s}$) at larger values of $\sigma_{\textrm{max},s}$ meaning the true CV is larger than stated and the prior will be more diffuse. The values  of $S_{\textrm{max},s}$ and $\sigma_{\textrm{max},s}$ for each stock in the Skeena watershed are provided in Table \@ref(tab:PrSmax).

<!-- TABLE C.1. Smax Priors -->
```{r PrSmax, echo=FALSE, results='asis'}
cap <- "Prior median values for stock-specific $S_\\textrm{max}$ and the prior standard deviation in the natural logarithm of $S_\\textrm{max}$ (i.e., $\\sigma_{\\textrm{max},s}$) based on results from previous lake productivity analyses.  The two instances where three stocks are listed in the same line, the available mean $S_\\textrm{max}$ values were added since the individual rearing lakes for these stocks were geographically very close together and the stock-recruit data for these stocks were thus combined into a single time series of stock-recruit data for stock-recruit parameter estimation."
# data.frame(Yes = 1:10, No = LETTERS[11:20]) %>%
HBM.results$Smax %>%
  filter(Basin == "Skeena") %>%
  mutate(prSmax = round(prSmax)) %>%
  rename(
    `$S_{\\textrm{max},s}$` = prSmax,
    `CV/$\\sigma_{\\textrm{max},s}$` = prCV
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l","l", "l", "r", "r"),
    caption = cap
  )
```
The values for $\sigma_{\textrm{max},s}$ were obtained for most of the Skeena Sockeye salmon stocks from @KormanEnglish2013. It should be noted that a value for $\sigma_{\textrm{max},s}$ of 2.00 makes the corresponding prior for $S_{\textrm{max},s}$ vague and diffuse. For example,  relative to the median, the lower and upper 95% credible intervals would be about fifty times lower and fifty times higher than the median.  In cases where $\sigma_{\textrm{max},s}$ was set at 2.00, at least some members of the Working Group had raised concerns about the empirical basis for the values for $S_{\textrm{max},s}$ that could be obtained for those stocks.  The Babine stocks represented a stock aggregate that was divided five ways, the Asitka was based on an approximation and Morice was a two lake system where only the larger lake was measured. Due to these issues diffuse priors were used for these stocks.

At the time of analysis no lake productivity estimate was available for Asitka.  As a result a crude approximation of the prior median $S_{\textrm{max},s}$ for Asitka was obtained from the product of the lake surface area for Asitka and the average $S_{\textrm{max},s}$ per unit lake area for Skeena watershed lakes for which $S_{\textrm{max},s}$ estimates for Sockeye salmon were available.  Due the large uncertainty associated with $S_{\textrm{max},s}$ for Asitka, a value of 2.00 was assumed for $\sigma_{\textrm{max},s}$ for Askita which makes this into a vague prior.   

For the five Babine Lake stocks there was admittedly highly uncertain over the appropriate stock-specific lake capacity due to uncertainty over how the common resource is shared between the five stocks as juveniles for at least five of the recognized Babine stocks rear in Babine Lake.  The prior median for $S_{\textrm{max},s}$ for each of the five Babine Lake stocks were obtained by dividing the total Babine lake productivity estimate by  five, providing an equal allocation of the shared resource to each Babine stock.  Because the true proportion of lake productivity used by each stock remains unknown a high level of uncertainty (i.e., a $\sigma_{\textrm{max},s}$  value 2.00) was assigned for each for each Babine Lake stock.

As in @KormanEnglish2013 a Normal likelihood function was applied in the HBM and the same prior was applied to the standard deviation in the deviations between predicted and observed productivity:

\begin{equation}
  \log{\left(\frac{R_{s,y}}{S_{s,y}}\right) \sim   \textrm{Normal}\left(\mu_{s,y}^{RS},\  \tau_s \right)} 
\end{equation}

where $\log(R_{s,y}/S_{s,y})$ is the observed natural logarithm of the ratio of recruits produced by spawners (i.e., productivity) in brood year $y$ for stock $s$, $\mu_{s,y}^{RS}$ is the stock-specific productivity predicted by the Ricker stock-recruit model (see equation \@ref(eq:HBMLinRick)), conditioned on parameters $a_s$ and $\beta_s$, and $\tau_s$ is the stock-specific precision used in the likelihood function and is given by:

\begin{equation}
  \tau_s=\frac{1}{\sigma_s^2}
\end{equation}
  
where $\sigma_s$ is the stock-specific standard deviation in the deviates between observed and predicted productivity.  The prior for $\sigma_s$ is the same as that applied by @KormanEnglish2013:

\begin{equation}
  \sigma_s \sim \textrm{Uniform}(0.05,\ 10).
\end{equation}

The original @KormanEnglish2013 HBM did not consider time varying changes in productivity; because this was of interest in the current investigation, the @KormanEnglish2013 HBM was extended to include a common shared year effect on stock productivity (i.e., $\log(R_{s,y}/S_{s,y})$). It is hypothesized that due to the spatial proximity of the Sockeye salmon stocks within a basin (i.e., Skeena basin) and overlap between stocks in migration pathways in freshwater and the marine environment that the annual productivity of different stocks will deviate from Ricker model predictions in the same direction and with similar magnitude in each year.   Therefore, in addition to a stock-specific deviate between the Ricker model prediction of productivity and the observed productivity, a common shared deviate or year effect was also included in the HBM.  The linearized form of the Ricker model that includes the common shared year effect $T_y$ in year $y$ is thus given by:

\begin{equation}
  \mu_{s,y}^{RS}=a_s-\beta_s\cdot S_{s,y}+T_y
\end{equation}
  

The prior for $T_y$ is given by,

\begin{equation}
  T_y \sim \textrm{Normal}\left(0,\ \tau_T\right)
\end{equation}
  
where $\tau_T$  is the prior precision in the common shared year effect that is computed as $\tau_T=1/\sigma_T^2$ with $\sigma_T$ representing the standard deviation in common shared year effects.  The hyperprior for $\sigma_T$  was determined as:

\begin{equation}
  \sigma_T \sim \ln \textrm{Normal} \left(\log{\left(0.8\right),\ \frac{1}{{0.7}^2}}\right).
\end{equation}
  
The prior for $\sigma_T$ was chosen to be mildly informative with the prior central tendency and prior precision set at values to allow the data to inform a large range of values and accommodate potentially large interannual variation in common shared year effects. 

The estimated common shared year effects are expected to represent average deviations from Ricker model stock-productivity predictions across stocks in a given year.  The common shared year effect could potentially result from better than average or worse than average survival rates from natural mortality within a given year experienced by the stocks in either freshwater or saltwater.  However, it could also result from run reconstruction errors, for example, underestimation or overestimation of the total catch. 

## Model Fitting

The MCMC algorithms applied for posterior integration of the HBM included WinBUGS 1.4.3 and JAGS and practically identical parameter estimates were obtained between the two software packages.    Under the initial WinBUGS implementations the burn-in was achieved within about 10,000 iterations and well-pronounced posterior density functions were obtained for parameter estimates.   However, under both software packages, for some of the stocks, the MCMC chains could stray very rarely to extreme low values for the Ricker $\beta$ parameter or extreme high values for the Ricker $a$ parameter.  Extreme low values for the Ricker $\beta$ parameter map out to extreme high values, e.g., ten times the value of the posterior mode, for derived parameters such as the average unfished spawner abundance, $S_0$, or spawning stock abundance associated with the maximum sustainable yield, $S_{msy}$.  Such extreme outlier values were considered to be well outside of the support of the data and may be artifacts of the operation of the MCMC algorithms applied. 

To prevent the inclusion of extreme outlier values, a minimum boundary was applied to the prior for the Ricker $\beta$ parameter.  The prior minimum value for the Ricker $\beta$ parameter was established as follows.  An upper bound on the value for $S_\textrm{max}$ was obtained for each stock as five times the prior central tendency specified for $S_\textrm{max}$ :  




\begin{equation}
  S_{\textrm{max},\textrm{max},s} = 5 \times S_{\textrm{max},s} \textrm{ and } \beta_{\textrm{min},s}=\frac{1}{S_{\textrm{max},\textrm{max},s}}. 
\end{equation}


The adjusted prior for $\beta_s$ that was applied was thus:

\begin{equation}
  \textrm{max}\left(\beta_s \sim \ \ln \textrm{Normal}\left(\mu_{\beta,s},\ \ \tau_{\beta,s}\right),\ \beta_{\textrm{min},s}\right)
\end{equation}

The application of this modified prior for $\beta_s$ thus prevented anomalously low values for $\beta_s$ and anomalously large MCMC values for abundance reference points such as  $S_\textrm{max}$, $S_\textrm{msy}$, and $S_0$. 


The posterior predictive distribution for the Ricker $a$ parameter for an unsampled population, $a_p$, is given by:

\begin{equation}
  a_p \sim\  \ln \textrm{Normal}\left(\mu_a,\ \tau_a\right)
  (\#eq:unsamp)
\end{equation}
  
This distribution reflects the effective prior density function for the Ricker $a$ parameter that was applied to estimate this parameter for each stock. 

To provide approximations of how productivity could be varying systematically over years, representations of time varying productivity were obtained by adding the running $n = 4$ or 5 year averages of the common shared year effects to the $a$ parameter for each stock:

\begin{equation}
  a_{s,y,n}=a_s+\frac{1}{n}\cdot\sum_{i=y-n+1}^{y}T_i
  (\#eq:timevarprod)
\end{equation}


Final estimates of the posterior distributions were derive in JAGS by running six 
independent chains with differen starting points for 100,000 MCMC iterations 
after a burn-in of 20,000. Posterior samples were thinned keeping every 10th 
sample in order to reduce auto-correlation in both fundamental and derived 
parameters. Convergence was assess through a combination of diagnostic plots (e.g., traceplots, posterior distributions and Gelman-Rubin-Brooks plots) and criterion such as Rhat, Gelman and Rubin's 
potential scale reduction factor [@GelmanRubin1992], including the multivariate version [@BrooksGelman1998], and Geweke's diagnostic [@Geweke1992]. In all cases plots showed goods sampling
from the posterior with little to no autocorrelation and all diagnostic 
criterion were within ranges that are generally associated with 
convergence (i.e., Rhat < 1.05, Gelman-Rubin within [0.99, 1.01], and Geweke within [-2,2]).




## Sensitivity Analyses {#app:HBMFits-2}

Some additional model runs were implemented to evaluate some different features of the HBM.  See Table \@ref(tab:SensitivityRuns) for brief descriptions of these additional model runs. We term the model run using above described specifications for the HBM the ''base case''.  Note that the results reported below were obtained from earlier WinBUGS 1.4.3 code versions of the HBM which included a simpler approximation for computing $S_\textrm{MSY}$ [i.e., from Table 7.2 of @HilbornWalters1992] than was used in the main body of the report.  The results from these additional runs are summarized further below.   


  
<!-- TABLE C.2. Sensitivity Analyses -->
```{r SensitivityRuns, echo=FALSE, results='asis'}
cap <- "Description of sensitivity runs to evalute the sensitivity of estimation results to model structure and some key inputs."

data.frame(
  `Run` = c(seq(1,6), "7-24", "25", "26"), 
  Description = c(
    "HBM base case but including Korman and English’s (2013) coding in computing precision in the likelihood function from $\\sigma$",
    "Same as HBM base case but with no prior lower bounds on $\\beta$.",
    "Same as HBM base case but leaving out common shared year effects.",
    "Non-hierarchical model run with no common shared year effect but including the same  $S_\\textrm{max}$ prior information as in the base case HBM.",
    "Same as HBM base case but normal priors on  $S_\\textrm{max}$ instead of the base case lognormal prior on Ricker $\\beta$.",
    "Same as HBM base case but with vague Ricker $\\beta$ priors, but including the prior lower bound on Ricker $\\beta$.",
    "Leave out stock-recruit data in the HBM, one stock at a time.",
    "Leave out stock-recruit data for the Babine Enhanced stocks in the HBM (i.e., Pinkut and Fulton).",
    "Same as HBM base case but leave out Babine Enhanced stocks (i.e., Pinkut and Fulton) and apply vague  $S_\\textrm{max}$ priors to Bear, Kitwanga, and Sustut."
  ),
  check.names = FALSE
) %>%
  csas_table(
    format = "latex", 
    escape = FALSE, 
    font_size = 10, 
    # align = c("p{1.2cm}","p{12cm}"),
    align = c("l", "l"),
    caption = cap, #paste("(tab:SensitivityRuns)", cap)
  ) %>%
  kableExtra::row_spec(seq(1,8), hline_after = TRUE) %>%
  kableExtra::column_spec(2, width = "12cm")
```


## Results

### Shrinkage in the Ricker $a$ Parameter in Going From a Non-HBM to an HBM  {#HBMShrinkage}

```{r, echo=FALSE}
shrink <- (HBM.results$Shrinkage$Shrinkage * 100) %>% range() %>% round
kitwanga <- round((HBM.results$Shrinkage %>% filter(Stock == "Kitwanga") %>% select(Shrinkage) %>% unlist)*100 )
pinkut <- round((HBM.results$Shrinkage %>% filter(Stock == "Pinkut") %>% select(Shrinkage) %>% unlist)*100 )
fulton <- round((HBM.results$Shrinkage %>% filter(Stock == "Fulton") %>% select(Shrinkage) %>% unlist)*100 )
other <- HBM.results$Shrinkage %>% filter(Stock %in% c("Kitwanga", "Pinkut", "Fulton" == FALSE)) %>% select(Shrinkage) %>% unlist %>% range
other <-round(other*100,1)
sig.sd <- (HBM.results$Shrinkage$`Diff.SD` * -100) %>% range() %>% round(digits = 1)  # % reduction
```

In both the non-HBM and HBM models a common shared year effect was estimated to make the shrinkage analysis valid.  Under the non-HBM the prior for the Ricker $a$ parameter for each stock was similarly lognormal, had $a$ prior median of $\log(0.5)$ and precision of 1.  Compared to the non-HBM a moderate amount of shrinkage can be seen seen for the HBM estimates (Figure \@ref(fig:HBMShrinkage)). For example the Asitka stock which had the highest posterior median value for the Ricker $a$ parameter under both models showed a `r -shrink[1]`% decrease in the Ricker $a$ parameter (Table \@ref(tab:HBMShrinkageVal)).  This was the largest percentage decrease among the Skeena Sockeye salmon stocks included in the HBM.  In contrast the Kitwanga stock which had the second lowest posterior median for the Ricker $a$ parameter under the non-HBM showed the largest increase of `r kitwanga`% when going from the non-HBM to the HBM.  In contrast the Pinkut and Fulton stocks which had posterior medians for Ricker $a$ under the non-HBM close to the middle range of the Posterior median estimates showed very little shrinkage, i.e., `r pinkut`% and `r fulton`%, respectively (Figure \@ref(fig:HBMShrinkage), Table \@ref(tab:HBMShrinkageVal)).  For all of the 18 Skeena Sockeye salmon stocks the posterior standard deviation (SD) for Ricker $a$ parameter estimates were all smaller, i.e., `r sig.sd[1]`% to `r sig.sd[2]`% smaller, under the HBM than under the non-HBM (Table \@ref(tab:HBMShrinkageVal)).


<!-- Figure C.1. Shrinkage Plot -->
```{r, echo=FALSE}
cap <- "Shrinkage plot for posterior median values for the Ricker $a$ parameter obtained under non-HBM and HBM models of stock-recruit data for 18 Skeena River Sockeye salmon stocks."
```
```{r HBMShrinkage, echo=FALSE, fig.width=7, fig.cap=cap}
HBM.results$Shrinkage %>%
  select(Stock, ends_with("median")) %>%
  gather(key=Model, value = Median, ends_with("median")) %>%
  mutate(
    Model = str_replace(Model, "[\\.[:space:]]median", "") %>% factor(levels = c("nonHBM", "HBM"))
  ) %>%
  ggplot(., aes(x=Model, y = Median)) +
  geom_point(aes(color = Stock)) +
  geom_line(aes(group = Stock, color = Stock)) +
  theme_classic(14) +
  theme(legend.title = element_blank()) +
  labs(
    y = "Posterior Median"
  )
```

<!-- TABLE C.3. Ricker a shrinkage -->
```{r HBMShrinkageVal, echo=FALSE, results='asis'}
cap <- "Posterior median estimates and standard deviations (SD) for the Ricker $a$ parameter for the 18 Skeena River Sockeye salmon stocks under non-HBM and HBM models  The percentage change shows the percentage change in the parameter estimate in going from the non-HBM to the HBM implementation.  "

HBM.results[['Shrinkage']] %>%
  mutate(
    nonHBM.median = round(nonHBM.median, 3),
    HBM.median= round(HBM.median, 3),
    nonHBM.SD = round(nonHBM.SD, 3),
    HBM.SD = round(HBM.SD, 3),
    Shrinkage = paste0(round(Shrinkage * 100,1), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100,1), "\\%")
  ) %>%
  rename(
    `nonHBM Median` = nonHBM.median,
    `HBM Median` = HBM.median,
    `nonHBM SD` = nonHBM.SD,
    `HBM SD` = HBM.SD,
    `SD \\% Diff` = Diff.SD,
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap #paste("(ref:tabSenRun1)", cap)
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

### Common Shared Year Effect

The common shared year effects represent average deviations from Ricker model stock-productivity predictions across stocks in a given year (Figure \@ref(fig:HBMYrEff)). Estimates varied from year to year with many years being close to the Ricker model predictions, while other years showing notable positive and negative deviations The 1994 common shared year effect represented the largest deviation from Ricker stock-productivity predictions, with a strong reduction in productivity. The 4-year and 5-year rolling means provide smoothed representations of estimated common shared year effects within four and five year blocks and make it easier to visualize potential common shared trends in stock productivity than plots of annual estimates.  The common shared year effects derive from the average of stock productivity deviates across the stocks where stock-recruit data are available in a given year and acquire credibility through commonality in annual productivity deviates across several of the stocks.  In contrast, Kalman filter representations of time varying productivity rely on apparent time series of deviates from Ricker-model predictions of long-term average productivity based on data from a single stock.  The Kalman filter approach does not allow any potential statistical representation of common patterns in productivity deviates between stocks in larger watershed. Also in contrast to the common-shared year effect approach the Kalman filter approach cannot be applied to stock-recruit data time series where there are one or more missing years of data within a time series.

<!-- FIGURE C.2. Common Shared Year Effect -->
```{r, echo=FALSE}
cap <- "Estimated common shared year effect shared across the 18 Skeena stocks with a 4-year rolling mean (A) and a 5-year rolling mean (B) overlaid. Points indicate mean with error bars indicating the 95\\% credible intervals, while line and shading indicates the mean and 95\\% credible intervals for the rolling mean."
```
```{r HBMYrEff, fig.width=7.5, fig.cap=cap}
include_graphics("data/HBM/Appendix-HBM-common_year_eff.pdf")
```

While these yearly deviations, whether from common shared year effects or Kalman filter, could represent improved or diminished survival in the fresh or saltwater system, they could also represent an under or overestimation in total catch, and there is currently no way to distinguish between authentic shared survival rate effects and run reconstruction error effects. As such, negative or positive year effects cannot be directly attributed to either an environmental or run reconstruction effect as the two will be confounded. 

As such, time varying productivity should be viewed as a method to improve model fitting, but does not provide objective criteria that can be used to set or judge management responses.   For example, where there were runs of strongly positive or negative common shared year effects in recent years, this could reflect either runs of either high or low stock productivity shared between stocks.  Alternatively, it could reflect a run of years in which fishery catches for all or most stocks were systematically under-reported or over-reported.  If the latter hypothesis were correct, then any adjustments to harvest control rules, e.g., adjustments to escapement targets, to attempt to respond to apparent productivity changes, could have unintended consequences in achieving fishery and salmon population conservation objectives.

To evaluate the sensitivity of reference points to estimated changes in productivity, high and low productivity periods were identified and the posterior predictive distributions for $S_{msy}$ and \Umsy were computed based on the common shared year effects in these blocks of years based on estimates of time varying productivity by stock (see equation \@ref(eq:timevarprod)).  The years showing the largest mostly positive common shared year effects included 1980-1992 (Fig. \@ref(fig:HBMYrEff)).  The years showing the lowest mostly negative common shared year effects included 1999-2014 (Fig. \@ref(fig:HBMYrEff)).  $S_{msy}$ and \Umsy estimates were significantly lower in the latter block of years with the posterior means for \Smsy and also \Umsy ranging between 21% and 50% lower than those estimates in the high productivity period from 1980-1992 (Tables \@ref(tab:SmsyProd) and \@ref(tab:UmsyProd)).  The 95% credible intervals for the percentage differences for both \Smsy and \Umsy included only negative values (Tables \@ref(tab:SmsyProd) and \@ref(tab:UmsyProd)).  These results suggest that systematic reductions in stock productivity could result in estimates of both \Umsy and \Smsy being lowered.  Should \Smsy be used in forming escapement targets and reference points for fishery management and conservation, lowered productivity could result in lower escapement targets.  Because the \Umsy estimates are highly correlated with the \Smsy estimates, the \Umsy estimates could also be expected to decrease with decreases in \Smsy estimates and it would be appropriate also to reduce harvest rates to accommodate lowered productivity.  However, if only changes in estimates of \Smsy were considered in responses to apparent changes in stock productivity and reductions in harvest rates were not implemented, this could also lead to unintended consequences in meeting stock conservation and fishery objectives.

<!-- TABLE C.4. Smsy in high/low years -->
```{r SmsyProd, echo=FALSE}
cap <- "Comparison of stock-specific $S_\\textrm{MSY}$ in high (i.e., 1980-1992) and low (i.e., 1999-2014) periods of productivity and the percentage differnce between periods."

HBM.results[['Smsy Productivity']] %>%
  mutate(
    High = round(High),
    Low = round(Low),
    Diff      = ifelse(is.na(Diff), NA, paste0(round(Diff*100), "\\%")),
    Diff_2.5  = ifelse(is.na(Diff), NA, paste0(round(Diff_2.5*100), "\\%")),
    Diff_97.5 = ifelse(is.na(Diff), NA, paste0(round(Diff_97.5*100), "\\%"))
  ) %>%
  # select(Low, High, Diff, Diff_2.5, Diff_97.5) %>%
  rename(
    `High` = High,
    `Low` = Low,
    `\\% Diff` = Diff,
    `Lower CI` = Diff_2.5,
    `Upper CI` =Diff_97.5
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 5)),
    caption = cap #paste("(ref:tabSenRun1)", cap)
  ) %>%
  kableExtra::column_spec(2:5, width = "1.5cm")
```

\clearpage
<!-- TABLE C.5. Umsy in high/low years -->
```{r UmsyProd, echo=FALSE}
cap <- "Comparison of stock-specific $U_\\textrm{MSY}$ in high (i.e., 1980-1992) and low (i.e., 1999-2014) periods of productivity and the percentage differnce between periods."

HBM.results[['Umsy Productivity']] %>%
  mutate(
    High = round(High),
    Low = round(Low),
    Diff      = ifelse(is.na(Diff), NA, paste0(round(Diff*100), "\\%")),
    Diff_2.5  = ifelse(is.na(Diff), NA, paste0(round(Diff_2.5*100), "\\%")),
    Diff_97.5 = ifelse(is.na(Diff), NA, paste0(round(Diff_97.5*100), "\\%"))
  ) %>%
  # select(Low, High, Diff, Diff_2.5, Diff_97.5) %>%
  rename(
    `High` = High,
    `Low` = Low,
    `\\% Diff` = Diff,
    `Lower CI` = Diff_2.5,
    `Upper CI` =Diff_97.5
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 5)),
    caption = cap #paste("(ref:tabSenRun1)", cap)
  ) %>%
  kableExtra::column_spec(2:5, width = "1.5cm")
```

\newpage
### Sensitivity Run 1:  Effect of Korman and English’s (2013) Coding Error for their HBM {#app:HBMFits-2-1}
```{r, echo=FALSE}
sig.mean <- (HBM.results$`Run 1`$Diff.mean * 100) %>% range() %>% round
sig.sd <- (HBM.results$`Run 1`$Diff.SD * 100) %>% range() %>% round
```


There was a coding error in the model used by @KormanEnglish2013; their code incorrectly transformed the parameter $\sigma_i$ to $\tau_i$ using the incorrect code:  \texttt{tau[i]<-pow(sd[i],-0.5)} which instead should have been \texttt{tau[i]<-pow(sd[i],-2)}. This coding error changed very little the HBM posterior results for the Ricker $a$ and $\beta$ parameters for the 18 stocks (Table \@ref(tab:SenRun1)).  However, the estimates of the $\sigma_i$ parameter were between `r sig.mean[1]`% and `r sig.mean[2]`%  of those obtained under the HBM base case.   The posterior SDs for $\sigma_i$ ranged between `r sig.sd[1]`% and `r sig.sd[2]`%  larger than those obtained under the base case.  Either the lower or upper boundary point on the prior for $\sigma_i$ was hit for some of the stocks.    If the coding error had persisted, any simulations of stock-recruit data using the estimated values for $\sigma_i$ may have led to results with excessive variability.  It is thus essential for this coding error to be corrected in any further implementations of Korman and English’s (2013) HBM.

<!-- TABLE C.6 Sensitivity Run 1 -->
```{r SenRun1, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $\\sigma$ from the base case (coding error removed) and model run that included Korman and English’s (2013) coding error."

HBM.results[['Run 1']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    Code.Error.mean = round(Code.Error.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    Code.Error.SD = round(Code.Error.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100), "\\%")
  ) %>%
  rename(
    `Base Case mean $\\sigma$` = Base.Case.mean,
    `Code Error mean $\\sigma$` = Code.Error.mean,
    `\\% Diff` = Diff.mean,
    `Base Case SD($\\sigma$)` = Base.Case.SD,
    `Code Error SD($\\sigma$)` = Code.Error.SD,
    `\\% Diff ` = Diff.SD,
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap #paste("(ref:tabSenRun1)", cap)
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```



\clearpage
### Sensitivity Run 2:  Same as HBM Base Case But With No Prior Lower Bound on the Ricker $\beta$ Parameter



When there was no prior lower bound included the Ricker $\beta$ parameter by stock, the estimates of $S_\textrm{max}$ and MSY-based reference points included some extremely high MCMC chain values that were way out in the tails of the Markov chains and very far removed from the range of values with support from the data; for these stocks, posterior means, medians, and probability interval values were highly sensitive to the inclusion of these extreme outlier values in the chains (see Table \@ref(tab:SenRun2) for some example results for $S_\textrm{MSY}$).  It is common in implementations of MCMC implementations such as WinBUGS to set bounds on key variables in the model to prevent extreme outlier values in the chains from affecting the posterior calculations [e.g., @MeyerMillar1999; @MichielsensMcAllister2004].  It is thus recommended that prior lower bounds on the Ricker $\beta$ parameter be implemented for each stock to eliminate this source of bias.

The $U_\textrm{MSY}$ estimates from the HBM were relatively insensitive to lifting the prior maximum bound on $S_\textrm{max}$ (Table \@ref(tab:SenRun2Umsy)).   While posterior means differed by less than a few percent for all stocks, posterior standard deviation values were up to 13\% larger when the prior maximum bound on \Smax was lifted.  


<!-- TABLE C.7 Sensitivity Run 2 -->
```{r SenRun2, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$  from the HBM base case and model run where no prior upper bounds were placed on \\Smax."

HBM.results[['Run 2 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    No.Bound.mean = round(No.Bound.mean),
    Base.Case.SD = round(Base.Case.SD),
    No.Bound.SD = round(No.Bound.SD),
    Diff.mean = paste0(round(Diff.mean*100,1), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 1), "\\%")
  ) %>%
  rename(
    `Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
    `No Bound mean $S_\\textrm{MSY}$` = No.Bound.mean,
    `\\% Diff` = Diff.mean,
    `Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
    `No Bound SD($S_\\textrm{MSY}$)` = No.Bound.SD,
    `\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap, #paste("(ref:tabSenRun2)", cap)
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- TABLE C.8 Sensitivity Run 2 -->
```{r SenRun2Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run where no prior upper bounds were placed on \\Smax."

HBM.results[['Run 2 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean,3),
    No.Bound.mean = round(No.Bound.mean,3),
    Base.Case.SD = round(Base.Case.SD,3),
    No.Bound.SD = round(No.Bound.SD,3),
    Diff.mean = paste0(round(Diff.mean*100,1), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 1), "\\%")
  ) %>%
  rename(
    `Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
    `No Bound mean $U_\\textrm{MSY}$` = No.Bound.mean,
    `\\% Diff` = Diff.mean,
    `Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
    `No Bound SD($U_\\textrm{MSY}$)` = No.Bound.SD,
    `\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap, #paste("(ref:tabSenRun2)", cap)
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Run 3: Same as HBM Base Case But Leaving Out Common Shared Year Effects

```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 3 Smsy']]$Diff.mean * 100)  %>% range() %>% round
sig.sd <- (HBM.results[['Run 3 Smsy']]$Diff.SD * 100) %>% range()  %>% round
```

Significant, strong common shared year effects were estimated for numerous years in the 1960-2014 brood year time series.  Separating out this effect provided more precise estimates of Ricker stock recruit parameters for several of the stocks and more precise estimates of management quantities of interest.  This also allowed for common shared systematic change in stock productivity to be estimated.  The posterior means for the Smsy reference point by stock for example were between about `r sig.mean[1]`% and `r sig.mean[2]`% different between runs when common shared year effects were excluded versus accounted for (Table \@ref(tab:SenRun3)).  When no common shared year effect was included in the HBM, estimates of Ricker stock-recruit parameters and associated management parameters for several of the stocks were on average less precisely estimated with posterior SDs being up to about `r sig.sd[2]`% larger for $S_\textrm{msy}$ for several of the stocks.

The $U_\textrm{MSY}$ estimates from the HBM were relatively insensitive to removing common shared year effects from the HBM (Table \@ref(tab:SenRun3Umsy)).  

<!-- TABLE C.9 Sensitivity Run 3 -->
```{r SenRun3, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the HBM base case and model run where no prior upper bounds were placed on \\Smax."

HBM.results[['Run 3 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    No.TE.mean = round(No.TE.mean),
    Base.Case.SD = round(Base.Case.SD),
    No.TE.SD = round(No.TE.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
  	`No \\newline $T_y$ \\newline mean $S_\\textrm{MSY}$` = No.TE.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
  	`No \\newline $T_y$ \\newline SD($S_\\textrm{MSY}$)` = No.TE.SD,
  	`\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap #paste("(ref:tabSenRun3)", cap)
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- Table C.10 Sensitivity Run 3 -->
```{r SenRun3Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run where no prior upper bounds were placed on \\Smax."

HBM.results[['Run 3 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    No.TE.mean = round(No.TE.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    No.TE.SD = round(No.TE.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
  	`No \\newline $T_y$ \\newline mean $U_\\textrm{MSY}$` = No.TE.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
  	`No \\newline $T_y$ \\newline SD($U_\\textrm{MSY}$)` = No.TE.SD,
  	`\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap #paste("(ref:tabSenRun3)", cap)
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Run 4: Non-Hierarchical Model Run With No Common Shared Year Effect But Including the Same $S_\textrm{max}$ Prior Information As in the Base Case HBM

```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 4 Smsy']]$Diff.mean * 100) %>% range()  %>% round
sig.sd <- (HBM.results[['Run 4 Smsy']]$Diff.SD * 100) %>% range()  %>% round

smsy.diff <- filter(HBM.results[['Run 4 Smsy']], Stock == "Sum Smsy across stocks") %>% select(Diff.mean, Diff.SD) %>% unlist * 100 %>% round

umsy.diff <- HBM.results[['Run 4 Umsy']] %>% select(Diff.SD) %>% unlist %>% max
umsy.diff <- round(umsy.diff*100)
```


When a nonhierarchical Bayesian model(nonHBM) with no common shared year effects was run, estimates of Ricker stock-recruit parameters and associated management parameters for several of the stocks were on average less precisely estimated with posterior SD in Smsy for example on average being about `r sig.sd[1]`% to `r sig.sd[2]`% different between the nonHBM and HBM runs (Table \@ref(tab:SenRun4)).  Percentage differences between the nonHBM and HBM for posterior mean estimates for Smsy parameters ranged by stock between `r sig.mean[1]`% and `r sig.mean[2]`%.  These results indicate that on average Ricker and msy-based parameter estimates are more precisely estimated with the HBM and results for some quantities for some stocks can differ considerably.  However, the sum of the $S_\textrm{MSY}$ estimates across stocks was only about `r round(smsy.diff['Diff.mean'])`% larger under the non-hierarchical model but the posterior SD in the $S_\textrm{MSY}$ values summed over the Skeena River stocks was `r round(smsy.diff['Diff.SD'])`% larger than that for the HBM (Table \@ref(tab:SenRun4)).

The posterior means for  $U_\textrm{MSY}$ were relatively insensitive to fitting a nonHBM, but the posterior SDs for $U_\textrm{MSY}$ mostly increased by up to `r umsy.diff`% larger than those under the HBM  (Table \@ref(tab:SenRun4Umsy)).  

<!-- TABLE C.11 - Sensitivity Run 4 -->
```{r SenRun4, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the base case HBM and nonHBM run."

HBM.results[['Run 4 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    nonHBM.mean = round(nonHBM.mean),
    Base.Case.SD = round(Base.Case.SD),
    nonHBM.SD = round(nonHBM.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
  	`nonHBM \\newline mean $S_\\textrm{MSY}$` = nonHBM.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
  	`nonHBM \\newline SD($S_\\textrm{MSY}$)` = nonHBM.SD,
  	`\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- TABLE C.12 - Sensitivity Run 4 -->
```{r SenRun4Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the base case HBM and nonHBM run."

HBM.results[['Run 4 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    nonHBM.mean = round(nonHBM.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    nonHBM.SD = round(nonHBM.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
  	`nonHBM \\newline mean $U_\\textrm{MSY}$` = nonHBM.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
  	`nonHBM \\newline SD($U_\\textrm{MSY}$)` = nonHBM.SD,
  	`\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Run 5: Normal Priors on Smax in the HBM Instead of the Lognormal Prior on Ricker $\beta$

```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 5 Smsy']]$Diff.mean * 100) %>% range()  %>% round
sig.sd <- (HBM.results[['Run 5 Smsy']]$Diff.SD * 100) %>% range()  %>% round
```

When a normal prior for $S_\textrm{max}$ was applied instead of a lognormal prior on the Ricker $\beta$ parameter in the HBM, but otherwise using the same input information from the lake productivity analyses, the posterior estimates for several of the quantities became less precise and posterior estimates differed markedly for some of the quantities.  Posterior SDs for $S_\textrm{MSY}$ were for example much larger on average, e.g., between about `r sig.sd[1]`% and `r sig.sd[2]`% larger (Table \@ref(tab:SenRun5)).  Posterior mean estimates for $S_\textrm{MSY}$ differed between the two model runs by `r sig.mean[1]`% to `r sig.mean[2]`%.  Though it appears the same information is used in a normal prior for $S_\textrm{max}$, this prior on average loses information about the Ricker $\beta$ parameter compared to a prior for the Ricker $\beta$ parameter that uses the same $S_\textrm{max}$ information. 

The $U_\textrm{MSY}$ estimates from the HBM were relatively insensitive to replacing the lognormal prior for the Ricker $\beta$ parameter with a normal prior for  $S_\textrm{max}$ (Table \@ref(tab:SenRun5Umsy)). 

<!-- TABLE C.13 - Sensitivity Run 5 -->
```{r SenRun5, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the HBM base case and model run with a Normal priors on $S_\\textrm{max}$."

HBM.results[['Run 5 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    Normal.Prior.mean = round(Normal.Prior.mean),
    Base.Case.SD = round(Base.Case.SD),
    Normal.Prior.SD = round(Normal.Prior.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
  	`Normal Prior mean $S_\\textrm{MSY}$` = Normal.Prior.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
  	`Normal Prior SD($S_\\textrm{MSY}$)` = Normal.Prior.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
<!-- TABLE C.14 - Sensitivity Run 5 -->
```{r SenRun5Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run with a Normal priors on $S_\\textrm{max}$."

HBM.results[['Run 5 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    Normal.Prior.mean = round(Normal.Prior.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    Normal.Prior.SD = round(Normal.Prior.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
  	`Normal Prior mean $U_\\textrm{MSY}$` = Normal.Prior.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
  	`Normal Prior SD($U_\\textrm{MSY}$)` = Normal.Prior.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Run 6: Application of Vague Ricker $\beta$ Priors in the HBM, but Including the Prior Lower Bound on $\beta$
```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 6 Smsy']]$Diff.mean * 100) %>% range() %>% round
sig.sd <- (HBM.results[['Run 6 Smsy']]$Diff.SD * 100) %>% range() %>% round

umsy.diff <- HBM.results[['Run 6 Umsy']] %>% select(Diff.mean) %>% unlist %>% max
umsy.diff <- round(umsy.diff*100)
```

When a vague prior was applied for the Ricker $\beta$ prior for all 18 Skeena Sockeye salmon stocks in the HBM, posterior estimates for the abundance-based management quantities were less precise on average with posterior SDs, e.g., for \Smsy ranging between `r sig.sd[1]`% to `r sig.sd[2]`% larger under the vague priors (Table \@ref(tab:SenRun6)).  Posterior means for $S_\textrm{MSY}$ estimates in the HBM that used vague priors for the Ricker $\beta$ parameter were between about `r sig.mean[1]`% and `r sig.mean[2]`% different from those obtained under the mixture of informed and vague priors in the base case HBM.  The use of informative priors for the Ricker $\beta$ parameter based on prior information on $S_\textrm{max}$ via the lake productivity analyses thus combined with the data to provide more precise stock-recruit parameter estimates that in some cases differed from the less precise estimates given by the stock-recruit data.  


The $U_\textrm{MSY}$ estimates from the HBM were relatively insensitive to prior for all Ricker $\beta$ parameter vague priors though for a few of the stocks the posterior means were up to `r umsy.diff`% larger (Table \@ref(tab:SenRun6Umsy)). 

<!-- TABLE C.15 - Sensitivity Run 6 -->
```{r SenRun6, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the HBM base case and model run with a Normal priors on $S_\\textrm{max}$."

HBM.results[['Run 6 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    Vague.Prior.mean = round(Vague.Prior.mean),
    Base.Case.SD = round(Base.Case.SD),
    Vague.Prior.SD = round(Vague.Prior.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
  	`Vague $\\beta$ Prior mean $S_\\textrm{MSY}$` = Vague.Prior.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
  	`Vague $\\beta$ Prior SD($S_\\textrm{MSY}$)` = Vague.Prior.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- TABLE C.16 - Sensitivity Run 6 -->
```{r SenRun6Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run with a Normal priors on $S_\\textrm{max}$."

HBM.results[['Run 6 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    Vague.Prior.mean = round(Vague.Prior.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    Vague.Prior.SD = round(Vague.Prior.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
  	`Vague $\\beta$ Prior mean $U_\\textrm{MSY}$` = Vague.Prior.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
  	`Vague $\\beta$ Prior SD($U_\\textrm{MSY}$)` = Vague.Prior.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Runs 7-25.  Leave Out Stock-Recruit Data in the HBM, One Stock at a Time

To evaluate the relative influence of each stock-recruit data set from each stock, the HBM was run dropping out the stock-recruit data for one stock at a time.  In run 25 the stock-recruit data for both of the Babine Enhanced stocks (i.e., Fulton and Pinkut) were left out of the HBM.  The posterior predictive distribution for the Ricker $a$ parameter for an “unsampled” stock was computed for each of these runs (see equation \@ref(eq:unsamp)).  The posterior predictive distributions were plotted under each of the HBM runs 7-25 and under the base case HBM shows that the posterior predictive distributions were very similar between all HBM runs 7-25 and the base case HBM run (Figure \@ref(fig:jackProd)).  The posterior mean estimates of the time series of common shared year effects also did not change their sign or markedly change in magnitude when the stock-recruit data from one of the stocks or both of the Babine enhanced stocks were removed from the HBM (Figure \@ref(fig:jackTE)).  These results indicate that no one stock, nor the enhanced Babine Sockeye salmon stocks, had a substantive influence on the HBM results.  Furthermore results indicate that regardless of whether Pinkut and Fulton channel data were included or excluded, very similar among stocks posterior distribution of the Ricker $a$ parameter (Figure \@ref(fig:jackProd)) and shared year-effects on productivity (Figure \@ref(fig:jackTE)) were produced.  

The temporal variation in shared year effect estimates represent the combined effects of potential changes in both freshwater and marine processes (Figure \@ref(fig:jackTE)).  The combined effects of spawning channels on recruits/spawner are clearly similar to those of nearby natural spawning streams (Figure \@ref(fig:jackTE)).  However, by providing more spawning gravel, the channels have increased the total abundance of adult recruits and spawners in the Skeena system (Randall Peterman, pers. comm.).

<!-- Figure C.3 - Sensitivity Run 7-25 -->
```{r, echo=FALSE}
cap <- "Posterior predictive distributions for the Ricker $a$ parameter when the Sockeye salmon stock-recruit data from one stock at a time was dropped from the HBM for the Skeena watershed (runs 7-24) and the stock-recruit data were dropped for both of the Babine Enhanced Sockeye salmon stocks, i.e., both Pinkut and Fulton (run 25). Base case posterior predictive distribution is coloured dark grey, with colours used for runs 7-25."
```
```{r jackProd,   fig.cap=cap}
include_graphics("data/HBM/Appendix-HBM-Productivity-jackknife.pdf")
```

<!-- Figure C.4 - Sensitivity Run 7-25 -->
```{r, echo=FALSE}
cap <- "Posterior mean estimates for the common shared year effect when the Sockeye salmon stock-recruit data from one stock at a time was dropped from the HBM for the Skeena watershed (runs 7-24) and the stock-recruit data were dropped for both of the Babine Enhanced Sockeye salmon stocks, i.e., both Pinkut and Fulton (run 25). Base case posterior predictive distribution is coloured dark grey, with colours used for runs 7-25."
```
```{r jackTE,   fig.cap=cap}
include_graphics("data/HBM/Appendix-HBM-yeareffect-jackknife.pdf")
```



\clearpage
### Sensitivity Run 26: Removal of Enhanced Stocks and Vague Ricker $\beta$ Priors for Select Stocks
```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 26 Smsy']]$Diff.mean * 100) %>% range() %>% round
sig.sd <- (HBM.results[['Run 26 Smsy']]$Diff.SD * 100) %>% range() %>% round


smsy.diff <- HBM.results[['Run 26 Smsy']] %>% 
  # select(Stock, Diff.mean, Diff.SD) %>% 
  arrange (Diff.mean) %>%
  mutate(Base.Case.CV = round(Base.Case.SD /Base.Case.mean*100)) %>%
   mutate(Run.26.CV = round(Run.26.SD /Run.26.mean*100)) %>%
  mutate(Perc.Diff =  round(Diff.mean*100), Perc.SD =  round(Diff.SD*100)) %>%
  arrange(Stock)
smsy.diff2 <- smsy.diff %>% filter(Stock %in% c("Bear", "Kitwanga", "Sustut"))

umsy.diff <- HBM.results[['Run 26 Umsy']] %>% select(Stock, Diff.mean) %>% 
  arrange (Diff.mean) %>% 
  mutate(Perc =  round(Diff.mean*100)) %>%
  tail(n=2) %>%
  arrange(Stock)
# umsy.diff <- round(umsy.diff*100)
```

For the final sensitivity run (hereafter, 'run 26') the enhanced stocks were removed and vague priors were applied to the Ricker $\beta$ prior for Bear, Kitwanga and Sustut, which were stocks highlighted as stocks of interest when the HBM results were compared to results form single stock models.  Estimates of the posterior mean for the abundance-based management quantity \Smsy were generally similar to the base case (i.e., less than 10% change) for stocks where the \Smax prior was not changed relative to the base case. The exception was Babine Late Wild, which had a `r filter(smsy.diff, Stock %in% c("Babine Late Wild"))$Perc.Diff`% reduction in the \Smsy estimate, however Babine Late Wild also had the highest posterior SD estimate of all stocks with a CV of `r filter(smsy.diff, Stock %in% c("Babine Late Wild"))$Base.Case.CV`% and `r filter(smsy.diff, Stock %in% c("Babine Late Wild"))$Run.26.CV`% for  base case and run 26 respectively. For Bear, Kitwanga, and Sustut applying a vague prior resulted in declines in the posterior mean of \Smsy  from `r max(smsy.diff2$Perc.Diff)`% to `r min(smsy.diff2$Perc.Diff)`% relative to the base case. Precision in \Smsy estimates varied from stock to stock ranging from a `r -sig.sd[1]`% reduction in the posterior SD to an increase of  `r sig.sd[2]`% relative to the base case (Table \@ref(tab:SenRun26)). For Bear, and Sustut the posterior SD was reduced (`r pasteList(-filter(smsy.diff, Stock %in% c("Bear", "Sustut"))$Perc.SD, "\\%")` respectively), while for Kitwanga the Posterior SD was increased by `r filter(smsy.diff, Stock %in% c("Kitwanga"))$Perc.SD`%. For stocks where changes were not applied to the \Smax  prior changes to the posterior SD ranged from  `r filter(smsy.diff, Stock %in% c("Bear", "Sustut", "Kitwanga") == FALSE)$Perc.SD %>% min`% to `r filter(smsy.diff, Stock %in% c("Bear", "Sustut", "Kitwanga") == FALSE)$Perc.SD %>% max`%.


The \Umsy posterior estimates  were relatively insensitive to these changes implemented in run 26 (i.e., under 10% change in the posterior mean), the exceptions were `r pasteList(umsy.diff$Stock[1:2])` which had posterior means that were   `r pasteList(umsy.diff$Perc[1:2], "\\%")` larger (Table \@ref(tab:SenRun26Umsy)). 

<!-- TABLE C.17 - Sensitivity Run 26 -->
```{r SenRun26, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the HBM base case and model run where enhanced stocks were removed and vague $S_\\textrm{max}$ priors  were used for Bear, Kitwanga, and Sustut."

HBM.results[['Run 26 Smsy']] %>%
  filter(Stock %in% c("Pinkut","Fulton") == FALSE) %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    Run.26.mean = round(Run.26.mean),
    Base.Case.SD = round(Base.Case.SD),
    Run.26.SD = round(Run.26.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean \\Smsy` = Base.Case.mean,
  	`Run 26 mean \\Smsy` = Run.26.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD(\\Smsy)` = Base.Case.SD,
  	`Run 26 SD(\\Smsy)` = Run.26.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- TABLE C.18 - Sensitivity Run 26 -->
```{r SenRun26Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run where enhanced stocks were removed and vague  $S_\\textrm{max}$ priors were used for Bear, Kitwanga, and Sustut."

HBM.results[['Run 26 Umsy']] %>%
  filter(Stock %in% c("Pinkut","Fulton") == FALSE) %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    Run.26.mean = round(Run.26.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    Run.26.SD = round(Run.26.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean \\Umsy` = Base.Case.mean,
  	`Run 26 mean \\Umsy` = Run.26.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD(\\Umsy)` = Base.Case.SD,
  	`Run 26 SD(\\Umsy)` = Run.26.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```


## Summary

HBM analysis provides an effective approach to formulating priors for key parameters in Bayesian ecological models and has often been applied to estimate productivity parameters for Pacific and Atlantic salmon and other fish species populations [@PrevostOthers2003; @MichielsensMcAllister2004; @SuPetermanHaeseker2004; @Clark2004; @ForestOthers2010; @PulkkinenOthers2011].  The diagnostics shown in this appendix which are routine to HBM analysis [e.g., @GelmanOthers2004; @MichielsensMcAllister2004; @ForestOthers2010] support the basic HBM assumption of exchangeability of the datasets from all 18 stocks with regards to the Ricker  $a$ parameter.  We did not find evidence to support the notion that the two enhanced stocks have consistently different productivity than the non-enhanced stocks.  This is for example based on results (1) from both HBM and non-HBM runs shown in Table \@ref(tab:HBMShrinkageVal) where the posterior means for the Ricker $a$ parameter for the two enhanced stocks were neither consistently higher nor lower than estimates from the 16 non-enhanced stocks, and (2) from the jackknife analysis which showed that the posterior predictive distribution for Ricker $a$ was highly insensitive to leaving out the two enhanced stocks, either one at a time or both at the same time (Figure \@ref(fig:jackProd)).  

Comparisons from the non-HBM and HBM also show that there was a moderate but not extreme amount of shrinkage in Ricker $a$ parameter estimates, where the maximum percentage change in Ricker $a$ was no more than 20% and less than 10% in 13 of the 18 stocks when going from the non-HBM to the HBM.  Shrinkage is to be expected in HBM analysis and the amount of shrinkage found here was not excessive and not so for the two enhanced stocks.  The posterior SDs of the Ricker $a$ parameter for the two enhanced stocks from the non-HBM and HBM (Table \@ref(tab:HBMShrinkageVal)) were also neither consistently larger nor smaller than the posterior SDs for this parameter for the 16 wild stocks.  The estimates of derived parameters such as \Smsy for the two enhanced stocks were also not excessively imprecise with posterior CVs for \Smsy being 0.56 for Pinkut and 0.37 for Fulton (Table \@ref(tab:SenRun2)).  These posterior CVs are well within the range of posterior CVs for fish stock parameters commonly estimated using Bayesian methods. <!--These results thus also refute the notion that due to lack of contrast in the spawner data from the practice of channel loading, the parameter estimates obtained from fitting a Ricker model to their stock recruit data will be excessively imprecise.  -->

Estimation of common shared year effects showed that the estimated effects were statistically significant with 95% PIs not overlapping with zero for 18 of the 55 years.  The estimated effects were highly pronounced in some years, and could change abruptly and considerably in relatively few years.  For example, the estimated effects went from the absolute minimum to the absolute maximum extreme in three years, i.e., -1.37 in 1994 to +1.02 in 1997.  And the most pronounced year effect of all could be linked to epizootic disease event, e.g., in 1994, indicating that at least some of the estimated effects could be attributed to ecological events and the notion that the estimated effects were all due to stock assessment errors is unlikely.  

The above results on common shared year effects suggest that caution is needed should it be of interest to implement random walk models for the Ricker $a$ parameter.  Random walk models typically assume that there exists positive correlation between adjacent years, e.g., for the Ricker $a$ parameter, and tend to provide smoothed trajectories of parameter estimates.  When estimates of common shared year effects from models that exclude random walk jump abruptly been years and shift from minimum to maximum value in only a few years, bias in annual estimates of the Ricker $a$ parameter could thus be expected from random walk models for this parameter.  The common shared year effect also suggested, despite frequent extreme variation between years, extended periods of on average potentially higher productivity in the 1980s to early 1990s and lower than average productivity from 2003-2014.  Estimates of quantities of interest such as \Smsy were also moderately sensitive to the inclusion and exclusion of common shared year effects in the HBM (Table \@ref(tab:SenRun3))).  

Careful attention to how the prior for \Smax was applied in MCMC was also an important consideration in the analysis.  In MCMC integration, values for estimated parameters in Markov Chains even after burn in can wander considerably and occasionally jump to extreme values well outside of the area of support of the data when no constraints are imposed.  The presence of extreme values in approximations of posteriors from MCMC can potentially cause pronounced bias in the estimated parameters.  We found that this was the case in the application of the non-HBM and HBM and extreme parameter values and the incidence in MCMC chains of quantities of interest such as Smsy could be eliminated by implementation of upper and lower bounds for key parameters such as \Smax or Ricker $\beta$.  It is thus appropriate to apply upper and lower bounds for key parameters in MCMC code to prevent these rare but extreme values from occurring and biasing posterior results.  

<!--chapter:end:06_04-appendix_HBM.Rmd-->


# SENSITIVITY TESTS: DATA TREATMENT AND BENCHMARK CALCULATIONS

## Spawner-Recruit Data Filtering and Infilling {#AltSRTest}

### Purpose

We filtered out implausible spawner-recruit observations and infilled gaps to allow fitting model forms that require complete time series (Section \@ref(AvailableSRData)). We tested the effect of alternative data treatments on benchmark estimates from the basic Ricker model.

Infilling a few return years can drastically increase the number of brood years available for spawner-recruit analyses.  For example, if a single spawner estimate is missing from the time series, then recruits cannot be calculated for 3-6 earlier brood years, depending on the age composition for the stock. If there are several gaps, many brood years may have incomplete cohort information and can't be used in the analyses. 

### Methods

We applied two alternative data filter options and then either infilled or didn't infill 1-yr gaps in spawner estimates. Infilled spawner values were calculated as the average of previous and subsequent estimates, and then the corresponding run size was calculated using the year-specific exploitation rate estimate from the run reconstruction models. The infilled spawner and run size estimates were then used in the recruit calculation based on on available age composition data.

This generated six alternative versions of the spawner-recruit time series: 

* *Main*: original data set generated by the data review documented in @SkeenaNassSkDataRep
* *Filter1k*: exclude brood years where R/S > 1,000 
* *Filter45*: exclude brood years where R/S > 45
* *Main_Infill*: original data with infills where possible
* *Filter1k_Infill*: Filter1k data with infills where possible
* *Filter45_Infill*: Filter45 data with infills where possible

This sensitivity test applied the Basic Ricker model (Section \@ref(ModelForms)) with capped uniform capacity priors (Section \@ref(Priors)) to all stocks where any filtering or infilling occurred. The Basic Ricker model is the only one that can be applied to all stocks, because it does not require a continuous time series.

### Results


There were very few cases where a filtered year could be infilled afterwards (Table \@ref(tab:AltSRTestTab1)). The number of infilled return years and resulting additional brood year estimates varied between stocks. In some cases, a few infills allowed for many additional brood year estimates. For example, infilling spawner and run size estimates for Bear made it possible to complete recruit estimates for another 13 brood years (from 36 to 49 data points). 

Benchmark and parameter estimates were quite stable across data variations for some stocks (e.g., Bear, Johnston, Sustut, Kitsumkalum, Mcdonell), but very sensitive for others (e.g., Kwinageese, Swan/Stephens). 



### Conclusions

We chose to use the *Filter45_Infill* version of the data for the analyses presented in this Research Document, because it excluded several extreme outliers and it completed the time series for several stocks, allowing AR1 and TVP models to be applied.



(ref:AltSRTestTab1) Summary of Filtering and Infilling Test. For each data version, table lists the number of spawner-recruit data points (BrYr), the number of filtered years that were infilled and included (Filter), the number of years the were infilled (Infill), and the resulting % change in median posterior estimates of Ricker parameters and standard benchmarks. All SR model fits used the Bayesian Basic Ricker (BR) with capped uniform prior (cu), with the same MCMC settings (as described in Section \@ref(SRFitting)).

```{r AltSRTestTab1, echo = FALSE, results = "asis"}


table.in <- alt.sr.test1 %>% mutate_all(as.character) %>% select(-Model)

# https://stackoverflow.com/questions/36084147/index-of-the-first-occurence-of-each-value-in-a-vector
lines.idx <- which(!duplicated(table.in$Stock))[-1]  # drop first element


table.in[is.na(table.in)] <- ""
table.in$Stock[duplicated(table.in$Stock)] <- ""

col.names.use = c("Stock","Version","BrYr","Filter","Infill","beta","ln.alpha","Seq","Smsy","Sgen")



   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
	 mutate_all(function(x){gsub("_", "\\\\_", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l",rep("r",8)),
                  caption = "(ref:AltSRTestTab1)", col.names = col.names.use ) %>%
   add_header_above(c(" " = 1, "Data" = 4, "Change in median estimates (%)" = 5)) %>%
     kableExtra::row_spec(lines.idx -1, hline_after = TRUE) #%>%
     #kableExtra::column_spec(3, width = "10em") %>%
     #  kableExtra::row_spec(c(1:5,7:14,16:22), extra_latex_after = "\\cmidrule(l){2-8}") 

```


\clearpage
## Infilling with a Generic State-Space Model  {#StateSpaceTest}

### Purpose

The infilling approach and sensitivity test summarized in Section \@ref(AltSRTest) were debated at the peer review meeting in April 2022, and further sensitivity testing of the infill approach was requested. 

Rather than setting up a bootstrap test to evaluate sensitivity, we decided to test a Bayesian approach that has been commonly used for Alaskan and northern transboundary salmon escapement goal analyses: a  Bayesian state-space model that integrates the run reconstruction and spawner-recruit parameter estimation steps into a single model fit [e.g., @BernardJones2010AlsekCk; @HamazakietalKusko2012; @Fleischmanetal2013CJFASStateSpace; @FleischmanMcKinleyKenai2013; @MillerPestalTakuSk; @Connorsetal2022]. 

In this type of model we don't need to infill missing brood years up front to fit AR1 or TVP model forms, but instead the model searches for SR parameters *and* annual estimates of run reconstruction components (e.g., spawners, harvest, age composition) that *together* give the best fit. Any missing brood years are filled in as part of the Bayesian run reconstruction.

Previous applications have been highly case-specific in terms of the run reconstruction components and their prior distributions. For example:

*  @Fleischmanetal2013CJFASStateSpace modelled the Karluk River Chinook run based on a weir count in the lower river, three different fisheries below the weir (subsistence, recreational, commercial), and a recreational fishery above the weir. Observation errors were specified based on the estimate type: weir counts and commercial harvest estimates based on fish sales slips were considered precise, but recreational harvest  estimates based on mail-in surveys were considered more uncertain.
* @FleischmanMcKinleyKenai2013 modelled the late run of Kenai River Chinook using eight components covering various time periods and locations: multi-beam sonar, in-river test fishery, split-beam sonar,  lower river sport fishery, commercial set-net fishery, sonar echo-length, radio-telemetry capture-recapture estimates, and genetic capture-recapture estimates.
* @MillerPestalTakuSk modelled the Taku Sockeye run reconstruction based on three components: in-river mark-recapture estimates at the border, below-border harvests, and above border harvests.

This level of detail is prohibitive for our project covering 20 stocks in two aggregates. However, a generic version of an integrated run reconstruction and spawner-recruit model could be applied efficiently across multiple stocks while providing some flexibility for stock-specific considerations. Such a generic state-space model is being developed by Toshihide Hamazaki (ADF&G), who generously shared an [interactive online prototype](https://hamachan.shinyapps.io/Spawner_Recruit_Bayes/) implemented in Shiny-R. We refer to this tool as the *Hamazaki App* throughout the paper.

The Hamazaki App allows users to fit alternative SR models, explore standard probability profiles based on the SR parameters (e.g., probability of achieving at least 75% of MSY at different fixed escapement targets), and even generate simple forward simulations with different types of harvest strategy. The state-space option in the Hamazaki App implements the methods described in @HamazakietalKusko2012, but simplifies the run reconstruction to three components:  Harvest estimates, either escapement or run, and run age composition.  For each annual abundance observation, users can specify a level of uncertainty, expressed as a CV, and for run age composition a weight to be used, expressed as an effective sample size (*efn*). With a structure like this, users can capture changes in assessment approach over time (e.g., earlier data based on aerial surveys can be assigned a larger CV than more recent estimates from a capture-recapture program). Individual run age composition observations that are considered very poor can be down weighted by assigning higher CV (e.g., if the weir was washed out partway through the season and the estimate was expanded to account for it) or lower efn (e.g., if a year has fewer completed age readings).


### Methods

We used the Hamazaki App to test 10 alternative versions of SR model fit, covering three Ricker model forms (Basic, AR1, and TVP; Section \@ref(ModelForms)), two estimate types (regular, state-space), and two data sets (with or without infilling). Only the Basic Ricker model could be applied to data without infilling with the regular estimation approach, but in the state-space approach all three model forms could be applied.

We tested these alternatives on two stocks: Kwinageese, which has a shorter time series and four missing brood years, and Lakelse, which has a longer time series and two missing brood years (Figure \@ref(fig:SRDataOverview)). For both stocks, we assigned moderate uncertainty to the spawner and run data (CV = 0.2) and large effective sample size (efn = 100). The "no infill" version of the data for the state-space estimates used the infilled numbers in order to populate all the fields in the data file, but assigned much larger uncertainty (CV = 0.6) and a very low effective sample size (efn = 0), so that the state-space model puts very little weight on the infilled values in the estimation step. The "infill" version of the data for the state-space estimates used the infilled numbers and assigned a large effective sample size (efn = 100), so that the model treats the infilled values just like  observed values.

Note that results for the time-varying productivity model (TVP) are not directly comparable to our results. The Hamazaki App reports average parameter and benchmark estimates across all brood years as the default, and those estimates are reported here. However, in our analyses we subsampled from various time windows (Section \@ref(ModelSelection)) to generate alternative productivity scenarios (e.g., last 2 generations). The Hamazaki App also identifies shifts in productivity regimes and generates benchmark estimates for each regime, but we did not fully explore this feature, and do not report the results here.


### Results

For both stocks, Bayesian parameter estimates for all 10 alternative fits converged and generated median posterior estimates of biological benchmarks (Figure \@ref(fig:StateSpaceComp), Tables \@ref(tab:StateSpaceTab1) and \@ref(tab:StateSpaceTab2)). However, the sensitivity of estimates differed between stocks and varied between benchmarks: (1) Benchmark estimates were less sensitive than abundance estimates for individual brood years, (2) Smax and Seq estimates were more sensitive than Smsy estimates; (3) Lakelse estimates were more sensitive than Kwinageese estimates, even though Kwinageese has fewer brood years of SR data and has more missing years in the time series.

For all state-space model fits, the posterior distribution of spawner estimates was more uncertain (i.e., wider) with the "no infill" version of the data (with larger CV on the input values) and the median estimate differed depending on the SR model form (Figure \@ref(fig:StateSpaceComp)). The difference in posterior median abundance estimates was larger for Lakelse than for Kwinageese.

Median posterior benchmark estimates for Kwinageese are so similar across the 10 alternative fits that they are identical for practical purposes (Table \@ref(tab:StateSpaceTab1)). A more in-depth comparison may show differences in the shape of the posteriors (i.e., wider or narrower, more or less skewed), but this would require more thorough testing of the model settings (i.e., the CV and efn values) and the MCMC specifications (i.e., sample size, burn-in, thinning), which falls outside the scope of this example.

Median posterior benchmark estimates for Lakelse differ more between model forms and estimate types than between data versions with or without infilling (Table \@ref(tab:StateSpaceTab2)). State-space estimates are lower than the regular Bayesian estimates for all model forms and data versions.



### Conclusions

For the two stocks tested in this example, the effect of infilling depends more on stock-specific details (e.g., what the scatter of SR data points looks like, and where the infill values fall) and model fitting approach than the specific details of the infilling step itself. This result supports our current infilling approach for this round of work, and sets the stage for future work to more fully explore the strengths and limitations of applying generic state-space models across all 20 modelled Skeena and Nass Sockeye stocks. 



\clearpage

(ref:StateSpaceComp) State-space posterior estimates of spawner abundance for missing brood years. Each panel shows six alternative estimates of spawner abundance for a stock and brood year, comparing three SR model forms (Basic, AR1, TVP) and two alternative data sets (*Infill* = assign infilled values the same uncertainty and weight as observed values, *No Infill* = assign larger uncertainty and lower weight to infilled values). Top panels show two of four missing brood years for Kwinageese, bottom panels show both missing brood years for Lakelse.

```{r StateSpaceComp,  fig.cap="(ref:StateSpaceComp)" }
include_graphics("data/StateSpaceTest/StateSpace_EstimateComparison.png")
```



\clearpage
(ref:StateSpaceTab1) Kwinageese: Posterior median estimates of biological benchmarks for alternative model forms, estimate types, and data versions.

```{r StateSpaceTab1, echo = FALSE, results = "asis"}


table.in <- read.csv("data/StateSpaceTest/HamazakiAppOutputs_ReportTableSource.csv",stringsAsFactors = FALSE) %>% 
	dplyr::filter(Stock == "Kwinag") %>% select(-Stock)
							

table.in$ModelForm[duplicated(table.in$ModelForm)] <- ""

col.names.use = c("Model Form","Est Type","Data Version","Smsy","Smax","Seq")

table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
	 mutate_all(function(x){gsub("_", "\\\\_", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l",rep("r",3)),
                  caption = "(ref:StateSpaceTab1)", col.names = col.names.use ) %>%
     kableExtra::row_spec(c(4,7), hline_after = TRUE) %>%
    kableExtra::row_spec(c(2,5,8), extra_latex_after = "\\cmidrule(l){2-6}") 

```



(ref:StateSpaceTab2) Lakelse: Posterior median estimates of biological benchmarks for alternative model forms, estimate types, and data versions.  

```{r StateSpaceTab2, echo = FALSE, results = "asis"}


table.in <- read.csv("data/StateSpaceTest/HamazakiAppOutputs_ReportTableSource.csv",stringsAsFactors = FALSE) %>% 
	dplyr::filter(Stock == "Lakelse") %>% select(-Stock)
							

table.in$ModelForm[duplicated(table.in$ModelForm)] <- ""

col.names.use = c("Model Form","Est Type","Data Version","Smsy","Smax","Seq")

table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
	 mutate_all(function(x){gsub("_", "\\\\_", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l",rep("r",3)),
                  caption = "(ref:StateSpaceTab2)", col.names = col.names.use ) %>%
     kableExtra::row_spec(c(4,7), hline_after = TRUE) %>%
    kableExtra::row_spec(c(2,5,8), extra_latex_after = "\\cmidrule(l){2-6}") 

```






<!--chapter:end:06_06-appendix_AltSRTest.Rmd-->

\clearpage
## Test of Alternative Benchmark Calculation Approaches {#BMCalcTest}


### Purpose



This appendix summarizes results for the following tests: (1) alternative benchmark calculation approaches for a single set of $ln.a$ and $b$ parameters (e.g. Hilborn 1985 vs. Scheuerell 2016 Smsy calculations), (2) alternative benchmark calculation approaches across a grid of *ln.a* and *b* parameter values, (3) speed test for the alternative implementations.

Appendix \@ref(BiasCorrtest) summarizes tests related to the bias correction on $ln.a$.


### Alternative Smsy Calculations

We implemented four alternative Smsy calculation approaches (Table \@ref(tab:SmsyCalcs)) as part of the *RapidRicker* package [@RapidRicker], including the approximations from @Hilborn1985Proxies and @PetermanPyperGrout2000ParEst, the explicit solution from @Scheuerell2016, and a brute force calculation (i.e., for each parameter set $[ln.a,b]$ calculate recruits for 3,000 increments of spawner abundance, then select the increment with the largest difference between recruits and spawners)  . The R code for all four versions is included in Appendix \@ref(BMFunsSmsy).


(ref:SmsyCalcs) *Alternative Smsy Calculation Approaches*.

```{r SmsyCalcs, echo = FALSE, results = "asis"}


table.in <- smsy.eq


table.in$Calculation = linebreak(table.in$Calculation)
   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l"),
                  caption = "(ref:SmsyCalcs)") #%>%
     #kableExtra::column_spec(3, width = "10em") 


```

### Alternative Sgen Calculations


We implemented four alternative Sgen calculation approaches as part of the *RapidRicker* package [@RapidRicker], including three versions of  optimization code [@HoltOgden2013; @samSim; @Connorsetal2022 ] and a brute force calculation (i.e., for each parameter set $[ln.a,b]$ calculate recruits for 3,000 increments of spawner abundance, then select the increment with the smallest spawner abundance for which $Rec \geq Smsy$). The R code for all four versions is included in Appendix \@ref(BMFunsSgen).

Note that the @samSim version has been incorporated in the the [samSim package](https://github.com/Pacific-salmon-assess/samSim) and we label that option *samSim* in the *RapidRicker* functions. 



\clearpage
### Tests


* *Test 1: Sample Parameter Set*: Applied the alternative calculation approaches to a sample parameter set with $ln.a = 1.3$ and $b = 5e-4$. Sgen calculations are relative to Smsy values, so this gives 16 total variations: 4 alternative Smsy calculations, and then 4 alternative Sgen calculations for each Smsy value.
* *Test 2: Grid of ln.a and b parameters*: Applied the alternative calculation approaches to a set with all possible combinations of 100 $ln.a$ values from $ln(1.1)$ to $ln(10)$ and 100 $b$ values from 100 to 1 Mill capacity ($b = 1/Smax$, $b$ values from $1/100$ to $1/10^6$), resulting in 16 estimates for each of 10,000 alternative sets of $[ln.a, b]$.  
* *Test 3: Computing Speed*: Applied the 4 Smsy calculation methods and 4 Sgen calculation methods to 10,000 parameter sets. 


### Results


All the alternative calculation methods (4 for Smsy, 4 for Sgen) generated benchmarks values that are essentially identical for a sample parameter set (Table \@ref(tab:BMCalcTest1)).

For 10,000 alternative combinations of ln.a and b, Smsy values varied by a maximum of `r paste0(test2.vals[1],"%")` across 4 alternative calculation methods. Sgen values varied by a maximum of `r paste0(test2.vals[2],"%")` across 16 alternative calculation methods (4 alternative Smsy calculations by 4 alternative Sgen calculations).

Computing speed differed between calculation implementations, with brute force calculations much slower than the approximate Smsy calculations (Hilborn 1985, Peterman et al. 2000), the exact solution for Smsy (Scheuerell 2016), and the three alternative Sgen solver implementations (Table \@ref(tab:BMCalcTest3)). 


### Conclusions

Based on these results, we decided to use in this report:

* the @Scheuerell2016 method for Smsy, because it is the only exact solution
* the @Connorsetal2022 version of the Sgen optimizer, because it is the only non-brute-force method that did not crash for any of the $[ln.a, b, sd]$ combinations in the bias correction tests (Appendix \@ref(BiasCorrtest)).



\clearpage


(ref:BMCalcTest1) Benchmark Calculation Test 1. Estimates of biological benchmarks for $ln.a = 1.3$ and $b = 5e-4$ using 4 alternative Smsy calculations and 4 alternative Sgen calculations. Note that the @HoltOgden2013 version of the Sgen optimizer has a built-in Smsy calculation using the @Hilborn1985Proxies approximation, and therefore generates the same result for the four alternative Smsy inputs.

```{r BMCalcTest1, echo = FALSE, results = "asis"}


table.in <- test1.table #%>% select(-ln.a,-b)

   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","r","r","r","l","r","r"),
                  caption = "(ref:BMCalcTest1)") %>%
     kableExtra::row_spec(c(4,8,12), hline_after = TRUE) #%>%
     #kableExtra::column_spec(3, width = "10em") %>%
     #  kableExtra::row_spec(c(1:5,7:14,16:22), extra_latex_after = "\\cmidrule(l){2-8}") 

```



(ref:BMCalcTest3) Benchmark Calculation Test 3. Computing time for alternative benchmark calculation approaches over 10,000 sample values.

```{r BMCalcTest3, echo = FALSE, results = "asis"}


table.in <- test3.table %>%
	mutate(Time_s = round(Time_s,2)) %>% 
	dplyr::rename(Benchmark = BM, "Time(s)" = Time_s) %>%
	select(-n)

   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","r"),
                  caption = "(ref:BMCalcTest3)") #%>%
     #kableExtra::row_spec(c(6,15,23), hline_after = TRUE) %>%
     #kableExtra::column_spec(3, width = "10em") %>%
     #  kableExtra::row_spec(c(1:5,7:14,16:22), extra_latex_after = "\\cmidrule(l){2-8}") 

```








<!--chapter:end:06_06-appendix_BMCalcTests.Rmd-->

\clearpage
## EFFECT OF BIAS CORRECTION ON BENCHMARK ESTIMATES {#BiasCorrtest}

### Purpose

Preliminary benchmark estimates were flagged as potentially erroneous during the TWG process. Sgen values in particular seemed too low for several of the stocks. Once we verified the benchmark calculation code (Appendix \@ref(BMCalcTest)), we explored the effect of the log-normal bias correction for the productivity parameter alpha (Table \@ref(tab:BiasCorrCalcs)) on estimates of Smsy and Sgen. Note that this section refers to both the alpha parameter and the its natural log, ln.alpha, depending on the context.


### Approach


* Generated combinations of [alpha,sigma] that spanned the range of preliminary estimates for Skeena and Nass Sockeye stocks:
   * alpha parameters from 1.4 to 20 (ln.alpha from 0.336 to 3)
   * sigma parameters from 0.2 to 1.6
   * beta parameter does not affect the relative values, only the absolute scale, so fixed at  0.0005.
* Used @Scheuerell2016 method for Smsy, because it is the only exact solution. 
* Used @Connorsetal2022 method for Sgen, because it was the only non-brute-force method that did not crash for any of the [ln.alpha, b, s] combinations tested.
* Calculated Smsy and Sgen using either ln.alpha or ln.alpha' = ln.alpha + (sigma^2)/2
* Calculated the % differences due to bias correction for Smsy, Sgen, and the Ratio of Smsy/Sgen 
* Repeated the calculation with the simple deterministic parameter estimates (ln.alpha, beta, sigma) for those Skeena and Nass Sockeye stocks included in our analyses (i.e., wild stocks with at least 5 brood years of spawner-recruit data).



### Results

Larger sigmas resulted in small Smsy increases for stocks with higher intrinsic productivity (alpha >5, ln.alpha > 1.6), but resulted in substantial Smsy increases for lower productivity (alpha < 3, ln.alpha < 1.1). For example, Smsy roughly doubles (Perc diff = 100%) due to the bias correction for alpha = 1.5 and sigma = 1 (ln.alpha = 0.405, ln.alpha' = 0.905). Skeena and Nass Sockeye stocks fall on different gradients, with % difference due to bias correction ranging from ~5% to ~60% (Figure \@ref(fig:BiasCorrSmsyEffect)). Bias correction increased or decreased Sgen values, depending on the combination of ln.alpha and sigma (Figure \@ref(fig:BiasCorrSgenEffect)). Sgen decreases for all but one of the Skeena and Nass Sockeye stocks. For many stocks, Sgen decreased by more than 20%. The bias correction increased the distance between Smsy and Sgen as sigma increased (Figure \@ref(fig:BiasCorrRatioEffect)). For 3 stocks, the ratio of Smsy/Sgen more than doubled due to the bias correction.

Table \@ref(tab:TableBiasCorr) lists results by stock.


### Conclusions

Given these observed effects, we chose to report medians and percentiles without bias correction throughout this Research Document, but included the bias-corrected version in Appendix \@ref(BiasCorrectedBM). Section \@ref(BMMethods) describes how the bias correction is linked to how management objectives are defined.



\clearpage
(ref:BiasCorrSmsyEffect) Effect of Bias Correction on Smsy. Each line shows how, for a specific value of the alpha parameter, the difference between original and bias-corrected estimate changes as the sigma parameter increases. Uncertainty in the model fit increases from left to right, as sigma increases, resulting in a larger difference between estimates (i.e., lines curve upward). The effect of bias correction is larger at lower productivity (i.e., lower alpha parameter). Points show where each stock falls on the gradients of uncertainty and productivity, using a simple deterministic Ricker fit to all available data. The red horizontal line separates the results into the range where bias corrected estimates are larger than the original estimates (top) or lower than original estimates (bottom).

```{r BiasCorrSmsyEffect,  fig.cap="(ref:BiasCorrSmsyEffect)" }
include_graphics("data/BiasCorr/BiasCorr_Smsy_Effect.png")
```


\clearpage
(ref:BiasCorrSgenEffect) Effect of Bias Correction on Sgen. Layout as per Figure \@ref(fig:BiasCorrSmsyEffect).

```{r BiasCorrSgenEffect,  fig.cap="(ref:BiasCorrSgenEffect)" }
include_graphics("data/BiasCorr/BiasCorr_Sgen_Effect.png")
```


\clearpage
(ref:BiasCorrRatioEffect) Effect of Bias Correction on Ratio of Smsy/Sgen. Layout as per Figure \@ref(fig:BiasCorrSmsyEffect).

```{r BiasCorrRatioEffect,  fig.cap="(ref:BiasCorrRatioEffect)" }
include_graphics("data/BiasCorr/BiasCorr_Ratio_Effect.png")
```


\clearpage
(ref:TableBiasCorr) Effect of bias correction on estimates of Smsy, Sgen, and the ratio of Smsy/Sgen. Stocks sorted from lowest to highest productivity (ln.alpha). All results for a simple deterministic Ricker fit to all available data.

```{r TableBiasCorr, echo = FALSE, results = "asis"}
table.in <- read.csv("data/BiasCorr/BiasCorr_RickerDetFits.csv",stringsAsFactors = FALSE) %>%
          select(Stock,		ln.alpha,	sigma, Smsy.PercDiff,	Sgen.PercDiff,	Ratio.PercDiff) %>%
          mutate(ln.alpha = round(ln.alpha,2),	sigma = round(sigma,2), Smsy.PercDiff =round(Smsy.PercDiff),	Sgen.PercDiff=round(Sgen.PercDiff),	Ratio.PercDiff=round(Ratio.PercDiff)) %>% arrange(ln.alpha)

names(table.in) <- gsub(".PercDiff","",names(table.in) )

table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",5)),
                  caption = "(ref:TableBiasCorr)" ) %>%
   # kableExtra::row_spec(c(4,5,9,10), hline_after = TRUE)# %>%
	add_header_above(c(" " = 3, "% Difference" = 3)) 

```



<!--chapter:end:06_06-appendix_BiasCorrTests.Rmd-->

# SIMULATION MODEL EXTENSIONS  {#ModelExt}

Outcome uncertainty and covariation in productivity were identified as key revisions during the peer review meeting in April 2022, and then developed with feedback from the independent reviewers for the overall escapement goal review process (Sec. \@ref(EGProcess)). The specific implementations summarized in the appendices have not been formally peer-reviewed  through the CSAS process, but helped show the potential magnitude of effects on simulation results for the worked examples.  
 
## OUTCOME UNCERTAINTY {#OutcomeUncApp}

### Introduction


Within a simulation model, all the variables can be known exactly, and harvest strategies can be implemented perfectly. In practice, however, perfect control of the outcome is not possible. Target harvest and ER for the aggregate can differ from what the target should be, if run size were known perfectly. Actual harvest and ER will also differ from target ER due to factors such as physical and biological variables that affect the vulnerability of fish to fishing gear (e.g., river conditions, depth of fish in the water column, migration routes, migration timing), enroute mortality, and non-compliance with fishing regulations. Finally, ER for component stocks differ from the aggregate ER, depending on timing and area of fisheries relative to migration routes and timing.

Outcome uncertainty was not included in the simulation model described in initial version of this Research Document which was presented for peer review in April 2022, but was subsequently approximated in the current model implementation based on historical ER patterns. More complex mechanisms could be implemented in the future, which would bring this model closer to a full management strategy evaluation (MSE).

### Historical Patterns in Aggregate Exploitation Rate and Harvest

To investigate historical patterns, we generated time series of aggregate run size (catch plus number of spawners), spawner abundance, harvest, and ER by summing the estimates from the run reconstructions for component stocks.

The total amount harvested and the percent of the run that were harvested (ER) have declined for both aggregates since the mid-1990s (Figure \@ref(fig:OutUncPlotAggErHarv)). The time series can be split into three distinct periods:

* *Pre-1995*: For both aggregates, harvest amounts were highly variable, but ER was fairly stable in the range of 50%-70%.
* *1995-2009*: For the Skeena Wild aggregate, the mid-1990s are a clear breakpoint, with lower harvests and ER following (1) a large-scale fleet reduction in 1996 (the Mifflin Plan), (2) the introduction of gear restrictions to reduce interceptions of non-target species such as Coho and Steelhead (i.e., limiting fishing activity to daylight hours, mandatory weed lines and shorter length and set times for gillnets), and (3) the implementation of the 1999 PST Chapter 2 Annex, which introduced the Week 30/31 provisions for the District 104 purse seine fishery to reduce U.S. interceptions of Skeena Sockeye in July. For the Nass aggregate, ER stayed similar to the earlier time period until 2007, and harvest amounts were at or above the 1980s harvests.
* *2010+*:  For both aggregates, ER and harvest amount were much reduced compared to earlier years. Reasons for this change include (1) the implementation around 2009/2010 of the current Canadian domestic harvest rule for Skeena Sockeye following the work of the Independent Science Review Panel [@Waltersetal2008ISRP], (2) shifting of Canadian fishing effort later to avoid early-timed stocks, and (3) low returns for many years during the recent time period. 

By comparing annual ER and harvest to run size, we can approximate the overall harvest approach across all fisheries (Figure \@ref(fig:OutUncPlotAggFit), Figure \@ref(tab:OutUncTabAggFit)). This is the overall outcome at the end of the fishing season, which reflects environmental conditions, all the fishery-specific pre-season planning, in-season decision-making based on uncertain and rapidly changing information, and actual behaviour of fish and harvesters. Clear abundance-based patterns emerge for both aggregates:

* *SkeenaWild*: Aggregate ER tended to be lower for years with run size near or below the current assumed interim escapement goal of 500,000 (Figure \@ref(fig:OutUncPlotAggFit), Panel A). However, even at run sizes below the assumed current interim goal, the aggregate ER was highly variable, and as high as 60% for some early years with low run size. The aggregate harvest amount has declined with run size, and annual harvests cluster tightly around a fitted regression line for each time period (Figure \@ref(fig:OutUncPlotAggFit), Panel B). The slope for the most recent time period is shallower (i.e., amount of additional harvest for each incremental increase in run size is less in recent years than it was in earlier years).
* *Nass*: Aggregate ER tended to be lower for years with run size near the current assumed interim escapement goal of 200,000 (Figure \@ref(fig:OutUncPlotAggFit), Panel C), and aggregate run size in the reconstructions from 1982-2009 has never fallen below the interim goal. The aggregate harvest amount has declined with run size, and annual harvests cluster tightly around a fitted regression line for each time period (Figure \@ref(fig:OutUncPlotAggFit), Panel D). The slope for the recent time periods is shallower than in earlier years.

### Estimating Aggregate-level Outcome Uncertainty from Historical Patterns

Using the approach by @Collieetal2012RiskFW we can use the fitted regression lines in Panels B and D of Figure \@ref(fig:OutUncPlotAggFit) to estimate two properties of the historical harvest outcomes (Table \@ref(tab:OutUncTabAggFit)):

1.	*No Fishing Point*: Extrapolate the harvest amounts to the lower run sizes lower than any observed and identify the implied run size below which there would have been no harvest (i.e., point of no fishing or the lower management reference point, which is the x intercept of the fitted line). Note that these empirically derived estimates of the implied no-fishing point reflect the net outcome of all sources of variation in catch for a given run size, and hence are not the same as the limit reference points that managers may have had in mind at the time. 
2.	*Outcome Uncertainty*: Use the scatter of points around the fitted line to estimate the overall outcome uncertainty (i.e., assuming that the fitted line represents the actual strategy, how far off was the outcome in each year?). Statistically, this is estimated as the coefficient of variation (CV) based on the Root Mean Square Error (RMSE) scaled by Mean Harvest. A lower CV means that actual outcomes are closer to the estimated strategy (i.e., lower outcome uncertainty).

*Skeena Wild Aggregate*

Mean run size and harvest have declined over time, from a run size of over 1 million  and 650,000 harvested in the years before 1995, to 470,000 run size and 150,000 harvested for 2010-2019 (Figure \@ref(tab:OutUncTabAggFit)). The implied no fishing point is basically the same for all three time periods, at a run size of about 150,000. Outcome uncertainty was lower in earlier years (CV = 11%), then almost doubled in recent years (CV= 18%), but still much lower than the 30-50% CV for four Alaskan Chum stocks analyzed by @Collieetal2012RiskFW.

Based on the implied no-fishing point of 150,000 fish for Skeena Wild, we can infer a lower reference point that was used for the total Skeena aggregate in the past. Total Skeena Sockeye escapement (wild plus enhanced) has averaged about 3 times larger than the wild-stock escapement alone, ranging from 2 to 5 times larger. This roughly translates into an average historical lower reference point (i.e., no fishing point) of about 450,000 total Skeena run size, with a range from 300,000 to 750,000 total Skeena run size.

*Nass Aggregate*

Mean run size and harvest have declined in recent years, from more than 600,000 run size and more than 400,000 harvest in the two earlier time periods, to 350,000 run size and  170,000 harvest for 2010-2019 (Figure \@ref(tab:OutUncTabAggFit)). The implied no fishing point has roughly doubled over time, from about 59,000 before 1995 to about 116,000 since 2010. Outcome uncertainty was similar to Skeena Wild in earlier years (CV = 11%), then dropped (CV= 7-8%), again much lower than the 30-50% CV for four Alaskan Chum stocks analyzed by @Collieetal2012RiskFW.


*Magnitude of observed aggregate-level outcome uncertainty*

The outcome uncertainty described by the CVs for the linear fits in parts B and D of Figure \@ref(fig:OutUncPlotAggFit) appears small, but when translated into variation in ER across years for a given run size (Figure \@ref(fig:OutUncPlotAggFit), panels A and C), the result is a very large range in % ER. For instance, for the Skeena, a run size of roughly 0.5 million resulted in anywhere from a 25% to 50% ER in the 1995-2009 period and over 60% pre-1995. For low-productivity stocks, the high end of this ER range is potentially detrimental.


\clearpage
(ref:OutUncPlotAggErHarv) Time series of aggregate exploitation rate (ER) and harvest for two aggregates. Aggregate spawners, harvest, and run size were calculated as the sum of stock-specific run reconstructions. The time series are split into three time periods that roughly line up with major changes in the management approach. 

```{r OutUncPlotAggErHarv,  out.width= 440,  fig.cap="(ref:OutUncPlotAggErHarv)"}
include_graphics("data/OutcomeUncertainty/ERandHarvest_4Panels.png")
```







\clearpage
(ref:OutUncPlotAggFit) Annual aggregate exploitation rate and harvest as a function of run size. These plots summarize the overall outcome of annual stock-specific and fishery-specific management actions and physical/biological conditions, and can be used to approximate the underlying harvest strategy that was in place. In Panels A and C, various shapes of harvest control rule could be fitted to the observed data (e.g., a curvilinear function like Eqtn. 1 in @HoltPetermanOutcomeUnc, a hockey stick, or a step function with incremental increases in ER), but this would require either specifying or estimating various shape parameters for the functions (e.g., slopes, inflection points, breakpoints). In panels B and D, however, strong linear relationships between total harvest and total run size emerge (coefficient of determination $r^2$, adjusted for the number of observations and number of parameters is larger than 0.9 for all time periods for both aggregates; Table \@ref(tab:OutUncTabAggFit)). 

```{r OutUncPlotAggFit,  out.width= 440, fig.cap="(ref:OutUncPlotAggFit)"}
include_graphics("data/OutcomeUncertainty/OutcomeUncertainty_4Panels.png")
```





\clearpage
(ref:OutUncTabAggFit) Summary of estimated historical harvest strategy. For each aggregate and time period, table shows the mean run, mean harvest, estimated no fishing  point (i.e., x-intercept for linear regression fit in Figure \@ref(fig:OutUncPlotAggFit) ), estimated exploitation rate (i.e., slope of the fitted line), and associated adjusted $r^2$ and CV.


```{r OutUncTabAggFit, echo = FALSE, results = "asis"}


table.in <- read_csv("data/OutcomeUncertainty/Harvest_CV_Calcs_Report.csv") 

table.in$Aggregate[duplicated(table.in$Aggregate)] <- ""

table.in$Mean.Run <- prettyNum(round(table.in$Mean.Run), big.mark=",")
table.in$Mean.Harvest <- prettyNum(round(table.in$Mean.Harvest), big.mark=",")	
table.in$NoFishingPoint <- prettyNum(round(table.in$NoFishingPoint), big.mark=",")

table.in$TimeWindow <- recode(table.in$TimeWindow, "pre95" = "Up to 1994",
															"from95to2009" = "1995 to 2009",
															"since2010" = "Since 2010",
															"allyears" = "All years")


table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","r","r","r","r","r","r"),
                  caption = "(ref:OutUncTabAggFit)" ,
   					 col.names = c('Agg', 'Time\nWindow', 'Run', 'Harvest', 'No\nFishing','ER','Adj $r^2$','CV')
   					 )  %>%
	kableExtra::row_spec(2:dim(table.in)[1]-1, hline_after = TRUE) %>%
	kableExtra::column_spec(8, bold = TRUE,background = "lightgray") 


```


\clearpage
### Historical Differences between Aggregate-level and Stock-level ER

To investigate observed differences in annual ER between stocks, we generated various diagnostic plots of stock-level ER and aggregate-level ER, where aggregate-level ER was calculated from the sum of stock-level run reconstructions. Specifically, we examined the ratio of stock and aggregate-level ERs and their differences over time and relative to aggregate ER. Ratios of stock-level ER and aggregate ER have changed substantially for many stocks since the mid-1990s, consistent with the observed changes in aggregate ER and harvest highlighted above.

Figure \@ref(fig:StkERDiffMorice) shows one example for Morice Sockeye in the SkeenaWild aggregate. Figure \@ref(fig:StkERDiffComp)  summarizes the mean and spread of ratios across stocks, for two different time periods. Tables \@ref(tab:OutUncTabStkScalars95) and \@ref(tab:OutUncTabStkScalarsAll) list the corresponding values. Some notable observations:

* Nass stocks tend to return earlier than the bulk of the Skeena run.
* Nass stocks tend to have very similar ER, with a mean ratio near 1 and a narrower spread than observed for the Skeena stocks.
* The latest-timed Skeena stocks (i.e., Babine LW) generally have higher ER (due to Week 31 provision and later-timed Canadian fisheries), while the earlier-timed stocks generally have lower ER. This difference is more pronounced when looking at more recent data only (starting 1995) than for all years of data.
* *Lakelse* and *Mcdonell*: These are the earliest SkeenaWild stocks, and they have the lowest ER.
* *Babine* stocks: Mean ER is similar for the three component wild stocks, but the link between run timing and estimated ER is still clear. Babine Early Wild has the lowest mean ER, which almost matches the aggregate ER. Babine Mid Wild migrate later and have a slightly higher mean ER than the aggregate.  Babine Late Wild have the latest migration among the wild Skeena stocks, and have the highest mean ER (except for Sustut, see below).
* *Sustut*: Estimated exploitation rates for Sustut are a clear outlier among the SkeenaWild stocks. While escapement data for the Sustut stock comes from a weir count and is considered to be reliable, there is a terminal FSC fishery just downstream of the weir facility with an average reported harvest of 682 (min = 135, max = 1,954) since 1994, when the current fishery started (road access to the site was only established in the early 1990s). This terminal harvest, which is additional to the harvests in mixed stock fisheries in the mainstem Skeena and marine fisheries that affect all other Skeena stocks, may explain the higher and more variable ERs observed for this stock.


\clearpage
(ref:StkERDiffMorice) Example of ER diagnostics – Morice Sockeye (Middle Skeena Lake Type). Plot shows ratios and differences, both over time and relative to aggregate ER. For many stocks, these patterns show a break point in the mid-1990s, so data are split into earlier years up through 1994, and more recent years starting in 1995. Before 1995, Morice ER and aggregate SkeenaWild ER are very similar (ratio around 1, differences around 0), but have increasingly diverged in recent years. ER values in the panels on the right are in %. For example, if aggregate ER was 45% and Morice ER was 32%, then the ratio was 0.71 and the difference was -13.

```{r StkERDiffMorice,  out.width= 415, fig.cap="(ref:StkERDiffMorice)"}
include_graphics("data/OutcomeUncertainty/ER_DiagnosticPlots_Morice_3_AdjustmentDiagnostics.png")
```



\clearpage
(ref:StkERDiffComp) Stock-specific scalars for exploitation rate (ER) estimated for two alternative time periods. Estimates are based on the observed ratio of stock-specific ER and aggregate ER. Points and whiskers show the mean ± 2 SD. Stocks are grouped by aggregate, and sorted based on spawning location within each aggregate, from the mouth of the river to upstream locations. Stocks are also assigned to one of five timing groups, from 1 = earliest to 5 = latest. Peak timing and run duration of stocks relative to each other vary by year and differ by area (e.g., Alaskan fisheries, Canadian marine fisheries, in-river fisheries). Timing assignments are rough groupings based on long-term average peak migration through lower river assessment projects (Tyee test fishery for the Skeena, and Nass fish wheels). Tables \@ref(tab:OutUncTabStkScalars95) and \@ref(tab:OutUncTabStkScalarsAll) list the corresponding values.

```{r StkERDiffComp,  out.width= 400, fig.cap="(ref:StkERDiffComp)"}
include_graphics("data/OutcomeUncertainty/ER_Scalars_byStock.png")
```




\clearpage
(ref:OutUncTabStkScalars95) Distribution parameters for stock-level ER scalars based on observed differences to aggregate ER using data since 1995. *n* is the number of years with stock-specific ER estimates from the run reconstruction. Mean values larger than 1.1 or smaller than 0.9 are highlighted and marked with an asterisk (i.e., stocks where the mean ER differs by more than 10% from the aggregate ER). Note that these scalars are relative to the aggregate ER, so that a scalar of 1.1 (a 10% difference) means an Agg ER of 30% becomes a stock-level ER of 33%, not a stock-level ER of 40%. Stocks are grouped by life history and adaptive zone (LHAZ).

```{r OutUncTabStkScalars95, echo = FALSE, results = "asis"}


table.in <- read_csv("data/OutcomeUncertainty/Generated_ER_Scalars_ByStock.csv") %>%
							dplyr::filter(Version == "Starting 1995") %>%
							select(MU, LHAZ,StkNmS,n,mean, sd,p10,p25,p50,p75,p90) %>%
							mutate(across(c(mean, sd,p10,p25,p50,p75,p90),~format(round(.x, 2), nsmall = 2)))
							

table.in$MU[duplicated(table.in$MU)] <- ""
table.in$LHAZ[duplicated(table.in$LHAZ)] <- ""

cols.mean <- rep("white",dim(table.in)[1])
cols.mean[table.in$mean < 0.9] <- "cyan"
cols.mean[table.in$mean > 1.1] <- "orange"


table.in$mean[table.in$mean < 0.9] <- paste0("*",table.in$mean[table.in$mean < 0.9])
table.in$mean[table.in$mean > 1.1] <- paste0("*",table.in$mean[table.in$mean > 1.1])



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","r","r","r","r","r","r"),
                  caption = "(ref:OutUncTabStkScalars95)" ,
   					 col.names = c('Agg', 'LHAZ','Stock', 'n', 'mean', 'sd','p10','p25','p50','p75','p90')
   					 )  %>%
  kableExtra::row_spec(c(4), hline_after = TRUE) %>%
  kableExtra::row_spec(c(1,9,15), extra_latex_after = "\\cmidrule(l){2-11}") %>%
  kableExtra::row_spec(c(2:3,5:8,10:14, 16:19), extra_latex_after = "\\cmidrule(l){3-11}") %>%
  kableExtra::column_spec(5, background =  cols.mean)




```


\clearpage
(ref:OutUncTabStkScalarsAll) Distribution parameters for stock-level ER scalars based on observed differences to aggregate ER using all available data. *n* is the number of years with stock-specific ER estimates from the run reconstruction. Mean values larger than 1.1 or smaller than 0.9 are highlighted (i.e., stocks where the mean ER differs by more than 10% from the aggregate ER). Note that these scalars are relative to the aggregate ER, so that a scalar of 1.1 (a 10% difference) means an Agg ER of 30% becomes a stock-level ER of 33%, not a stock-level ER of 40%.

```{r OutUncTabStkScalarsAll, echo = FALSE, results = "asis"}


table.in <- read_csv("data/OutcomeUncertainty/Generated_ER_Scalars_ByStock.csv") %>%
							dplyr::filter(Version == "All Years") %>%
							select(MU, LHAZ, StkNmS,n,mean, sd,p10,p25,p50,p75,p90) %>%
							mutate(across(c(mean, sd,p10,p25,p50,p75,p90),~format(round(.x, 2), nsmall = 2)))
							

table.in$MU[duplicated(table.in$MU)] <- ""
table.in$LHAZ[duplicated(table.in$LHAZ)] <- ""

cols.mean <- rep("white",dim(table.in)[1])
cols.mean[table.in$mean < 0.9] <- "cyan"
cols.mean[table.in$mean > 1.1] <- "orange"

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","r","r","r","r","r","r"),
                  caption = "(ref:OutUncTabStkScalarsAll)" ,
   					 col.names = c('Agg', 'LHAZ', 'Stock', 'n', 'mean', 'sd','p10','p25','p50','p75','p90')
   					 )  %>%
  kableExtra::row_spec(c(4), hline_after = TRUE) %>%
  kableExtra::row_spec(c(1,9,15), extra_latex_after = "\\cmidrule(l){2-11}") %>%
  kableExtra::row_spec(c(2:3,5:8,10:14, 16:19), extra_latex_after = "\\cmidrule(l){3-11}") %>%
  kableExtra::column_spec(5, background =  cols.mean)




```


\clearpage

### Model Implementation of Aggregate and Stock-level ER Scalars

Given the observed patterns summarized above, we decided to simulate outcome uncertainty in the current model as 2 multiplicative scalars, rather than additive variation. Specifically, for Stock *i* in aggregate *j*, for year *k* in simulated trajectory *l*:


\begin{equation} 
	Stk.ER_{i,j,k,l} = Target.ER_{j,k,l} * Agg.Scalar_{j,k,l} * Stk.Scalar_{i,j,k,l}
\end{equation} 


For example:

* If the ER Target for the Skeena Wild aggregate is 10% and the randomly sampled aggregate scalar for Skeena Wild is  0.94, then the actual ER for the Skeena Wild aggregate is 9.4%.
* If the randomly sampled scalar for Alastair is 0.51, then the actual ER for Alastair is 4.8% (10 * 0.94 * 0.51). 

This approach for the aggregate scalar is analogous to the approach by @HoltPetermanOutcomeUnc, who estimated aggregate-level multiplicative scalars for each component of an abundance-based harvest rule that had three inputs (maximum ER, Run size below which the ER is 0, and a shape parameter).

The second step of also applying a stock-specific scalar captures two important properties. Simulated outcomes in terms ER will differ between stocks, but they will be correlated with each other, and with the aggregate (i.e., there is random variation around each ER value, but for a simulated year with larger target ER for the aggregate, all the component stocks will also tend to have larger ER).

The parameterization of these distributions of scalars is critical. To be useful, the modeling approach needs to approximately reflect the mean magnitude of the scalar, as well as variation around that mean. Even if the specifics are wrong, but the overall properties are right, the model will give useful guidance. 
 We created the following alternative scenarios for sensitivity testing:
 
*	*Aggregate Scalars*:  Three variations that cover the observed range (Table \@ref(tab:OutUncTabAggFit)). *None* =  no difference between aggregate target ER and aggregate ER outcome; *Narrow* = normal distribution with CV= 5%; *Wide* = normal distribution with CV= 15%. 
* *Stock-level Scalars*: Three variations. None = no difference between aggregate ER and stock-level ER; All year and Starting 1995 = use the sample distributions (Figure \@ref(fig:StkERDiffComp), Tables \@ref(tab:OutUncTabStkScalars95) and \@ref(tab:OutUncTabStkScalarsAll)).

Together this gives 3 x 3 = 9 alternative scenarios of outcome uncertainty to be tested against alternative productivity assumptions, alternative harvest strategies, and alternative assumptions about covariation in productivity.

Three fundamental questions need to be considered:

1.	*How does the model specify the target ER for the aggregate?*  The aggregate target ER in the simulation will depend on the user-specified type and specific values for the harvest rule. The current priority is to test alternative levels of a fixed escapement strategy. We are also testing alternative levels of a fixed ER strategy to show the contrast in expected performance, and provide support for the recommendation to explore various types of abundance-based rules in the future. 
2.	*How can we capture additional properties of the aggregate scalars?* The Skeena data (Figure \@ref(fig:OutUncPlotAggFit), Panel A) show not only variation around some target nonlinear ER function, but also a bias upward in the harvest rate at low run size compared to the optimal nonlinear function that is associated with an interim escapement goal of 300,000. That bias is important to capture in order to fully reflect the conservation consequences of outcome uncertainty. However, it cannot be easily implemented and tested in the current model structure. We consider this extra level of complexity a high priority for future work, but beyond the scope of the current worked example of the simulation model.
3.	*How can we capture additional properties of the stock-specific scalars?* Outcome uncertainty is likely correlated between stocks (e.g., ER for all the early migrating stocks in a simulated year will tend to differ from the aggregate ER in the same direction, because they pass through the same gauntlet of fisheries at the same time). This could be implemented in the current model structure, similar to the covariation in productivity, which is the second major model extension in response to the science review. However, it would take considerable effort to replicate the productivity covariation analyses with the ER differences to generate the parameters for this. We consider this extra level of complexity a high priority for future work, but beyond the scope of the current worked example of the simulation model.




<!--chapter:end:06_10_ModelExt1_OutcomeUncertainty.Rmd-->

\clearpage
## MODELLING COVARIATION IN PRODUCTIVITY {#CovarProdApp}



### Concepts

The current simulation model simulates 20 stocks in two aggregates: Nass aggregate (4 stocks), SkeenaWild aggregate (16 modelled stocks). Simulated recruits are based on spawner numbers for the brood year, the fitted relationship between spawners and productivity (i.e., recruits/spawner), with randomly sampled noise to reflect natural variability and uncertainty. The initial model results presented in the CSAS science review process assumed a stock-specific amount of variability around the underlying spawner-recruit relationship, but the randomly sampled variability for an individual stock was independent of the variability in other stocks (e.g., in a simulated year, Babine Late Wild could have worse-than-expected recruitment and Babine Early wild could have better-than-expected recruitment). However, the spawner recruit data suggest that covariation in recruitment productivity occurs for some nearby stocks (e.g., positive covariation would mean that in a year with a good productivity for one stock, other stocks would also tend to have good productivity).  Covariation in salmon productivity has been documented at different scales, from stocks in an aggregate to coastwide patterns by species [e.g., @CkCov2017].

Depending on the type of harvest strategy, the level of covariation can strongly influence the aggregate and individual trajectories of run size, harvest, and spawner abundance. Participants in the science review therefore identified covariation in productivity as a high-priority extension of the simulation model. 



### Estimating Historical Covariation in Productivity 

To estimate historical covariation in productivity, we estimated the log residuals from the basic Ricker model fit (i.e., the one without a time-varying productivity parameter), and then estimated the correlation between each pair of productivity time series for modelled stocks. We  then averaged the correlations for groups of stocks (Figure \@ref(fig:AltCorrMat)). Stocks were grouped based on life history and freshwater adaptive zone (LHAZ).  There are two LHAZ with modelled stocks on the Nass (Lower Nass Sea & River Type, Upper Nass Lake Type) and three LHAZ with modelled stocks on the Skeena (Lower, Middle, and Upper Skeena Lake Type). 

Notable observations included:

* *longer time period*:  Positive correlations were observed within and between stocks in the Skeena LHAZ, but the correlation is weaker between Middle Skeena stocks and others. Specifically, correlations are larger than 0.4 within all three Skeena LHAZ, and between lower and upper Skeena stocks. Correlations between middle Skeena stocks and the other Skeena stocks are lower, around 0.175. We observed negative correlations between Lower Nass SRT and all other Skeena and Nass stocks (strongest negative for correlation with Upper Skeena).
* *shorter time period*: We observed stronger correlations than for the longer time period within 2 of the 3 Skeena LHAZ stocks, similar correlations between lower and upper Skeena stocks, and no correlation between middle Skeena LHAZ and other Skeena stocks. There is a stronger negative correlation between Nass Lake Type and Skeena Lake Type stocks.
* Very weak correlation was observed within Upper Nass Lake Type for either time period, so set to 0 in both time periods.
* No correlation was calculated within Lower Nass SRT, because it consists of only a single stock.

\clearpage
(ref:AltCorrMat)  Observed correlation in productivity within and between groups of Skeena and Nass Sockeye stocks. Estimates are based on residuals, ln(recruits/spawner), from Ricker fits for the long-term average productivity scenario (i.e., no time-varying productivity parameter). Missing brood years for some stocks were either left as NA or infilled based on mean residual for other stocks with the same life history and in the same adaptive zone (LHAZ). Note that diagonal cells with bold font are the correlations among stocks within the LHAZ, not the correlation of the LHAZ with itself, which would be 1. Estimates only cover 20 modelled wild stocks. Numbers in brackets show the number of stocks in each LHAZ.

```{r AltCorrMat,  out.width= 400, fig.cap="(ref:AltCorrMat)"}
include_graphics("data/CovarProd/AltCorrMat_Plot.PNG")
```



\clearpage
### Model Implementation of Covariation in Productivity


We incorporated covariation in productivity into the simulaton model by generating correlated time series of standardized residuals, which are then scaled up based on each stock’s observed magnitude of variability. 

We created four alternative covariation scenarios for sensitivity testing:

-	*No covariation*: productivity for each stock is independent of the other stocks.
-	*Simplified correlations – 1984 to 2013 Brood Years*: Using the values from Panel A of Figure 1 for each stock in a group.
-	*Simplified correlations – 1999 to 2013 Brood Years*: Using the values from Panel B of Figure 1 for each stock in a group.
-	*Detailed pairwise correlations – 1984 to 2013 Brood Years*: Using the observed correlations between individual stocks (i.e., the numbers that were averaged to generate Figure \@ref(fig:AltCorrMat)). For example, in this version the productivity correlation between Babine Late Wild and Johnston is a bit less than the correlation between Babine Mid Wild and Johnston. In the simplified versions above, these two correlations are the same.


<!--chapter:end:06_11_ModelExt2_CovarInProd.Rmd-->

# BIAS-CORRECTED BENCHMARK ESTIMATES {#BiasCorrectedBM}


## Nass Summary Tables - Bias Corrected

### Nass Smsy

(ref:SmsyLtAvgNassBC) Comparison of bias-corrected aggregate and stock-level Smsy estimates: Nass / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyLtAvgNassBC, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Smsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyLtAvgNassBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SmsyRecentNassBC) Comparison of bias-corrected aggregate and stock-level Smsy estimates: Nass / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyRecentNassBC, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Smsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyRecentNassBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
### Nass Sgen


(ref:SgenLtAvgNassBC) Comparison of bias-corrected aggregate and stock-level Sgen estimates: Nass / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SgenLtAvgNassBC, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Sgen.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenLtAvgNassBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SgenRecentNassBC) Comparison of bias-corrected aggregate and stock-level Sgen estimates: Nass / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SgenRecentNassBC, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Sgen.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenRecentNassBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage

### Nass Umsy

(ref:UmsyLtAvgNassBC) Comparison of bias-corrected aggregate and stock-level Umsy estimates: Nass / Long-term average productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyLtAvgNassBC, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Umsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyLtAvgNassBC)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```



(ref:UmsyRecentNassBC) Comparison of bias-corrected aggregate and stock-level Umsy estimates: Nass / Recent productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyRecentNassBC, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Umsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyRecentNassBC)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```





\clearpage
## Skeena Wild Summary Tables - Bias Corrected

### Skeena Wild Smsy

(ref:SmsyLtAvgSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Smsy estimates: Skeena Wild / Long-term average productivity. Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyLtAvgSkeenaWildBC, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Smsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyLtAvgSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
(ref:SmsyRecentSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Smsy estimates: Skeena Wild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyRecentSkeenaWildBC, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Smsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyRecentSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
### Skeena Wild Sgen


(ref:SgenLtAvgSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Sgen estimates: Skeena Wild / Long-term average productivity. Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SgenLtAvgSkeenaWildBC, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Sgen.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenLtAvgSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
(ref:SgenRecentSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Sgen estimates: Skeena Wild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added.  

```{r SgenRecentSkeenaWildBC, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Sgen.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenRecentSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```




\clearpage

### Skeena Wild Umsy

(ref:UmsyLtAvgSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Umsy estimates: Skeena Wild / Long-term average productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyLtAvgSkeenaWildBC, echo = FALSE, results = "asis"}

umsy.bm.tab.src <- read_csv("data/SummaryTables_UMSY.csv")

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Umsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyLtAvgSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```


\clearpage
(ref:UmsyRecentSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Umsy estimates: Skeena Wild / Recent productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyRecentSkeenaWildBC, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Umsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyRecentSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```



<!--chapter:end:06_07-appendix_BiasCorrectedBenchmarks.Rmd-->

# REVIEW OF WILD AND BLDP-ENHANCED BABINE SOCKEYE PRODUCTION {#ChannelReview}

## Context

Considerations for developing management reference points for wild and enhanced Skeena Sockeye include potential interactions between the enhanced and wild Babine stocks, which have distinctive run timing and geographic separation between spawning areas. We reviewed production data for wild and enhanced Babine Sockeye to assess general trends in adult returns, escapement quality (size, sex ratio and fecundity), egg production, and fry and smolt outputs. This was not intended to be a comprehensive assessment of Babine Sockeye production, or a detailed analysis of the effects of the BLDP enhancement program on wild Babine and other Skeena Sockeye stocks. Rather, we provide a high-level overview of observed trends in freshwater production based on available information. An integrated review of BLDP production and updated recommendations for loading targets and operational procedures is a major undertaking that will require input and advice from the facility operator (Fisheries and Oceans Canada - Salmonid Enhancement Program) and is outside the scope of the current review of Skeena and Nass Sockeye escapement goals.



## Babine Sockeye Stocks

Babine Lake is the largest natural freshwater lake in British Columbia, encompassing an area of nearly 500 km^2^ which drains a watershed of approximately 10,000 km^2^. Morrison Lake and Tahlo Lake, which drain through Morrison River into Morrison Arm upstream of Babine Lake. The North Arm, upstream of Harrison Narrows on the northwest side of Babine Lake, flows through a short section of the Upper Babine River into Nilkitkwa Lake, then into the Lower Babine River, a 5th order tributary of the middle Skeena. 

Babine Sockeye have been counted at the Babine weir downstream of Nilkitkwa Lake annually since 1949. The Babine weir which is currently operated by Lake Babine Nation, under contract to Fisheries and Oceans Canada, provides daily counts for all salmon species from the middle of July until the end of September and encompasses most of the Sockeye return. The weir operation has been extended to the end of November in some years. The weir program is assumed to provide a complete count for most years, but adjusted in some years for estimated passage during times when the fence was not operational.

Sockeye salmon escapements to Babine Lake have ranged from 71,000 to 2.1 million past the Babine weir. Very low returns were observed after a catastrophic landslide in Babine River in 1951 that restricted fish passage in 1951 and 1952, and until repairs were completed in 1953 [@Godfry1954BabineSlideEffects]. The lowest Sockeye return of just over 71,000 was recorded in 1955 following the 1950 brood year of 141,000 (Figure \@ref(fig:BabineCounts)).  

Wild Babine Sockeye are assigned to three groups based on adult run timing: an early timed group which primarily spawn in tributaries that drain into the main basin of Babine Lake; a mid-timed group which spawn in Morrison Creek, Morrison Lake, and Tahlo Creek, and a late-timed group of Babine Sockeye includes Sockeye that spawn in sections of the Upper Babine River between Babine Lake and Nilkitkwa Lake, and downstream of Nilkitkwa Lake. The progeny of early and mid-timed wild Babine spawners rear in the main basin of Babine Lake with the exception of Sockeye returning to spawn in Morrison River, Morrison Lake and Tahlo Creek, which rear in Morrison Lake [@WoodLifeHist1995]. The late-timed group exhibits an upstream migration pattern for fry which migrate upstream following emergence and rear in Nilkitkwa Lake and the North Arm of Babine Lake. 

Visual escapement estimates of up to 30 wild Babine Sockeye spawning tributaries are conducted annually by foot or aerial surveys led by DFO and Lake Babine Nation. Estimates from visual escapement surveys for wild Babine systems are adjusted to account for underestimation bias using methods described in @WoodLifeHist1995. Annual stream counts for individual Babine systems are maintained in the Fisheries and Oceans Canada [NUSEDS database](https://open.canada.ca/data/en/dataset/c48669a3-045b-400d-b730-48aafe8c5ee6).  The raw spawner estimates for the different wild Babine systems are expanded and combined into adjusted estimates for the early, mid and wild run timing components using a run-reconstruction procedure described by @WoodLifeHist1995.

Fulton River and Pinkut Creek were, along with Babine River, the most abundant Babine Sockeye stocks and largest contributor to Babine Lake Sockeye before the start of the  BLDP. In the post-BLDP period, Sockeye salmon returns to the enhanced systems increased following while returns wild Babine systems have declined. The pattern of declines has varied between stocks over time. Early and late-timed Babine wild stocks have seen steady declines in spawner abundances. The numbers of recruits produced per spawner (recruits per spawner) since the late 1990s, while mid-timed wild stocks appear to have recovered from low returns in the late 1990s but have been in a state of decline since the mid-2000s.  

The asynchrounous population dynamics between wild and enhanced Babine Sockeye, and among the different wild stocks, suggest that straying of enhanced surplus spawners into wild systems is not likely given that there have been large surpluses and low observed spawner escapement to wild Babine tributaries have been observed in some years, and the reverse in others (Figures \@ref(fig:BabineProdRpS) and \@ref(fig:BabineProdResid)).

(ref:BabineCounts) Babine weir counts 1950 – 2021. Figure shows estimated wild (light grey) and enhanced (dark grey) components of the run.

```{r BabineCounts,  fig.cap="(ref:BabineCounts)" }
include_graphics("data/ChannelReview/BabineWeirCounts.png")
```


\clearpage
(ref:BabineProdRpS) Observed productivity of Babine Sockeye stocks. Panels show productivity in terms of recruits/spawner (R/S), log-transformed to adjust for the commonly observed skewed distribution and smoothed as a 4-yr running average to highlight the underlying pattern. Spawners exclude the channel surplus. Red horizontal lines mark the corresponding raw numbers that can be more directly interpreted: At 1 R/S (*Repl*), the stock replaces itself *in the absence of any harvest*. At  2 R/S, the stock could sustain  50% exploitation rate while maintaining the same spawner abundance (under theoretical stable long-term conditions, i.e., *equilibrium*). For each stock, the largest observed productivity, Max(R/S), and the stock's contribution to the total Skeena spawner abundance since 2000 (%Spn) are listed. Figure \@ref(fig:BabineProdResid) shows changes in productivity after accounting for density dependence.

```{r BabineProdRpS,  fig.cap="(ref:BabineProdRpS)" }
include_graphics("data/ChannelReview/ProdPatterns_BabineStocks_LogRpS.png")
```



(ref:BabineProdResid) Productivity residuals for Babine Sockeye stocks. Panels show productivity patterns as deviations from the expected log(R/S) based on a simple deterministic Ricker fit, smoothed as 4-yr running average to highlight the underlying pattern. The Ricker residuals residuals, in units of ln(R/S), account for within-stock density effects, so that the pattern is a better reflection of fundamental, underlying productivity changes as spawner abundance naturally varies from year to year. With these residuals, the pattern can be directly interpreted, but the specific values are not as biologically meaningful as the observed productivity series in Figure \@ref(fig:BabineProdRpS).

```{r BabineProdResid,  fig.cap="(ref:BabineProdResid)" }
include_graphics("data/ChannelReview/ProdPatterns_BabineStocks_RickerResids.png")
```





\clearpage
## Babine Lake Development Project 

### History

The BLDP spawning channels, adult control weirs, and flow control structures were built in stages starting with construction of Fulton Channel 1 in 1965, and the Fulton weir and Pinkut flow-control structures in 1966. Pinkut Channel and weir, and the Fulton flow-control structures were installed in 1968, followed by Fulton Channel #2, which was completed in two phases, in 1969 and  1971. For the first two years of operation, only the top half of Fulton Channel #2, representing 55% of its eventual capacity, was loaded. Pinkut Channel, which was initially built in 1968, experienced high egg mortality in the first two years of operation as a result of anchor ice formation in the channel bed. In 1970, an auxillary water system was installed to supply warm lake water to the channel. In subsequent years, spawning habitat quality in Pinkut Channel was affected by heavy siltation caused by erosion of the unarmoured banks, and the channel was entirely rebuilt in 1976-77 [@West1987]. Starting in 1973, an airlift operation was used to transport spawners to an inaccessible section of the creek above Pinkut Falls in some years of high returns.

The BLDP spawning channels increased available spawning habitat by 116,000 m^2^ to accommodate approximately 190,000 additional spawners, and flow control provides stable spawning and incubation habitat in Pinkut Creek and Fulton River (West 1987, Table 1). Sockeye returning to Pinkut Creek and Fulton River also spawn in natural stream sections downstream of the respective weirs, which have an estimated capacity for 5,000 and 45,000 effective spawners. The current combined spawning capacity for Pinkut and Fulton Sockeye, including spawning channels, flow-controlled river sections, inaccessible spawning habitat serviced by the Pinkut Airlift program, (which has not operated since 2007), and downstream areas, is 509,000 spawners. The area of available spawning habitat, year of implementation, and current loading targets for BLDP enhanced channel, river and creek sections are provided in Table
\@ref(tab:ChannelTargets).


(ref:ChannelTargets) Area, loading capacity and date of construction for BLDP components. The original target density of one female per 1.25 m^2^ was increased  in the early 1990s for components marked by **\***.

```{r ChannelTargets, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ChannelReview/ChannelTargets.csv") %>% mutate(Target = prettyNum(Target,big.mark = ",",scientific = FALSE)) %>% 			mutate_all(as.character)

table.in[is.na(table.in)] <- "-"

col.names.use <- c("Component",
									 "Area\n(1000$m^2$)",
									 "1st year\noperated",
									 "Target\nspawners\n1000")


table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l","r","r","r"),
                  caption = "(ref:ChannelTargets)" ,col.names = linebreak(col.names.use ,
                  																												align = "c")) %>%
    kableExtra::row_spec(c(6,7,12,13), hline_after = TRUE)# %>%
	#add_header_above(c(" " = 2, "Scenarios" = 4)) 

```


### Channel Loading

Sockeye escapements, or loading, of spawning channels and Pinkut Creek and Fulton Rivers are managed to maintain target densities of spawners to maximize fry production and reduce risks of over-escapement including redd superimposition following wave spawning events, and disease outbreaks. The channels are loaded by female counts, and the target loading density of about 1.25 females/m^2^ of available spawning gravel is designed to achieve an optimal egg density of 2000-2500 eggs/m^2^. Loading targets for Fulton River were adjusted upwards in the 2000s to mitigate for pre-spawn mortality (PSM) caused by parasites and warmer temperatures. The actual number of eggs deposited in a given year depends on a number of factors including fecundity, egg retention, PSM and the ability to reload spawners, if available, in the event of high PSM. Current loading targets are provided in Table \@ref(tab:ChannelTargets).

Loading events for the BLDP  spawning channels ideally occur in a single event for each channel to avoid wave spawning. Sockeye spawners are enumerated as they pass through weirs located near the mouths of Pinkut Creek and Fulton Rivers. Once the spawning channels and river sections upstream of the weir have reached capacity, any Sockeye remaining holding below the fence are locked out. Visual estimates of the number of Sockeye holding below the BLDP facilities are conducted regularly during the spawning season in most years. If significant pre-spawn mortality is detected in the spawning channels, they may be reloaded with the Sockeye that remain holding below the fences.

The loading target for  Fulton River upstream of the  counting fence was increased from 100,000 to 200,000 in the early 2000s to offset potential PSM related to parasite infection, and because it was thought that additional fry could be produced, albeit at a lower egg-to-fry survival rate, from the lower quality, more marginal spawning habitats that are not included in the estimated area of  good quality spawning habitat for Fulton River above the counting fence (67,000 m^2^).

A disease outbreak (the parasite *Ichthyophthirius multifilis*) caused high pre-spawn mortality in the Fulton and Pinkut spawning channels in 1994 and 1995 [@Traxler1998BabineDisease] and resulted in low spawner escapements to the enhanced facilities in subsequent return years starting in 1998.  Although escapements for both systems rebounded somewhat in the early 2000s, it has remained lower than the pre-outbreak period), with further declines observed for Pinkut Sockeye since 2010.

Loading targets for the Pinkut and Fulton spawning channels and managed river sections have been maintained at full capacity  in most years except for years of exceptionally poor returns, including 1998 and 1999 (following the disease outbreak that affected the 1994 and 1995 brood years), and more recently in 2013 and 2019 when spawning targets were not attained for Fulton Channel #2, and for Pinkut Channel in 2013. Fulton River did not attain its loading targets in 1969, 1991 and 2013.  


## Available biological information for wild and enhanced Babine Sockeye



### Age Sampling

Babine Sockeye primarily rear for 1 year in freshwater following emergence as fry, migrate to sea in their second year of life and return to spawn after 1-3 years at sea, for a total ages ranging from 3 (“jack” Sockeye which spend 1 winter at sea) or 4-5 (“adult” or “large” Sockeye, which spend 2-3 winters at sea). Age sampling has not been regularly conducted at the Babine weir or BLDP facilities since the mid-1990s. Age composition estimates for Babine Sockeye are derived from the aggregate Skeena Sockeye return, which is sampled at the Tyee Test Fishery, of which Babine Sockeye typically account for about 90% of the total return. 

Age-3 Sockeye, which are not effectively sampled at the Tyee Test Fishery are counted at the Babine weir. The estimated returns of “large”, or 4- and 5-year old Sockeye arriving at the Babine fence, which are based on the sampled proportions of Sockeye with a single freshwater year at the Tyee Test Fishery (age 4~2~ and 5~2~ in Gilbert-Rich notation), are added to the counts of age-3 Sockeye from the Babine fence to calculate the proportions for all age classes. 

The proportions of age 3-, 4-, and 5-year old Skeena and thus Babine Sockeye varies across years. Since 1970, the annual proportion of age-3 Sockeye returns to Babine Lake has ranged from 0 – 40%. The proportions of age 4 and age 5 age classes have both ranged between 3 – 92%. Exceptionally low returns of one age class can signal a brood year failure related to poor marine survival for the siblings of a cohort that went to sea in a common year. For example, a poor return of age 3 Sockeye may signal a poor return of age 4 the next year, followed by age 5 in the following year. Because the dominant age class (4~2~ or 5~2~) of spawning females varies by year, there is no clear trend in declining total age at return for Babine Sockeye.


### Body Size

Length-at-age and overall body length have decreased over time for Skeena Sockeye, which are sampled at the Tyee Test Fishery. For Sockeye sampled at the Tyee Test Fishery, length at age decreased by 2-3% for 5-, 6- and 7-year old fish and remained constant for 4 year old fish between the 1980s and 2010s (decadal averages). The pattern of observed changes in overall length and length-at-age for Skeena and Nass Sockeye, which are consistent with decreases observed for Sockeye salmon populations in Southeast Alaska [e.g., @Oke2020RecentDeclinesBodySize], are not linear, with less pronounced declines in older age classes and steeper declines observed since 2010.  

The magnitude of the observed declines in body length for Skeena and Nass Sockeye are consistent with those observed for other Sockeye populations in the North Pacific, and are related to decreases in fecundity [@Oke2020RecentDeclinesBodySize; @Ohlbergeretal2020CkEscQual]. 

### Fecundity

Fisheries and Oceans Canada, Salmon Enhancement Program collects and maintains production datasets for Pinkut and Fulton Sockeye. BLDP Production data to 1985 are reported in @West1987, and are currently being updated by Pacific Salmon Foundation. The data presented here are preliminary, and availabilty by year varies by project and channel. 

DFO-Salmon Enhancement Program (DFO-SEP) personnel collect biological data at the Pinkut and Fulton facilities, including sex ratio and estimated percentage of prespawn mortality (PSM). Biosampling is conducted at both spawning channels to assess body size, fecundity, and egg retention for spawners, which are incorporated into estimates of total egg deposition and density for each BLDP component. Potential fecundity, or the average number of eggs carried by spawning females, is measured from sacrificed samples collected across the observed size spectrum of spawning females at the Pinkut and Fulton spawning channels. Average potential fecundity is estimated by regressing egg counts to body length of sampled fish, and applying the regression equation to the average length of female spawners for each channel, river or creek.

Apparent fecundity, or the actual average number of eggs deposited, which accounts for egg retention (estimated from sampling spawned-out carcasses), is combined the number of effective females after accounting for  prespawn mortality to estimate the actual deposition of eggs deposited in each river, creek or section of channel. While potential fecundity is an indicator of the condition of female spawners entering the channels, apparent fecundity is required to estimate total egg deposition and egg to fry survival for a given year.

Estimates of potential fecundity, which are available for the spawning channels, Pinkut Creek and Fulton Rivers from 1998 onward show a decreasing trend, likely related to a trend in decreasing body size during that time period (Figure \@ref(fig:PotentialFecundity)). Estimates of apparent fecundity (potential fecundity minus egg retention), which are available for longer time series, show a decreasing and nonlinear trend since the 1970s which are likely related to overall declines in average body length that have been observed during the same time period (Figures \@ref(fig:PotentialFecundity) and \@ref(fig:ReducedFecundity)). 


(ref:PotentialFecundity) Calculated potential fecundity for Sockeye sampled at Pinkut and Fulton spawning channels, 1960-2020. Estimates cover 1998-2020 for Fulton systems and 2000-2020 for Pinkut systems.

```{r PotentialFecundity,  fig.cap="(ref:PotentialFecundity)" }
include_graphics("data/ChannelReview/PotentialFecundity.png")
```

(ref:ReducedFecundity) Trends in average apparent fecundity for Babine Sockeye sampled at Fulton Channel 2, 1960-2020. 

```{r ReducedFecundity,  fig.cap="(ref:ReducedFecundity)",out.width = "80%" }
include_graphics("data/ChannelReview/ReducedAverageFecundity.png")
```

### Egg Deposition and Fry Production

From 1973 to 1984, hydraulic sampling was used to assess egg survival in the channels, river and creek.  Hydraulic sampling was then discontinued because it does not assess all mortality prior to hatch and was not considered a replacement for the downstream fry enumeration program [@West1987].

Fry production for Pinkut Creek and Fulton River is assessed annually during the spring outmigration period after emergence, using fan or converging throat traps operated during the spring migration period to generate an estimate of the total abundance of fry entering the lake from both projects. In recent years (2015-2019), Fulton Channel 1 has been operated as part of the river and the fry counts have been combined.

Fry production is not assessed directly for wild Babine systems. A biostandard of 233 fry/spawner, derived from average egg to fry survival in the natural sections of Pinkut Creek and Fulton River is applied to estimate fry production for the early, mid and late-timed wild Babine Sockeye groups (MacDonald and Hume 1984), which are added to reported BLDP fry production to estimate total combined fry abundances for Babine Lake [e.g., @Woodetal1998Babine; @CoxRogersSpilsted2012Babine].

Egg deposition and fry production have remained relatively constant over time for Fulton River and Pinkut Creek. The three channels have seen decreasing trends in  egg deposition since the 1970s which may be related to lower fecundity for spawning females. Fry production has decreased somewhat for Fulton Channel two during this time period, but there is no clear pattern of reduced fry production for Fulton Channel #2 or Pinkut Channel. 

Egg to fry survival rates are higher in the spawning channels than in the natural river sections with regulated flow. This is not surprising, because the channels have been designed and managed for ideal spawning conditions, including water depth, flow, and substrate. For the post-BLDP period between 1970-2020, the average egg to fry survival rates for Pinkut Channel, Fulton Channel 1 and Fulton Channel 2 were 49%, 35%, and 48% respectively compared with 25% for Pinkut Creek and 18% for Fulton River. Egg to fry survival has not changed from pre-BLDP conditions since flow control structures were installed in Fulton River. There is no clear relationship between egg density and egg to fry survival for the spawning channels. Although there is evidence of decreasing egg to fry survival rates with increasing egg densities in Pinkut Creek and Fulton River, fry production increases with increasing egg densities in both systems, but the rates of increase are slower at higher densities than 2000-2500 eggs/m^2^, which are considered ideal for maximum fry production without a decrease in egg-to-fry survival (C. West, unpublished data).

(ref:EggDeposition) Total estimated egg deposition (x 10 million) for BLDP enhanced river, stream and channels, 1960-2020. Here, the upper and lower sections of Pinkut Creek (including the Pinkut Airlift) are combined, as are Fulton Channel 1 and Fulton River, which are assessed together in spring fry enumeration programs. 

```{r EggDeposition,  fig.cap="(ref:EggDeposition)" }
include_graphics("data/ChannelReview/TotalEggDepo.png")
```

(ref:FryProd) Estimated egg to fry survival for BLDP enhanced river, stream and channel, 1960-2020. Here, the upper and lower sections of Pinkut Creek (including the Pinkut Airlift) are combined, as are Fulton Channel 1 and Fulton River, which are assessed together in the spring fry enumeration programs.

```{r FryProd,  fig.cap="(ref:FryProd)" }
include_graphics("data/ChannelReview/FryProd.png")
```


(ref:EggToFry) Estimated fry production (x 10 million) for BLDP facilities, 1960-2020. Here, the upper and lower sections of Pinkut Creek (including the Pinkut Airlift) are combined, as are Fulton Channel 1 and Fulton River, which are assessed together in the spring fry enumeration programs.

```{r EggToFry,  fig.cap="(ref:EggToFry)" }

include_graphics("data/ChannelReview/EggToFry.png")
```


### Smolts

Babine Sockeye smolts are assessed annually at the outlet of Nilkitkwa Lake during the spring migration. Annual smolt abundance estimates are produced by mark and recapture estimation using a parsimonious model reported in MacDonald and Smith (1980). Smolts are sampled for length, weight, age, and prevalence of the parasite Eubothrim salvelini.  The smolt migration is bimodal, which allows for separate for a smaller first peak, consisting of smolts leaving Nilkitwka lake and the North Arm of Babine Lake, which are likely the progeny of late-timed Babine River spawners, and a second larger peak consisting of main-basin populations, including smolts originating from BLDP facilities. The smolt program, which did not operate from 2002-2012 due to budgetary restrictions, resumed in 2013 and is currently operated by Lake Babine Nation.

Smolt production from the BLDP has increased linearly with increasing fry production since the start of the BLDP. The average weight of sampled smolts in the pre and post BLDP periods were 5.4 g (SD 0.5 g) prior to 1975 and 4.8 g (SD 0.4 g) since 1976. The significant decrease in mean weight occurred in the pre BLDP period, and it has remained relatively constant since the production of BLDP smolts began.

Although there is a clear positive relationship between fry production and seaward migrating smolts from Babine Lake, the benefits of increased smolt production to adult returns are less clear, with high variability in smolt to adult survival.  From 1960-2000 (the years prior to the closure of the Babine smolt fence), smolt-to-adult survival ranged from 0.71 – 13.8 adult returns per smolt, with the highest survival rate observed for the 1995 brood year, following disease and associated prespawn mortality in 1994 and 1995 and associated prespawn mortality, and low fry production from BLDP facilities. There are no clear trends in smolt to adult survival, which is highly variable, however there is a weak positive relationship between smolt to adult survival (SAS) and smolt weight and a negative relationship between SAS and smolt abundance. 

(ref:SmoltPlots) Exploratory data analysis (EDA) plots of smolt abundance and weight. (A) Annual abundance of smolts in the main Babine basin, with construction start of enhancement facilities marked by the red vertical line; (B) Changes in smolt weight over time, with simple linear regressions fitted to two time periods: 1950-1968, 1969-2013; (C) Relationship between fry abundance and smolt abundance in the main basin; (D) Relationship between smolt weight and abundance in the main basin;  (E) Relationship between smolt survival and smolt weight; (F) Relationship between adult returns and smolt abundance, with simple linear trendline as a visual reference. More recent observations are shaded darker red in all six panels. Panels C-F include a simple linear trend line as a visual reference.

```{r SmoltPlots,  fig.cap="(ref:SmoltPlots)" }

include_graphics("data/ChannelReview/SmoltPlots.png")
```

### Limnology of Babine Lake

Limnological assessments conducted in the 1950s and 1960s found that the Sockeye rearing capacity of Babine Lake was underutilized [i.e., @Brett1951; @Johnston1956] and estimated that Babine Lake could support up to 300 million fry. The initial target for increased fry production for the BLDP of an additional 100 million fry was exceeded, with BLDP facilities estimated to have contributed an average of 125 million fry (range: 37 – 212 million) to Babine Lake since 1971, which together with the estimated fry production from wild spawning populations (1950-2021 average 68.0 million, range 9.4 – 209.2 million) is less than the estimated capacity of 300 million.

A more recent limnological assessment in 2000 used the PR (phtotosynthetic rate) capacity model estimated the rearing capacity of the main basin of Babine Lake  to be 219 million (Hume and Maclellan 2000), which combined with unsampled habitats in North Arm, Morrison Arm and Hagen Arm would likely approach 300 million fry, with additional rearing capacity in Nilkitkwa Lake. Hydroacoustic fall fry estimates carried out in Nilkitkwa Lake in 2013 and 2016 observed 0.99 and 0.67 million fry, respectively (Carr-Harris and Doire 2017).

Updated limnological assessments are needed to identify any large-scale changes that have occurred during the last two decades, during which Babine Lake has experienced higher temperatures and lower Sockeye returns than in the previous decades, which potentially affect nutrient loading. The results from relatively recent limnological surveys that were carried out at Babine Lake in 2013 and 2015, are not available at this time (D. Selbie, pers. comm., DFO Cultus Lake Salmon Research Laboratory, 2021).







<!--chapter:end:06_08-appendix_ChannelReview.Rmd-->

# SR MODEL FITTING RESULTS AND BIOLOGICAL BENCHMARK ESTIMATES FOR ENHANCED PINKUT AND FULTON {#PinkutFultonResults}


```{r , echo = FALSE, results = "asis"}


source("data/PinkutFultonApp/FUNCTIONS_prepTable.R")

sampled.posteriors.df <- read.csv("data/PinkutFultonApp/SampledPosteriors_Summary.csv",stringsAsFactors = FALSE)


pinkut.sampled.pars <- sampled.posteriors.df %>% dplyr::filter(Stock == "Pinkut")
fulton.sampled.pars <- sampled.posteriors.df %>% dplyr::filter(Stock == "Fulton")


```

This Research Document focuses on SR modelling for wild Sockeye stocks (16 Skeena, 4 Nass), but corresponding SR data, parameter estimates and benchmark estimates for Pinkut and Fulton are included here as a reference. Note, however, that these should not be used given SR model fitting issues and management differences discussed in Section \@ref(AltApproachEnhanced).

Observed productivity, in terms of ln(R/S), does not show a clear density-dependent pattern (Figures \@ref(fig:RpSPlotPinkut) and \@ref(fig:RpSPlotFulton)). This is due to a combination of spawning channel development, annual channel management, natural variation in productivity, density-dependence, and uncertain estimates of spawners and recruits, especially linked to estimating the non-spawning surplus (Section \@ref(SurplusEst)). Given this noisy data, Bayesian estimates of Ricker model parameters are highly sensitive to alternative data treatment assumptions (as illustrated for the Skeena aggregate in Figure \@ref(fig:AltFitPlotSkeena)) as well as alternative priors on productivity and capacity (i.e., the y-intercept and slope of a line fitted through the scatterplot in Figures \@ref(fig:RpSPlotPinkut) and \@ref(fig:RpSPlotFulton) is strongly affected by what we set as a plausible starting point). 

Biological benchmarks for Pinkut are substantially lower under the recent productivity scenario (Table \@ref(tab:BMTableLTAvgPinkut)) than  under the long-term average productivity scenario (Table \@ref(tab:BMTableNowPinkut)). For Fulton, the benchmarks are quite similar, with Smax, Smsy, and Seq a bit lower under recent productivity than under long-term average productivity, and Sgen a bit higher (Table \@ref(tab:BMTableLTAvgFulton)  vs. Table \@ref(tab:BMTableNowFulton)).







\clearpage
(ref:RpSPlotPinkut) Ln(R/S) Plot - Pinkut. Scatter plot of log productivity ln(R/S) vs. spawner abundance. Observations are colour-coded, with earlier data in fainter shading. The secondary axis illustrates the corresponding raw R/S values. Variations of the Ricker model attempt to fit a straight line through this scatter of points. The y axis intercept of the fitted line captures intrinsic productivity (i.e., R/S at very low spawner abundance) and the slope reflects the capacity (i.e., a steeper slope means more of a density-dependent reduction for each additional spawner, indicating lower capacity). 

```{r RpSPlotPinkut,   fig.cap="(ref:RpSPlotPinkut)"}
include_graphics("data/PinkutFultonApp/Pinkut_RpS_ScatterPlot.png")
```



\clearpage
(ref:RpSPlotFulton) Ln(R/S) Plot - Fulton. Scatter plot of log productivity ln(R/S) vs. spawner abundance. Observations are colour-coded, with earlier data in fainter shading. The secondary axis illustrates the corresponding raw R/S values. Variations of the Ricker model attempt to fit a straight line through this scatter of points. The y axis intercept of the fitted line captures intrinsic productivity (i.e., R/S at very low spawner abundance) and the slope reflects the capacity (i.e., a steeper slope means more of a density-dependent reduction for each additional spawner, indicating lower capacity). 

```{r RpSPlotFulton,   fig.cap="(ref:RpSPlotFulton)"}
include_graphics("data/PinkutFultonApp/Fulton_RpS_ScatterPlot.png")
```




\clearpage
(ref:BMTableLTAvgPinkut) Posterior distributions for selected SR parameters and resulting biological benchmarks - Pinkut with long-term average productivity scenario. This table shows estimates using parameters sampled from the AR1 model fit with capped uniform capacity prior. Variables with the ".c" suffix are the bias corrected version (e.g., Smsy vs. Smsy.c).

```{r BMTableLTAvgPinkut, echo = FALSE, results = "asis"}

prepTableBM(bm.df = pinkut.sampled.pars %>% dplyr::filter(Scenario == "LTAvg"),
            caption = "(ref:BMTableLTAvgPinkut)")
```

 
  
(ref:BMTableNowPinkut) Posterior distributions for selected SR parameters and resulting biological benchmarks - Pinkut with recent productivity. This table shows estimates using parameters sampled from the most recent generation (i.e., last 4 brood years) of the time-varying productivity (TVP) model fit with capped uniform capacity prior. Variables with the ".c" suffix are the bias corrected version (e.g., Smsy vs. Smsy.c).

```{r BMTableNowPinkut, echo = FALSE, results = "asis"}

prepTableBM(bm.df = pinkut.sampled.pars %>% dplyr::filter(Scenario == "Now"),
            caption = "(ref:BMTableNowPinkut)")
```




\clearpage
(ref:BMTableLTAvgFulton) Posterior distributions for selected SR parameters and resulting biological benchmarks - Fulton with long-term average productivity.  This table shows estimates using the AR1 model fit with capped uniform capacity prior.  Variables with the ".c" suffix are the bias corrected version (e.g., Smsy vs. Smsy.c).

```{r BMTableLTAvgFulton, echo = FALSE, results = "asis"}

prepTableBM(bm.df = fulton.sampled.pars %>% dplyr::filter(Scenario == "LTAvg"),
            caption = "(ref:BMTableLTAvgFulton)")
```

 
  
(ref:BMTableNowFulton) Posterior distributions for selected SR parameters and resulting biological benchmarks - Fulton with recent productivity. This table shows estimates using parameters sampled from the most recent generation (i.e., last 4 brood years) of the time-varying productivity (TVP) model fit with capped uniform capacity prior. Variables with the ".c" suffix are the bias corrected version (e.g., Smsy vs. Smsy.c).

```{r BMTableNowFulton, echo = FALSE, results = "asis"}

prepTableBM(bm.df = fulton.sampled.pars %>% dplyr::filter(Scenario == "Now"),
            caption = "(ref:BMTableNowFulton)")
```



<!--chapter:end:06_09-appendix_PinkutFultonResults.Rmd-->

