---
title: "Biological Benchmarks and Building Blocks for Aggregate-level Management Targets for Skeena and Nass Sockeye Salmon (*Oncorhynchus nerka*)"
author: |
  Gottfried. P. Pestal^1^ and Charmaine Carr-Harris^2^
author_list: "Pestal, G.P. and C. Carr-Harris"
address: |
  ^1^SOLV Consulting Ltd.\
     Vancouver, B.C.\
  ^2^Fisheries and Oceans Canada\
     Prince Rupert, B.C.\
month: "Month"
year: 2024
report_number: nnn
region: "Pacific Region"
isbn: "Insert ISBN"
cat_no: "Insert Cat No"
citation_other_language: "French citation goes here"
abstract: |
    Under the renewed Pacific Salmon Treaty (PST) provisions, Canada has agreed to complete a comprehensive escapement goal review for sockeye salmon (*Oncorhynchus nerka*) returning to the Skeena and Nass rivers, which include 31 stocks with a range of life histories and observed productivities. We tested alternative spawner-recruit model fits, developed guidelines for chosing alternative productivity scenarios based on the model fits, and calculated biological benchmarks for the selected scenarios. We also compare alternative approaches for combining stock-level estimates of biological benchmarks into aggregate reference points. A large proportion of sockeye salmon returns to the Skeena originates from the Babine Lake Development Project (BLDP), a low-intensity enhancement program that consists of a series of spawning channels and managed river sections on two Babine Lake tributaries (Pinkut and Fulton). As part of this review, we summarized production trends in BLDP production data and found that while loading densities for these systems have remained relatively constant over time, the overall productivity for the enhanced component of Skeena sockeye has decreased during the past 20 years.

header: "Prepublication: Not for further distribution" # or "" to omit
output:
 csasdown::resdoc_pdf:
   french: true
   copy_sty: true
   line_nums: false
   line_nums_mod: 1
   lot_lof: false
# ------------
# End of options to set
knit: bookdown::render_book
link-citations: true
bibliography: bib/refs.bib
# Any extra LaTeX code for the header:
header-includes:
# - \usepackage{tikz}
 # rest is from page 4 of https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf
 - \usepackage{booktabs} 
 - \usepackage{longtable}
 - \usepackage{array}
 - \usepackage{multirow}
 - \usepackage{wrapfig}
 - \usepackage{float}
 - \usepackage{colortbl}
 - \usepackage{pdflscape}
 - \usepackage{tabu}
 - \usepackage{threeparttable}
 - \usepackage{threeparttablex}
 - \usepackage[normalem]{ulem}
 - \usepackage{makecell}
 - \usepackage{xcolor}
 - \newcommand{\Smax}{$S_\textrm{max}$} 
 - \newcommand{\Smsy}{$S_\textrm{MSY}$}
 - \newcommand{\Umsy}{$U_\textrm{MSY}$}

---

```{r setup, echo=FALSE, cache=FALSE, message=FALSE, results='hide', warning=FALSE}
library(knitr)
if (is_latex_output()) {
  knitr_figs_dir <- "knitr-figs-pdf/"
  knitr_cache_dir <- "knitr-cache-pdf/"
  fig_out_type <- "png"
} else {
  knitr_figs_dir <- "knitr-figs-docx/"
  knitr_cache_dir <- "knitr-cache-docx/"
  fig_out_type <- "png"
}
fig_asp <- 0.618
fig_width <- 9
fig_out_width <- "6in"
fig_dpi <- 180
fig_align <- "center"
fig_pos <- "htb"
opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  fig.path = knitr_figs_dir,
  cache.path = knitr_cache_dir,
  fig.asp = fig_asp,
  fig.width = fig_width,
  out.width = fig_out_width,
  echo = FALSE,
  #  autodep = TRUE,
  #  cache = TRUE,
  cache.comments = FALSE,
  dev = fig_out_type,
  dpi = fig_dpi,
  fig.align = fig_align,
  fig.pos = fig_pos
)
options(xtable.comment = FALSE)
options(kableExtra.latex.load_packages = FALSE)
```

```{r load-libraries, cache=FALSE}
# `french` and `prepub` variables are extracted from the YAML headers above and
#  are used throughout the document. To make the document all in french, change
#  the line in the YAML header above to `french: true`
meta <- rmarkdown::metadata$output
if (length(grep("pdf", names(meta)))) {
  french <- meta$`csasdown::resdoc_pdf`$french
  prepub <- meta$`csasdown::resdoc_pdf`$prepub
} else if (length(grep("word", names(meta)))) {
  french <- meta$`csasdown::resdoc_word`$french
  prepub <- meta$`csasdown::resdoc_word`$prepub
}
csl <- "csl/csas.csl"
if (french) {
  csl <- "csl/csas-french.csl"
  options(OutDec = ",")
}

# add other packages here:
library(dplyr)
library(ggplot2)
library(readr)
library(tibble)
library(csasdown)
library(tidyverse)
library(kableExtra)
library(rosettafish)


# custom functions
source("functions/FUNCTIONS_prepTable.R")



# data used throughout

stock.info <- read.csv("data/SR Data/StockInfo_Main.csv", 
                       stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM") %>%
                dplyr::rename(ERInd = ER_Indicator) %>% 
                arrange(StkSeq) %>%
                mutate(ContrVal = round(ContrVal,1))

stock.info$NumSurvLk[stock.info$LifeHistory != "Lake"] <- NA #
stock.info$Stock <- gsub("Motase ","Motase",stock.info$Stock)

data.notes.tab <- read.csv("data/SR Data/NotesonSRDataInputs.csv", 
                       stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM")
data.notes.tab$Stock <- gsub("Motase ","Motase",data.notes.tab$Stock)

alt.sr.data <-  read.csv("data/SR Data/SR_Data_AltVersions_MERGED.csv", 
                       stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM")


# BM CALC TESTS

smsy.eq <- read_csv("data/Reference Tables/Smsy_Calc_Equations.csv")
test1.table <- read.csv("data/GeneratedReportTables/BMCalc_Test1_SampleParValues.csv",
                        stringsAsFactors = FALSE) %>% 
                        mutate(Smsy = round(Smsy,1),Sgen = round(Sgen,1))


test1.table$SgenCalc <- gsub("Connorsetal2022","Connorsetal2023", test1.table$SgenCalc)


test2.vals <- read.csv("data/GeneratedReportTables/BMCalc_Test2_GridTest_CalcVersions_PercRanges.csv", stringsAsFactors = FALSE)


test3.table <- read.csv("data/GeneratedReportTables/Test3_SpeedTest.csv", stringsAsFactors = FALSE)


test3.table$Method<- gsub("Connorsetal2022","Connorsetal2023", test3.table$Method)

# Alt SR TESTS

alt.sr.test1 <- read_csv("data/ReportTable_AltSRTest_PercChange.csv")










```





---
csl: `r csl`    
---

<!--chapter:end:index.Rmd-->

## Acknowledgements {-}

The work presented in this paper is part of a larger initiative aanaysisnd would not have been possible without the constructive contributions and guidance from a diverse group. Rights holders, stakeholders, and technical experts participating in a scoping workshop in Prince Rupert in 2019 set the initial course. A technical working group (Appendix \@ref(app:TWG)) coordinated data reviews and analytical work. Dr. Randall Peterman and Dr. Milo Adkison served as independent reviewers for the overall initiative and steered us through rough patches. Dr. Murdoch McAllister and Dr. Wendell Challenger contributed a separate set of spawner-recruit analyses to allow comparisons between methods. We thank all these contributors who have helped us get this far, and we look forward to continuing this journey with them. 

## Acronyms and Initialisms {-}


(ref:TableAcronyms) Short forms and expansions of technical terms used throughout the report. 

```{r TableAcronyms, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Reference Tables/Acronyms.csv")# %>% select

table.in[,1:2]  %>%     
   #mutate_all(function(x){x = as.character(x)}) %>%
   #mutate_all(function(x){gsub("-", "", x)}) %>% 
   #mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
     #mutate_all(function(x){gsub("@", "\\\\@", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = TRUE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableAcronyms)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "10em") %>%
     kableExtra::column_spec(2, width = "24em") #%>%
   #  kableExtra::column_spec(3, width = "12em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage
# INTRODUCTION

## ESCAPEMENT GOAL REVIEW {#Project}

### Background {#Background}

Sockeye salmon populations are changing rapidly with the cumulative effects of anthropogenenic stressors including fishing pressure, habitat degradation, and climate change. Skeena and Nass Sockeye, which are the second and third largest Sockeye runs in British Columbia, have experienced  declining productivity, together with increasing variability in run size and increased frequency of low returns since 2000. In recent years, low returns of Skeena and Nass Sockeye have led to reductions or closures of Canadian commercial fisheries, and restrictions on Indigenous fisheries targeting Skeena Sockeye in some years. The four lowest Nass Sockeye returns were recorded from 2017-2021. For Skeena Sockeye, the lowest escapements since a catastrophic Babine landslide in the 1950s occurred in 2013, 2017, and 2019.

Under the renewed Pacific Salmon Treaty (PST) provisions, Canada has agreed to complete a  comprehensive escapement goal analysis for Sockeye salmon (*Oncorhynchus nerka*) returning to the Skeena and Nass rivers [@PST]. Aggregate escapement goals for Skeena and Nass Sockeye salmon are used to set Annual Allowable Harvests (AAH) for U.S. and Canadian fisheries targeting both stock aggregates. In addition to renewed PST provisions, biologically-based escapement goals for Skeena and Nass River Sockeye salmon are used for Canadian fishery management including the Nisga’a Treaty [@NisgaaFinalAg], and First Nations Food, Social and Ceremonial (FSC), economic opportunity and recreational fisheries in the Skeena and Nass rivers.    

Aggregate Sockeye salmon returns to the Skeena and Nass watersheds are comprised of numerous  distinct stocks, some of which are depressed and are considered stocks of concern, while others have small spawner abundances and few or no reliable estimates [@SkeenaNassSkDataRep]. Enhanced-origin Sockeye salmon from two tributaries to Babine Lake account for a large proportion of aggregate Skeena Sockeye salmon production. Although the individual wild stocks, with run sizes ranging from hundreds to tens of thousands, are  much smaller in terms of abundance than enhanced Babine stocks, they account for most of the genetic diversity among Skeena and Nass Sockeye. Under the Wild Salmon Policy [@WSP], Canada is seeking to maintain the future productivity of Skeena and Nass Sockeye salmon returns by maintaining the genetically unique wild Sockeye salmon populations that contribute to overall returns. Concerns about potential overharvesting of smaller, less productive Skeena and Nass in mixed stock fisheries targeting the large lake stocks have been noted since at least the 1960s, prior to the start of BLDP-enhanced returns [e.g., @Larkin1968SkeenaPopBio]. More recently, an independent review of Skeena emphasized the need to consider tradeoffs between fishery management measures and WSP requirements to conserve already depressed weaker stocks [@Waltersetal2008ISRP]. 

The current aggregate escapement goals for Skeena and Nass Sockeye salmon, based on previous estimates of aggregate spawner abundance to produce maximum sustained yield (Smsy) are 900,000 for Skeena Sockeye and 200,000 for Nass Sockeye [@ShepardWithlerSkeenaBM; @RickerSmithSkeenaBM; @BockingetalMeziadinBM; @CoxRogers2013SkeenaMemo]. These goals do not consider the complex stock structure of each aggregate. Furthermore, the productivity of Skeena and Nass Sockeye salmon aggregates has declined considerably in recent years, and stock composition within and between the two aggregates has changed, most notably after the implementation of the enhancement facilities at Babine Lake in the 1970s, and a more recent reduction in the proportion of Sockeye returning to Meziadin Lake, which is the largest Sockeye system in the Nass watershed. Aggregate escapement goals based on maximum sustainable yield, which assume long term average productivity and stable stock composition, do not account for these changes and may not reflect current or future conditions for Skeena or Nass Sockeye. 

The current escapement goal for Skeena Sockeye does not consider the contribution from enhanced Babine Sockeye originating from the BLDP, which accounts for an average of 67% (1970-2020) of the Skeena Sockeye return, with high variability between years (range 33-83%). Given that the average ratio of enhanced to wild Skeena Sockeye is approximately 2:1, the assumed interim escapement goal for the wild component of Skeena Sockeye is approximately 300,000, or 1/3 of the current 900,000 escapement goal.

Skeena and Nass Sockeye, which originate in Canadian waters, are primarily harvested in Canadian and Alaskan commercial fisheries, and Indigenous fisheries in marine approach areas and throughout the Skeena and Nass watersheds.  A small number of Skeena and Nass Sockeye are harvested in recreational fisheries in each river. From 1985-2021, the average total exploitation rate was 54% for Skeena Sockeye  and 64.6% for Nass Sockeye [@NBTC2020]. Total exploitation rates for both Skeena and Nass Sockeye have decreased during this time period, and the proportion of Skeena and Nass Sockeye in the total harvest has varied by sector. 

Aggregate escapement goals for Sockeye salmon returning to the Skeena and Nass Rivers (Skeena and Nass Sockeye) are required to implement Pacific Salmon Treaty Chapter 2 provisions.  Chapter 2 allows the U.S. to harvest 2.45% of the Annual Allowable Harvest (AAH) of Skeena and Nass River salmon prior to U.S. Statistical Week 31 in the District 104 purse seine fishery, and 13.8% of the AAH for Nass Sockeye in the District 101 gillnet fishery. The AAH each year is calculated as the combined total run above the combined escapement targets for Skeena and Nass Sockeye, unless the run size falls below the combined escapement target of 1.1m, in which case the AAH is defined as the total run less actual escapement.


### Escapement Goal Review Process {#EGProcess}

The escapement goal review is guided by bilaterally accepted Terms of Reference that specify the following objectives:

1.	Summarize and evaluate relevant biological information to inform the development of aggregate escapement goals for Skeena and Nass Sockeye including an assessment of key uncertainties and gaps in the data for Sockeye populations in these basins. 
2.	Evaluate  alternative aggregate escapement goals for Nass and Skeena River Sockeye, including an evaluation of stock status, production, and implications of key uncertainties. 

Canadian members of the Northern Boundary Technical Committee (NBTC) of the Pacific Salmon Commission (PSC) have been tasked with leading a technical review of data, methods and metrics that can be used to developed biologically based escapement goals for Skeena and Nass Sockeye. A Technical Working Group (TWG) was established to support this work. The TWG included participants from Fisheries and Oceans Canada, North Coast Area First Nations, Pacific Salmon Foundation, and consulting organizations. Two independent reviewers were also identified, one by each country, to guide the technical work and review the resulting science advice. The technical work is one part of the broader escapement goal review process, and this document is one step in the technical work. Appendix \@ref(app:TWG) lists TWG members and independent reviewers.

The TWG and independent reviewers have completed four key steps leading up to this Research Document:  

* *Scoping workshop*: We prepared a workshop with Canadian rights holders and stakeholders in the fall of 2019, presenting an overview of planned technical work, seeking feedback on analytical priorities, and compiling suggestions for the overall escapement goal review process.
* *Data review - Part 1: Stock structure*: We consolidated information on population structure for Skeena and Nass Sockeye aggregates, established an agreed-upon list of 31 stocks for subsequent analyses, and documented how stocks align with designated Conservation Units (CU) under Canada's Wild Salmon Policy [@WSP]. A key finding of the stock structure review was that some of the existing CU designations were likely errors that required further review, but this is outside the scope of the current process. 
* *Data review - Part 2: Spawner-recruit data*: We compiled, reviewed, and updated available data to generate agreed-upon spawner-recruit data sets for stocks and aggregates [@SkeenaNassSkDataRep]. The 31 stocks were organized into three groups based on relative abundance and available data for subsequent analyses. Based on the data review and extensive sensitivity testing, we identified priorities for the analysis. The key issues highlighted for investigation are:
   1. *Changes in productivity*: Productivity for Skeena and Nass Sockeye has declined considerably in recent years with increasing variability in total returns and productivity for both aggregates, and for many of the component stocks. An effective escapement goal needs to consider that these changes are likely to persist in the future. The TWG and both independent reviewers identified time-varying productivity as one of the most important factors to consider in the analytical plan.
   2. *Aggregate vs. stock-level management reference points*: The Skeena and Nass Sockeye aggregates are both comprised of many smaller stocks with unique characteristics and population dynamics. A key objective for the review of Skeena and Nass Sockeye escapement goals is to recommend a combined aggregate escapement goal for Skeena and Nass Sockeye which considers stock-level genetic diversity in addition to variable productivity.
   3. *Enhanced stocks vs. wild stocks*: The largest component of the combined aggregate Skeena and Nass Sockeye return  originates from the BLDP-enhanced systems where spawner escapement is relatively constant and managed to maintain optimal production of fry. The current aggregate escapement goal for Skeena Sockeye of 900,000 spawners is based on spawner recruitment analyses that were conducted in the 1950s and 1960s [@ShepardWithlerSkeenaBM] prior to the BLDP. A review of management targets for Skeena Sockeye needs to consider the large contribution and spawning capacity for the enhanced stocks.

This Research Document provides science advice regarding these analytical priorities. Trade-offs and eventual decisions regarding aggregate management goals and associated harvest strategies, which will consider biological as well as other factors (e.g., socio-economic objectives), will depend on policy decisions and the specific objectives of First Nation and stakeholder groups which will be defined in a subsequent management process. This Research Document presents candidate biological benchmarks for Skeena and Nass Sockeye stocks based on the best available data and compares alternative approaches for developing aggregate biological escapement goals for these stocks.  Section \@ref(AnalysisOverview) explains the rationale for the scope and structure of this Research Document.

This project is closely linked to other ongoing initiatives: 

* New *Fish Stock Provisions* defined under Canada’s revised *Fisheries Act* [@NewFisheriesAct] require the development of Limit Reference Points (LRP) for major fish stocks. Stock management units have been defined for Pacific salmon, which are groups of CUs that have been organized into Stock Management Units (SMU), which are considered to be major fish stocks which will be managed as a unit to achieve joint status. Under the proposed framework to define Stock Management Units, Skeena and Nass Sockeye are defined as separate SMUs [@smuref]. 

* A key pillar of Canada's Wild Salmon Policy (WSP) is to identify Conservation Units and assess their status using a standard suite of indicators, which are combined into an overall integrated status [@WSP; @HoltbyCiruna2007; @CURev2019; @Holtetal2009BM]. An overarching goal of the WSP is to maintain CUs above their lower benchmarks to buffer from extinction risk and conserve their adaptive diversity. Implementation guidelines for the new *Fisheries Act* provisions are a key deliverable under the updated WSP implementation plan [@WSPImplementationAddendum]. Guidelines and case studies were peer-reviewed in early March 2022 [@LRPGuidelinesSAR]. Work to develop LRPs for Pacific salmon SMUs is onging, and we summarize recommendations for the types of analyses and analytical tools that would be required to support the development of LRPs for Skeena and Nass Sockeye, but do not include candidate LRPs for Skeena and Nass Sockeye in this Research Document.

* A Canadian domestic engagement process started in the Fall of 2022, in which rights holders and stakeholders are reviewing technical information and are providing feedback regarding the current Skeena and Nass escapement goals. The examples included in this Research Document are intended to provide a sound technical basis for prioritizing future work to support the engagement process.


### Research Document Objectives {#PaperObj}

The project mandate established by the Northern Panel of the Pacific Salmon Commission (Section \@ref(EGProcess)) requires the "development and evaluation of candidate benchmarks at the stock level and aggregate level". Aggregate benchmarks are required to implement international management provisions under the renewed Pacific Salmon Treaty [@PST], while stock-level benchmarks are needed to address conservation objectives under  Canada’s Wild Salmon Policy [@WSP].

The specific objectives of this Research Document are to:

1. Develop an approach for the evaluation and selection of spawner-recruit model fits using alternative data sets and alternative model forms, including time-varying model forms, and apply this approach at the stock and aggregate levels for Skeena and Nass Sockeye salmon.
2. Develop an approach to identify plausible alternative productivity scenarios (e.g., long-term average vs. current productivity) and corresponding spawner-recruit parameter sets.
3. Develop stock-level biological benchmarks using current data sets and appropriate methods for wild and enhanced Skeena and Nass Sockeye salmon stocks including:
   a.	Estimate and evaluate candidate biological benchmarks (e.g., Smsy, Smax, Sgen, Umsy) from model fits based on the plausible alternative productivity scenarios for wild Skeena and Nass Sockeye salmon stocks.
   b.	Review channel capacity and observed patterns in productivity for enhanced Skeena Sockeye salmon stocks originating from the Babine Lake Development Project.
4. Compare alternative approaches for choosing aggregate-level biological reference points for Skeena and Nass Sockeye salmon, evaluate advantages and disadvantages for each approach, and compare uncertainties in aggregate reference points generated using different approaches.
5. Identify priorities for future work to support the development of stock-specific escapement goals and aggregate reference points.
6. Examine and identify uncertainties in stock-level benchmarks by comparing outputs generated using alternative spawner-recruit model forms and data sets, and compare uncertainties in aggregate reference points generated using alternative approaches. 


## STOCK STRUCTURE OF SKEENA AND NASS SOCKEYE


### Life History Types, Stocks, and Conservation Units

Skeena and Nass Sockeye are the second and third largest Sockeye salmon returns in Canada, after Fraser Sockeye. Together, Skeena and Nass Sockeye comprise dozens of genetically unique populations that return to different tributaries throughout both watersheds and are harvested in large-scale commercial and numerous constitutionally protected Indigenous fisheries throughout both watersheds [@Moore2015SellingFN]. The importance of stock-level diversity, which is protected by Canadian fisheries policy, is a key consideration for the current review of Skeena and Nass Sockeye escapement goals. 

A key characteristic of both Skeena and Nass Sockeye salmon returns is that a single large lake accounts for most Sockeye production (Babine Lake for Skeena, Meziadin Lake for Nass). These large lake populations are themselves aggregates that are comprised of many smaller spawning populations, and for both, stock composition has changed over time. The many other smaller stocks account for most of the genetic diversity of Skeena and Nass Sockeye, and some are harvested in small-scale in-river or terminal Indigenous fisheries which support local economies [@GottesfeldSkeenaBook]. 

@BeachamWithlerSeaType describe three alternative life history strategies observed in the juveniles of sea-going (anadromous) Sockeye salmon: 

* *lake-type Sockeye* spawn in lakes or lake tributaries, and rear in the lake for at least 1 year after hatching
* *sea-type Sockeye* spawn in tributaries or mainstem side channels, and the juveniles rear for several months in estuarine waters after hatching, with a total freshwater residency of less than one year
* *river-type Sockeye* spawn in tributaries or mainstem side channels, and the juveniles rear in a river environment for at least 1 year before migrating to the ocean

Lake-type Sockeye account for most of the large stocks on the Pacific Coast, but river- and sea-type Sockeye may have more adaptive potential, because they are less specialized for specific sites and are more versatile in their use of variable or changing habitats  [Sec. 9.2 in @HoltbyCiruna2007]. Evolutionary linkages between lake-, sea- and river-type Sockeye populations continue to be explored [e.g., @WoodetalLifeHist1987;  @BeachamWithlerSeaType; @WoodLifeHist1995;@WoodetalLifeHist2008; @Beachametal2004transboundary].

Most Sockeye that originate from the Skeena and Nass watersheds follow the lake-type life history, but there are river-type populations that spawn throughout both basins. There are also at least two sea-type populations that spawn in the lower Nass River in Gingit and Gityzon creeks [@Beveridgeetal2015GingitSea]. Contributions from these sea-type populations to the Nass aggregate have increased in recent years. The Lower Nass sea-type population, for which the most abundant spawning population (Gingit Creek) has been surveyed regularly since 2000 [@Beveridgeetal2015GingitSea], contributed about 31% of the Nass Sockeye return in 2019 [@NFWD2020].

Under the Wild Salmon Policy, Canadian anadromous salmon have been grouped into distinct *conservation units* (CU), defined as "a group of wild salmon sufficiently isolated from other groups that, if extirpated, is very unlikely to recolonize naturally within an acceptable timeframe" [@WSP]. For Nass and Skeena Sockeye, CU definitions are based on preliminary designations that were established in 2009, which for lake-type Sockeye generally correspond to the rearing lake of origin [@HoltbyCiruna2007].  Most of the stocks identified in our analyses align with a single CU, while for some smaller stocks, we have combined 2-3 CUs, either because they rear in cojoined lakes and the population structure is unclear, or they are assessed together, and the data cannot be separated. Babine, the largest CU, is split into 5 distinct stocks based on enhancement status and run timing. 

Nass and Skeena Sockeye have been organized into 31 stocks for this review, including 7 Nass and 24 Skeena stocks, as described in @SkeenaNassSkDataRep and summarized in Figure \@ref(fig:PopStrucGeneral). Stocks can be grouped together based on life history and adaptive zone, as well as by watershed.

###  Babine Lake Development Project (Pinkut and Fulton)

Babine Lake is the largest natural freshwater lake in British Columbia and the largest producer of Sockeye salmon in the Skeena basin, and has accounted for 87-93% of aggregate Skeena Sockeye returns since 2000 [i.e., @CoxRogersSpilsted2012Babine], and includes wild and enhanced populations. The Babine Lake Development Project (BLDP) consists of a series of spawning channels and flow control structures that were built on Pinkut Creek and Fulton River starting in the late 1960s to increase the production of Babine Sockeye.

Wild Babine Sockeye spawn in dozens of tributaries and lake-spawning habitats throughout the main basin of Babine Lake, in sections of Babine River between Babine and Nilkitkwa Lake, and in Morrison Lake and Tahlo Creek. Here, “Babine Sockeye” refers to all Sockeye returning to Babine Lake and upstream areas and includes wild and enhanced Sockeye from the Babine-Nilkitkwa and Morrison-Tahlo CUs, which are assessed together at the Babine weir and smolt enumeration programs and run-reconstruction procedures. Sockeye returning to Fulton River and Pinkut Creek, along with the late-timed Babine River stock, were the largest Babine Lake Sockeye populations before the BLDP.

From a fisheries perspective, the BLDP has proven to be a successful enhancement program that has substantially increased returns of Babine and Skeena Sockeye. West and Mason (1987) estimated that for the first two decades following completion of the BLDP spawning channels and associated infrastructure, total returns for Skeena Sockeye nearly doubled, from 1.3 to nearly 2.5 million, with commensurate increases in fishery landings. 


(ref:PopStrucGeneral) Population Structure of the Skeena/Nass Sockeye Management Unit. This figure summarizes all stocks and current Conservation Unit (CU) delineations, grouping them based on life history (i.e., lake-type vs. river-type and sea-type) as well as freshwater adaptive zone (LHAZ). Enhanced Pinkut and Fulton are part of the Middle Skeena Lake-Type LHAZ.

```{r PopStrucGeneral, out.width= 350,  fig.cap="(ref:PopStrucGeneral)",fig.pos="H"}
include_graphics("data/Pop_Structure_Small.PNG")
```



Sockeye returns to the channel-enhanced systems have exceeded the capacity of the available spawning habitat in the spawning channels and managed sections of Pinkut Creek and Fulton River in most years since the start of the BLDP (Wood 1995). Enhanced Sockeye returns to in excess of spawning capacity are considered surplus production. Some of these fish are harvested in ESSR (Excess Sockeye to Spawning Requirements) in Babine Lake after loading targets for wild and enhanced systems have been met, but these fisheries do not take place every year.

Concerns have been raised about negative effects of the enhanced Babine Sockeye on wild Babine and other Skeena Sockeye stocks. Increased returns of enhanced Babine Sockeye have introduced new fishing pressure on wild Babine and other Skeena Sockeye stocks in mixed stock fisheries, with less productive wild stocks experiencing higher exploitation rates following more intensive mixed-stock fisheries targeting enhanced returns. Sockeye returns to wild Babine systems, and the numerous non-Babine Skeena Sockeye populations, which were in already decline prior to the start of the BLDP, have seen further declines since its implementation.

Other potential interactions between wild and enhanced Babine Sockeye populations include  potential straying of enhanced Sockeye into wild spawning tributaries, and competition related to density dependence in freshwater and marine rearing habitats. The BLDP spawning channels were built after limnological assessments conducted in the 1950s and 1960s found that the Sockeye rearing capacity of Babine Lake was underutilized [i.e., @Brett1951; @Johnston1956] and that Sockeye production was limited by available spawning habitat. At the time, Babine Lake was estimated to have rearing capacity to support 300 million Sockeye fry [@West1987]. Further, size and size at age have declined for Skeena Sockeye consistent with other salmon populations in the North Pacific [@Oke2020RecentDeclinesBodySize], along with declines in fecundity, and modest declines in overall length are associated with much larger decreases in fecundity. For example, average fecundity for Pinkut and Fulton Sockeye, measured at the enhancement facilities, has declined over 10% since the 1980s. A significant decline in fecundity for wild and enhanced Skeena Sockeye could contribute to reduced productivity for these populations. An abundance based escapement goal based on the number of spawners which assumes constant egg production over time may not account for demographic changes such as changes in body size, age composition, or sex ratio, which have the potential to increase the escapements required to achieve maximum sustained yield over time [@Staton2021].

Incorporating BLDP capacity limitations into the development of an aggregate escapement goal for Skeena Sockeye poses challenges because loading targets for the spawning channels and flow-controlled sections of Pinkut Creek and Fulton River are fixed to maintain optimal densities of spawners to maximize fry production. Spawner-recruitment models such as the Ricker models which were used to develop biological benchmarks for other stocks, require a range of spawner escapements (i.e., contrast in the data) may not produce useful parameter estimates for the enhanced stocks. 

Enhanced Pinkut and Fulton Sockeye currently represent the largest component of Skeena Sockeye and are thus an important consideration for developing an aggregate escapement goal. However, a full review and updated recommendations for loading targets and operational procedures will require input and advice from the facility operator (DFO Salmonid Enhancement program) and is outside the scope of the current review, which focused on developing biological escapement goals for wild Skeena and Nass Sockeye stocks. In this paper we summarize estimation methods and run-reconstructions specific to Babine Sockeye, and assessed trends in surplus production and ratio of wild and enhanced Skeena Sockeye in order to develop advice for incorporating the enhanced stocks into an aggregate escapement goal for Skeena Sockeye.

To address questions about the effects of enhancement on wild Sockeye production, we reviewed and updated production data for wild and enhanced Babine Sockeye to assess general trends in adult returns, escapement quality (size, sex ratio and fecundity), egg production, and fry outputs. This was not intended to be a comprehensive assessment of Babine Sockeye production, or a detailed analysis of the effects of the BLDP enhancement facilities on wild Babine and other Skeena Sockeye stocks. Rather, we provide a high-level overview of observed trends in freshwater production based on available information and make recommendations for further work on this topic.


## ANALYSIS OVERVIEW {#AnalysisOverview}

### Research Document Scope and Organization {#PaperScopeOrg}

The analyses presented in this Research Document were bounded based on scoping discussions throughout the Skeena and Nass Sockeye escapement goal review process since 2019. These discussions included a technical working group process, scoping workshop, and feedback from two independent reviewers (Section \@ref(EGProcess)), as well as the peer-review process [@SkeenaNassSkSAR; @SkeenaNassSkPRO] which included the main Regional Peer Review (RPR) meeting in April of 2022 and a follow-up process led by CSAS to develop recommendations on alternative approaches for developing aggregate management reference points for Skeena and Nass Sockeye. 

Data issues were mostly resolved by the Technical Working Group (TWG) prior to the peer-review meeting, with details documented in a stand-alone report [@SkeenaNassSkDataRep]. Data-related revisions identified through the peer review focused on clarifications and some additional testing of data treatment choices, such as procedures for infilling missing brood years.

The focus of discussions around the scope of our analyses related to the three topics that were identified as key analytical priorities by the TWG (Section \@ref(EGProcess)), including:

* Methods for spawner-recruit SR model fitting to account for time-varying productivity
* Methods for developing aggregate management reference points based on stock-level SR model fits
* Treatment of enhanced Pinkut and Fulton in the analyses

This paper is structured as a series of modular steps, and includes extensive sensitivity testing of the initial steps (i.e., developing candidate SR models and model selection), or "building blocks" that are used in subsequent analyses. We provide examples for each of the subsequent steps which demonstrate how the biological information produced in the initial steps could be used to develop scientific advice for developing reference points. Depending on the specific context and requirements of current the Skeena and Nass Sockeye escapement goal review and future processes, this advice can be applied to develop specific questions, and specific analyses prioritized to address them. The examples provided in the second part of the results are intended to assist with identifying those priorities.

The next three sections summarize how we approached each of these three components, and how our work compares to previous analyses. Section \@ref(TORLink) describes how this Research Document addresses each of the objectives in the Terms of Reference, linking to specific sections of the paper.



### Spawner-recruit Model Fits, Productivity Scenarios and Biological Benchmarks {#AnalysisOverviewSR}



Previous work on SR model fits and biological benchmarks for Skeena and Nass Sockeye has differed widely in scope and approach (Table \@ref(tab:PastWorkTable)). Earlier work was mainly focused on estimating stock-level SR-based benchmarks like Smsy, Smax, and Umsy [@BockingetalMeziadinBM;  @Waltersetal2008ISRP; @KormanEnglish2013; @Hawkshaw2018Diss]. @PacificSalmonExplorer also included percentile benchmarks. SR model details varied widely across these analyses. 

To address the priorities and Research Document objectives listed in Section \@ref(PaperObj), we fitted alternative model forms of the Ricker model to each spawner-recruit data set (Skeena and Nass aggregates and 20 component stocks with sufficient SR data). The types of models used for each stock depended on data availability. We focused on developing simple single-stock models along with time-varying model forms (AR1 and Recursive Bayes models) to explore underlying productivity patterns for the different stocks and calculate biological benchmarks for alternative productivity scenarios.

Following a review of candidate SR model fits by TWG participants, we selected model fits that best described the dynamics of each stock. We sampled from the posterior distributions of selected single stock models to characterize high, low, recent, and long-term productivity scenarios, which were then used as inputs for examples of alternative methods for developing aggregate management reference points, as summarized in the next section.  

Standard biological benchmarks based on spawner recruitment parameters were calculated for each model fit (Smsy, Smax, Umsy; Section \@ref(BMMethods)). The resulting benchmark estimates were compared to observed spawner abundances and lake capacity estimates. 

Some TWG members recommended the development of an updated version of a hierarchal Bayesian model (HBM) used in a previous process to estimate biological benchmarks for Skeena Sockeye, which would allow for a direct comparison of results generated using updated data. McAllister and Challenger (Appendix \@ref(app:HBMFits)) contributed preliminary results for the updated HBM model fits. We include a comparison of the initial HBM results to our single-stock SR model fits and discuss potential sources for observed discrepancies (Sections \@ref(HBMExploration) and \@ref(HBMResultsComp)). However, we did not include the initial HBM results in the model selection process to define different productivity scenarios used for the example results in the rest of the paper. This may be a priority for future work.

The initial version of the Research Document was presented for peer review in April 2022 and included alternative SR model fits and alternative productivity scenarios. Revisions identified through the peer-review process focused on additional sensitivity testing (e.g., additional productivity scenarios), comparisons of alternative methods (e.g., single-stock vs. hierarchical model fits), and clarification of the analytical steps (e.g., log-normal bias correction, smoothing of time-varying parameter estimates).


### Alternative Approaches for Developing Management Reference Points for Stock Aggregates {#AltApproachesOverview}

The analyses presented in this Research Document are intended to inform a review of Skeena and Nass Sockeye management targets. The development of management targets needs to take into account social and economic considerations in addition to biological objectives, and requires additional work after biological benchmarks  have been estimated, such as Smsy [e.g., @HoltIrvineBMvsRP]. 

Considerations for choosing an appropriate approach for developing management targets include:

* *Type of harvest strategy*:  The purpose of management reference points is to trigger a response to changing conditions. Therefore, the approach to developing management reference points needs to fit the harvest strategy being used (i.e., fixed escapement, fixed exploitation rate, abundance-based rule). 
* *Specific objectives*:  Clearly specified objectives are needed to allow for a structured and consistent comparison of alternative management reference points. 

A straightforward approach that has been applied extensively in Pacific salmon fishery management is to select the estimate of a biological benchmark, such as Smsy, as the management target. This approach assumes (1) a harvest strategy where the stock overall is managed to a fixed escapement goal, and all returning adults in addition to the escapement goal are harvested; and (2) a management objective to maximize total harvest on average over the long-term, regardless of annual variability in harvest (i.e., most of the year-to-year variation in returns translates into variation of harvests).  

(ref:PastWorkTable) Overview of Previous Work on Biological Benchmarks and Escapement Goals for Skeena and Nass Sockeye. Previous work differs in terms of scope, estimation approach, analysis type, and performance evaluation. 

```{r PastWorkTable, echo = FALSE, results = "asis"}


past.work.df <- read.csv("data/Reference Tables/PastWork_Overview.csv",stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM") %>%
                        select(-Benchmark.Calculation)


colnames(past.work.df ) <- linebreak(c("Reference","Main\nObjective","Scope","Parameter\nEstimation", # "Benchmark\nCalculation",
                                         "Forward\nSimulation","Performance\nEvaluation"))



past.work.df %>%
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "l",
                  caption = "(ref:PastWorkTable)") %>%
  kableExtra::column_spec(1, width = "4em") %>%
  kableExtra::column_spec(2, width = "7em") %>%
  kableExtra::column_spec(3, width = "9em") %>%
   kableExtra::column_spec(4, width = "12em") %>%
  kableExtra::column_spec(5, width = "7em") %>%
  kableExtra::column_spec(6, width = "9em") %>%
  # kableExtra::column_spec(7, width = "8em") %>%
   kableExtra::row_spec(1:  (dim(past.work.df)[1] -1), hline_after = TRUE)


```




\clearpage
Alternatives to this basic approach have evolved to:

* capture other objectives (e.g., reduce variability in harvest)
* deal with practical considerations (e.g., constraints on implementing target harvests)
* consider current conditions (e.g., recent returns, recent productivity)
* consider multiple stocks in an aggregate

The initial version of the Research Document included examples of seven alternative approaches. Each of these had previously been used or recommended for the development of Pacific salmon escapement goals, and some had been previously applied to Skeena or Nass Sockeye (Table \@ref(tab:PastWorkTable)). The strengths and limitations of these alternative approaches were discussed extensively during the CSAS RPR, but participants did not reach consensus or  firm recommendation regarding a single approach, and recommended the inclusion of one additional candidate approach. This paper includes examples of a total of eight alternative approaches.

Peer review participants recommended that clear advice on choosing an appropriate approach *should* be developed. A subgroup of participants was then convened to develop this advice, which included (1) identifying criteria for evaluating the alternative approaches, (2) completing a detailed evaluation of each approach, and (3) generating a summary table of comparisons, along with an overview of practical challenges for the alternative approaches. This structured comparison of approaches is a key product of the peer review process, and is documented in Section \@ref(AltApproachesComp). This Research Document includes worked examples for each approach, so that decision makers have a tangible basis for prioritizing next steps.


### Alternative Approaches for Inlcuding Enhanced Pinkut and Fulton in the Analyses {#AltApproachEnhanced}

Most Skeena Sockeye production occurs in actively managed spawning channels at Pinkut and Fulton, which are fundamentally different from wild stocks where variations in observed spawner abundance and resulting production can be used to fit spawner-recruit models and estimate biological benchmarks. This creates challenges for modelling stock dynamics and for developing management targets for the stock aggregate. 

This Research Document focuses on wild Skeena and Nass Sockeye stocks. A comprehensive review of enhanced production dynamics and channel loading targets is outside the scope of this work, and may be addressed in a separate process led by DFO's Salmonid Enhancement Program, which operates the Babine enhancement facilities. An explicit analysis of trade-offs between enhanced production objectives and wild stock objectives is also outside the scope of the current project. 

Previous analyses of Skeena Sockeye, summarized in Table \@ref(tab:PastWorkTable), treated Pinkut and Fulton the same as for wild stocks, either in a single-stock analysis or as part of the aggregate. We assume that the spawner time series used for these analyses were actual channel loading plus capacity below the fence, rather than the gross escapement (i.e., accounted for top-ups and non-spawning surplus). However, this was not easily verifiable.

Three types of approach for accounting for enhanced Skeena Sockeye from Pinkut and Fulton were discussed during the peer-review and follow-up process: 

* *Develop SR-based benchmarks for all wild and enhanced stocks*: Fit alternative SR model forms to generate parameter sets for alternative productivity scenarios for wild and enhanced stocks, and include resulting benchmarks for enhanced stocks in the development of alternative management targets. Caveats should be clearly stated regarding the SR model fits for enhanced stocks and their implication for the results shown, and nuances for interpretation. For example, estimates of Smsy for Pinkut and Fulton are not only highly uncertain due to the low contrast in spawner abundance, but they are also not relevant to the current management approach which focuses on loading targets that maximize fry production. 
* *Explicitly model the distinct management and production dynamics of the enhanced stocks*: Depending on the chosen aggregation approach, the dynamics of enhanced stocks could be incorporated in different ways. For instance Pinkut and Fulton could be added to a forward simulation by developing additional model components, which would include an enhanced sub-model of the population dynamics (e.g., two steps linking channel loading to fry output, and then fry-to-adult survival), a harvest sub-model that reflects differences in wild and enhanced harvest (e.g., fisheries targeting the enhanced non-spawning surplus in some years), and a channel management sub-model that captures how loading targets might change with aggregate and stock-specific abundance. This type of model could test interactions between alternative harvest strategies and alternative channel operation strategies, and explore trade-offs between total harvest including enhanced fish and conservation objectives for wild stocks.
* *Clearly distinguish between considerations of wild and enhanced Skeena Sockeye*: Given WSP requirements, estimate biological benchmarks for wild stocks and apply the candidate approaches for aggregate management targets to the *Skeena Wild* stock aggregate, then explore how the resulting management targets could be scaled up to cover the entire Skeena return. 


The third option is the most feasible for the current project, given the complexity of capturing enhanced production dynamics in a detailed forward simulation. Thus, most of the analyses in this paper focus on the wild stocks, but we have included a review of available information the Babine Sockeye enhancement facilities (Appendix \@ref(ChannelReview)), and we include two examples of how management targets for *Skeena Wild* could be scaled up to account for enhanced production (Section \@ref(SkeenaExpResults)). We recommend future work to more fully integrate enhanced and wild considerations take place as part of a  management strategy evaluation for Skeena Sockeye. As a reference, some SR model results for the enhanced stocks are provided in Appendix H.


### How this Research Document Addresses the Terms of Reference {#TORLink}

We reviewed and updated the spawner recruit data, and set up a framework for fitting and selecting alternative spawner-recruit models. We then used the selected spawner-recruit model fits in worked examples of alternative approaches for developing aggregate management reference points. Our approach was designed with a focus on flexibility and the ability to respond rapidly to changing conditions, new data, alternative benchmark formulations, requests for alternative productivity scenarios, and comparison of alternative objectives. Much work remains to fully connect the information and decision makers through a structured process, such as a management strategy evaluation (MSE). 

Below we briefly summarize how this Research Document addresses each objective from the Terms of Reference:

1. *Develop an approach for the evaluation and selection of spawner-recruit model fits using alternative data sets and alternative model forms, including time-varying model forms, and apply this approach at the stock and aggregate levels for Skeena and Nass Sockeye salmon:* We developed a checklist for identifying candidate SR models for single-stock model fits, covering three alternative model forms (Figure \@ref(fig:CandidateModels)). We tested the candidate models on 20 stock-level data sets and three aggregate-level data sets (Sections \@ref(CandidateStockModels) and \@ref(CandidateAggModels)), and completed extensive sensitivity tests (Section \@ref(Convergence)). In addition, we compared the single-stock model fits to results from an independent SR analysis by McAllister and Challenger (Appendix \@ref(app:HBMFits)) using a Hierarchical Bayesian model on the same data sets (Section \@ref(HBMResultsComp)).
2. *Develop an approach to identify plausible alternative productivity scenarios (e.g., long-term average vs. current productivity) and corresponding spawner-recruit parameter sets:* We identified four alternative productivity scenarios intended to provide  contrasting stock dynamics, and established guidelines for sampling parameter sets from the candidate SR models to populate the scenarios (Figure \@ref(fig:ModelSelection)).
3. *Develop stock-level biological benchmarks using current data sets and appropriate methods for wild and enhanced Skeena and Nass Sockeye salmon stocks including:*
   a.	*Estimate and evaluate candidate biological benchmarks (e.g., Smsy, Smax, Sgen, Umsy) from model fits based on the plausible alternative productivity scenarios for wild Skeena and Nass Sockeye salmon stocks:* Section \@ref(BMResults) presents aggregate-level and stock-level estimates under alternative productivity assumptions.
   b.	*Review channel capacity and observed patterns in productivity for channel-enhanced Skeena Sockeye salmon stocks originating from the Babine Lake Development Project:* Appendix \@ref(ChannelReview) summarizes available production information for the Babine enhancement facilities.
4. *Compare alternative approaches for choosing aggregate-level biological reference points for Skeena and Nass Sockeye salmon, evaluate advantages and disadvantages for each approach, and compare uncertainties in aggregate reference points generated using different approaches:* We illustrated building blocks for eight alternative aggregation approaches (Sections \@ref(BMResults) to \@ref(ProjBesdResults)): aggregate model fits, simple sum of stock-level abundance benchmarks, comparison of aggregate and stock-level sustainable exploitation rates, stock-level equilibrium profiles based on fixed spawner targets, aggregate equilibrium profiles based on fixed exploitation rates, status-based aggregate limit reference points, aggregate reference points based on logistic regression, and forward simulations. The simulation results include two high-priority extensions identified during the peer review (covariation in productivity, outcome uncertainty). We compare the results in Section \@ref(AggregationComparison).
5. *Identify priorities for future work to support the development of stock-specific escapement goals and aggregate reference points:* Priorities for future work are listed Section \@ref(PrioritiesFuture).
6. *Examine and identify uncertainties in stock-level benchmarks by comparing outputs generated using alternative spawner-recruit model forms and data sets, and compare uncertainties in aggregate reference points generated using alternative approaches:* We completed extensive sensitivity tests of the spawner-recruit model fits, including alternative model forms and alternative prior assumptions (Section \@ref(SingleStockSRResults)), and compared the results to estimates from a Hierarchical Bayesian model by McAllister and Challenger (Appendix \@ref(app:HBMFits)) on the same data set (Section \@ref(HBMResultsComp)). In addition, we tested alternative data treatment options (Appendix \@ref(AltSRTest)), alternative calculation approaches for biological benchmarks (Appendix \@ref(BMCalcTest)), and the effect of including a lognormal bias correction on the productivity parameter ln.alpha (Appendix \@ref(BiasCorrtest)). We compare aggregation approaches in Section \@ref(AggregationComparison).
 
 

<!--chapter:end:01-Introduction.Rmd-->

\clearpage
# METHODS

This chapter describes the six steps of our analyses: 

- *Review of spawner-recruit (SR) data*: summarizes data review, run reconstruction, age composition assumptions, and available SR data by stock.
- *Enhanced production review*: briefly describes the data sources and compilation of available information for the BLDP-enhanced stocks (Pinkut and Fulton).
- *SR model fitting*: describes alternative model forms, Bayesian implementation, and criteria for identifying candidate models for each stock, depending on available data.
- *Productivity scenarios*: describes how parameter sets were sampled from shortlisted SR model fits to represent long-term average, recent, high, and low productivity scenarios for each stock.
- *Biological benchmarks*: describes how standard benchmarks (e.g., Smsy) were calculated for each stock, given alternative productivity assumptions.
- *Management reference points (MRP)* (Section \@ref(AnalysisOverview)): describes alternative approaches for developing MRPs (e.g., equilibrium profiles, forward simulations) given alternative productivity assumptions and various examples of management objectives, and rationale for the examples provided in this Research Document.  


## DATA SOURCES {#DataSources}



### Data Review {#DataReview}

The type and quality of available data shape the type and quality of scientific advice that can be developed regarding salmon management strategies [e.g., @AdkisonData]. The importance of developing an up-to-date and agreed-upon set of spawner-recruit data was a recurring topic in TWG discussions and the scoping workshop. Therefore, the TWG allocated a substantial part of project effort to an in-depth data review for Skeena and Nass Sockeye, which is documented in a stand-alone report [@SkeenaNassSkDataRep] and briefly summarized here.  The updated SR data has established a solid foundation for future work and the review process helped streamline the workflow for future data updates.

The data review addressed the first objective of the escapement goal review (Section \@ref(EGProcess)) and covered key components of the spawner-recruit data: spawner estimates for indicator systems and associated expansions, in-river run reconstructions (incl. adult return timing assumptions), First Nations catch estimates, and age composition estimates. Several other relevant reviews and data set updates have been occurring concurrently, including a review of the Northern Boundary Sockeye Run Reconstruction (NBSRR) model and a comprehensive review of the Nass salmon abundance estimation programs. We used the latest available information from these reviews as of December 2021, up through the 2019 return year.

The data review included six steps: (1) a review of stock structure for Skeena and Nass Sockeye populations, (2) updates to source data to incorporate all available information, (3) automated data processing, (4) automated data checks focused on contrast, changes over time, and potential outliers, (5) working with the TWG to generate data notes for each stock, and (6) extensive sensitivity testing (e.g., retrospective estimates of biological benchmarks using simple deterministic Ricker parameter estimates).








\clearpage
### Spawner Estimates {#SpnEst}

Estimates of spawner abundance for Skeena and Nass Sockeye come from a combination of assessment programs conducted throughout both watersheds. Stock assessment programs include fishwheels on the lower Nass and a test fishery on the lower Skeena, which generate estimates of abundance and age and stock composition for the two aggregates. High-accuracy census counts are conducted for the largest systems in each basin (Meziadin fishway for the Nass, Babine weir for the Skeena). Spawning ground enumeration programs, including weir counts, aerial surveys, mark-recapture programs, and stream walks, generate spawner counts for indicator systems, which are expanded to estimate the total escapement for each stock.

Babine Sockeye have been counted at the Babine weir downstream of Nilkitkwa Lake annually since 1949. The Babine weir which is currently operated by Lake Babine Nation, under contract to Fisheries and Oceans Canada, provides daily counts for all salmon species from the middle of July until the end of September and encompasses most of the Sockeye return. The weir operation has been extended to the end of November in some years. The weir program is assumed to provide a complete count for most years, but adjusted in some years for estimated passage during times when the fence was not operational.

Visual escapement estimates of up to 30 wild Babine Sockeye spawning tributaries are conducted annually by foot or aerial surveys led by DFO and Lake Babine Nation. Estimates from visual escapement surveys for wild Babine systems are adjusted to account for underestimation bias using methods described in Wood (1995). Annual stream counts for individual Babine systems are maintained in the Fisheries and Oceans Canada [NuSEDS database](https://open.canada.ca/data/en/dataset/c48669a3-045b-400d-b730-48aafe8c5ee6).  The raw spawner estimates for the different wild Babine systems are expanded and combined into adjusted estimates for the early, mid and wild run timing components using a run-reconstruction procedure described by @WoodLifeHist1995.

DFO completed an extensive review of spawner estimates for Skeena and Nass indicator streams. All available stream escapement information from local and regional data holdings were compiled and reviewed to assess whether any additional data were available for indicator streams and years that were identified as missing in previous versions of the NCCDSB database, and to check the accuracy of published NuSEDS estimates. For years that individual stream count data were available (1998 onwards for most systems), escapement estimates were recalculated and compared with the NuSEDS data to identify any discrepancies. 

\clearpage
### Estimating Biological Surplus of Enhanced Pinkut and Fulton {#SurplusEst}

Enhanced Babine Sockeye spawners are counted through weirs below Pinkut Creek and Fulton River, and as they enter the spawning channels. Wild spawners are enumerated by visual counts which are conducted by foot and aerial surveys to generate Area-Under-the-Curve (AUC) estimates for the wild tributaries. Wild Babine Sockeye are assigned to three groups (early, mid, and late) based on adult run timing.

The Babine fence count usually exceeds the sum of escapements to enhancement facilities and visual escapement estimates for wild systems, which are typically underestimated in visual surveys. The unaccounted difference between the fence count and escapement estimates is largely considered surplus production. The surplus may account for a large proportion of Babine Sockeye returns each year. Dive surveys in the 1990s confirmed that these additional fish are not successful lake spawners missed by the stream surveys. 



@WoodLifeHist1995 developed a reconstruction procedure to estimate the surplus production after correcting visual escapement estimates for wild tributaries groups for underestimation bias, which were updated in @Woodetal1998Babine and @CoxRogersSpilsted2012Babine and maintained in a DFO database. @Woodetal1998Babine and @CoxRogersSpilsted2012Babine describe the rationale for these adjustments. The equations are provided in Appendix Table 2 of @CoxRogersSpilsted2012Babine and the calculations are summarized in Appendix C.3 of @SkeenaNassSkDataRep. Briefly, the adjustments are calculated in the following steps:

* Jacks (sub-adult males, age 3) are assumed to contribute very little to the spawning population, so jack counts at Babine fence are excluded from the estimates. However, jack Sockeye counts from the Babine weir are incorporated into estimates of the total Skeena aggregate return and estimates of age composition.
* Spawner counts for the wild tributaries are combined by timing group (early, mid, and late) into unadjusted counts for the 3 wild stocks (early, mid and late).
* Effective spawners for the enhanced systems are estimated as the sum of fish that passed through the Pinkut and Fulton weirs, and the estimated capacity of natural spawning grounds below the channel (5,000 for Pinkut, 40,000 for Fulton).
* The combined estimates for wild tributaries are expanded to account for the underestimation bias of visual counts. [@Woodetal1998Babine], and the adjusted estimate is apportioned by wild timing group  based on their relative abundance in the visual surveys.
* The Babine enhanced surplus is calculated as the difference between the Babine weir count, adjusted wild spawners, effective enhanced spawners, and harvest above the Babine weir. These additional adults, which do not spawn are considered a biological surplus which contribute nutrients to Babine Lake but are excluded from the estimates of spawner abundance. They are, however, included in estimates of run size.

### Catch Estimates


Catch estimates are derived from numerous marine and in-river catch monitoring programs that record the number of fish harvested in the different fisheries, some of which are sampled for age and stock composition, using variation in scale patterns or genetic allele frequencies.

Estimates of catch in Canadian and Alaskan fisheries, exploitation rates, and total returns for the Skeena and Nass aggregate population have been estimated by the Pacific Salmon Commission Northern Boundary Technical Committee using the Northern Boundary Sockeye Run Reconstruction model since 1982. 

First Nations harvests, which are aggregated by fishing area, are incorporated into in-river models that estimate the total exploitation rate for aggregate and for component stocks. Indigenous groups harvest Sockeye throughout the Skeena and Nass watersheds. Their fisheries differ by area, timing, and gear type, and have different management and catch reporting requirements. TWG members worked with DFO fishery managers and Skeena and Nass First Nations groups to update catch estimates for each fishing area.


### Age Composition Estimates


Age composition estimates, which are used to estimate recruitment by brood year, are available from scale and otolith sampling programs. Annual age composition estimates are available for both aggregates, but not for most of the individual Skeena and Nass stocks, which have been infrequently sampled.

#### Aggregate age composition data

Annual estimates of age composition for aggregate Skeena and Nass Sockeye stocks come from aggregate test fishery programs including Tyee Test Fishery (Skeena, 1955–present), the Nisga’a Fish Wheels (1992-present) and the Monkley Dump Test Fishery (Nass, prior to 1992), and from Canada and U.S. marine commercial fisheries (Skeena and Nass, until the late 1990s). Scale samples from commercial and test fisheries have been aged by Alaska Department of Fish and Game since 2000, and by Fisheries and Oceans Canada (for Canadian fisheries) in years prior. 

Adjustments are made to account for size selective fisheries at the test fisheries. For the Skeena, age samples are collected from large Sockeye at the Tyee Test Fishery to determine the proportion of the major age classes, which are applied to the total escapement of large fish to apportion the return of large fish into the major age classes. The return of Age–3 Sockeye from terminal fence counts are added to the total return, and the annual return for each age class are recalculated to estimate the proportions of all age classes in the total return. For the Nass aggregate, annual estimates for jack Sockeye are developed  by expanding the total catch of jacks at the fishwheels using the annual adult mark rate that is adjusted to account for the assumption of a higher mark rate for the smaller fish. 

#### Stock-specific age composition data

Annual age composition data are not available for most component Skeena and Nass Sockeye stocks, except for Meziadin Sockeye. Most of these samples with some exceptions (below) were aged at the DFO scale ageing lab at Pacific Biological Station, and the data are stored in a regional database (PADS) in digitized records of individual age readings (1989-2019) or as scanned scale/otolith age cards (prior to 1989). 

We  reviewed and updated available age data for all Skeena and Nass Sockeye stocks to ensure that all available data were incorporated into SR analyses. All available age records for Skeena and Nass Sockeye stocks were downloaded from PADS. For years prior to 1989, the number of fish from each age class were tallied from scanned age cards for each stock/year for which data were available. Age proportions for each stream/year were calculated as the number of each age class divided by the total number of samples for that year, excluding partial ages or unreadable samples. 

Given the available data, recruitment calculations for most stocks are currently based on an average age composition (Table \@ref(tab:Age2Stock)). Stock-specific age composition estimates were used for most Nass stocks, but for most Skeena stocks, including the five Babine stocks, we relied on average aggregate Skeena age composition. Annual estimates of age composition were used for Lower Nass Sea/River types (Table 3).

Using average age composition can introduce bias in spawner recruitment parameters [@Zabelagecomp]. In addition to our review of age data to ensure that all available age data were incorporated into spawner recruit analyses, we conducted  sensitivity tests to assess the effect of using average rather than annual age composition were conducted as part of the data review [@SkeenaNassSkDataRep].  For stocks where annual age data were available, including Meziadin, Kwinageese, and the Babine stocks, we recalculated recruitment estimates both annual and average age composition, and estimated the differences in the resulting spawner-recruitment parameters. For these stocks, the difference in Smsy that resulted from using average compared with annual age composition data varied, ranging from -2 to 31%.  

 

\clearpage
(ref:Age2Stock) Stock-specific age composition estimates used in the recruitment calculations. Tables \@ref(tab:StockOverview) and \@ref(tab:DataOverviewSkeena) show the full stock names and list the number of brood years with spawner-recruit data, based on the matched age compositions from this table.

```{r Age2Stock, echo = FALSE, results = "asis"}


ages.match <- read.csv("data/Reference Tables/Match_Age2Stock.csv",stringsAsFactors = FALSE) %>%
                  left_join(stock.info %>% select(Stock, LHAZShort,LHAZSeq, Watershed, StkNmS), by= "Stock") %>%
                     arrange(LHAZSeq, Watershed, StkNmS) 


ages.match$LHAZShort[duplicated(ages.match$LHAZShort)] <- ""
ages.match$Watershed[duplicated(ages.match$Watershed)] <- ""
avg.idx <- grepl("Avg",ages.match$AgeComp)
ages.match$Type <- NA
ages.match$Type[avg.idx] <- "Avg"
ages.match$Type[!avg.idx] <- "Annual"

ages.match$AgeComp <- gsub("Avg","",gsub("Annual","",ages.match$AgeComp))
ages.match$AgeComp <- gsub("Babine","Skeena",ages.match$AgeComp)

ages.match <- ages.match %>% select(LHAZShort, Watershed, StkNmS, Type, AgeComp) %>%
               dplyr::rename(Stock= StkNmS, LHAZ = LHAZShort)

ages.match %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = "l",
                  caption = "(ref:Age2Stock)") %>%
   kableExtra::row_spec(c(1,6,7,13,22,30), hline_after = TRUE) %>%
   kableExtra::row_spec(c(3:5,9:12,18,20,21,23:27), extra_latex_after = "\\cmidrule(l){2-5}") 

```


 
 
 
\clearpage
### Lake Surveys

Rearing lakes for Skeena and Nass Sockeye are assessed regularly or periodically. Surveys include: 	

* *Juvenile surveys*: Rotating juvenile surveys with hydroacoustic transects and biological sampling to assess fry abundance and biomass in Skeena and Nass Sockeye rearing lakes. Except for Babine Lake, all the major lakes have been surveyed multiple times since the late 1990s. Where available, fry abundance data provides a cross-check for reconstructed spawner escapement estimates derived from visual surveys (i.e., whether the estimated spawner abundance is realistic compared with the observed fry abundance for a given brood year).
* *Productivity assessments*: Periodic assessments of lake productivity have been conducted for most Skeena and Nass Sockeye rearing lakes using photosynthetic rates to estimate freshwater rearing capacity for each lake (PR Capacity estimates). PR capacity estimates provide useful information about the limits of freshwater rearing capacity that can be used directly to estimate optimal spawner abundances for a given system, or incorporated in spawner recruitment modeling as priors on rearing capacity. Note that most PR-Capacity estimates for Skeena and Nass Sockeye rearing lakes have not been updated since the mid-2000s.


### Run Reconstructions {#RunReconEst}



Long-established and well-documented methods are used to combine catch and escapement data for Skeena and Nass Sockeye information into consistent estimates of spawner abundance, run size and exploitation rates for the two stock aggregates and for most of the component stocks. Skeena Sockeye production data, maintained by DFO-North Coast Stock Assessment Division, incorporate catch and escapement information from different sources. Total catches, exploitation rates and run sizes for Skeena and Nass Sockeye have been estimated annually since 1982 by the bilateral Northern Boundary Technical Committee using the Northern Boundary Sockeye Run Reconstruction Model (NBSRR) (Gazey 2000, English 2004). Further details are provided in Appendix C of @SkeenaNassSkDataRep. Stock-specific exploitation rates for component Skeena and Nass Sockeye stocks, including Babine Sockeye are derived from in-river models [i.e., @Englishetal2017SSIRR] which combine data from in-river harvests and stock-specific run timing with NBSRR outputs to generate annual estimates of total returns for each stock, as summarized in Appendix C of @SkeenaNassSkDataRep.  


Methods and assumptions for the run reconstruction models are documented in a series of technical reports [e.g., @Englishetal2006CSAFW; @Englishetal2012CUInd; @Englishetal2013StAD; @Englishetal2004NBRR; @Englishetal2019NCCReview]. 


The key analytical steps for the Skeena and Nass Sockeye run reconstructions are:

* Expansion of spawner estimates to estimate the total number of spawners based on the number observed in the surveys. These expansions account for fish that were not counted (depends on survey method and annual implementation), as well as fish from systems that were not surveyed.
* For Babine stocks, additional spawner calculations are performed to account for (1) the difference between aggregate counts at the Babine fence and spawning ground estimates, and (2) effective capacity of the channels and natural spawning habitat. Effective spawner abundance for the enhanced systems is the number of spawners let into the enhanced channels and stream sections plus any gross escapement *up to* the estimated capacity of natural spawning grounds downstream of the enhancement facilities. Additional adult returns that do not spawn in the wild Babine tributaries are considered a biological surplus, and are excluded from the estimates of spawner abundance, but are included in estimates of run size.
* Run reconstructions for the two stock aggregates, which account for Canadian and U.S. marine catches in approach waters [@GazeyEnglish2000NBRR; @Englishetal2004NBRR; @NBRRDeck2018]. Aggregate run reconstructions are bilaterally developed each year through the PSC Northern Boundary Technical Committee (NBTC) process.
* Run reconstructions for the component stocks in each basin, which account for in-river catches and stock-specific run timing 


In-river harvests represent an important component of the total Canadian harvest for Skeena and Nass Sockeye. Estimates of in-river harvest, exploitation rates, and total run size for the different stocks are calculated using different approaches for Skeena Sockeye and Nass Sockeye. 

For Skeena River fisheries, the *Skeena Sockeye In-river Run Reconstruction* (SSIRR) model [@Englishetal2013StAD; @Englishetal2017SSIRR] combines information from in-river harvests (timing and abundance), escapement and run timing to apportion catches to stocks based on run timing and fishery location. The SSIRR model uses the same approach as the peer-reviewed run reconstruction model for Fraser River Chinook [@English2007FrSkRR]. The SSIRR model builds run size estimates forwards along the upriver migration through the fisheries, starting from the Tyee test fishery estimates. It models in-river harvests for 12 Aboriginal in-river fishing areas throughout the Skeena watershed in daily time steps derived from daily aggregate run size estimates for Skeena Sockeye from the Tyee Test fishery in the Lower Skeena. 

For Nass Sockeye, annual in-river harvest rates for the aggregate Nass stock (i.e., total in-river harvest divided by the run size entering the Nass River) were applied equally across all Nass Sockeye sub-stocks. This approach was considered appropriate for Nass Sockeye stocks because the vast majority of in-river harvest occurs in the lower Nass where all stocks are vulnerable. In-river harvest rates are combined with marine exploitation rates from the NBSRR to estimate the total exploitation rate for the different substocks. 

In-river run reconstructions are done at the stock level. In some cases, stocks are modelled as a group, because current methods in genetic stock  identification cannot differentiate the component stocks to estimate individual run timing curves. The model therefore assumes equal run timing for the components. At present, the SSIRR models 20 Skeena Sockeye stocks and the annual in-river harvest rates for the aggregrate Nass Sockeye stock are applied to each of 10 Nass sub-stocks.  The same number of stocks were used by @Englishetal2019NCCReview, with some changes to stock groupings (e.g., Brown Bear/Cranberry, Gingit/Zolzap, and the addition of Strohn Creek). 

The NCCDSB was updated in 2021 to incorporate reviews of spawner estimates for indicator systems, age composition data for the aggregates and individual stocks and timing assumptions for Skeena and Nass substocks, using updated GSI data collected from 2000-2020 at the Tyee Test Fishery and Nass fish wheel programs, marine and in-river harvests for Skeena First Nations; and additional years of data to the 2019 return year.


### Recruitment Estimates

We applied age composition estimates and calculated brood-year recruitment for each stock (aggregate and component stocks). We used recruitment estimates based on the major age classes (i.e., ages that have contributed more than 2% of the run at least once). 

Spawner recruit data for the wild component of the Skeena aggregate (Skeena Wild aggregate data) were derived by subtracting the calculated spawners and run sizes for Pinkut and Fulton Sockeye from the aggregate estimates, then using the annual aggregate age composition to recalculate recruits.

Updates to the aggregate and stock-level recruit estimates were the result of updates to the run reconstructions and age composition estimates described above.


### Data Checks and Sensitivity Tests

Data quality metrics were calculated for each stock looking at the entire time series and at individual observations [@SkeenaNassSkDataRep]. Potential data concerns were identified if metric values fell below, above, or outside the range of user-specified trigger values, depending on the metric. Trigger values were selected based on published guidance where available, or based on TWG consensus. 

The following metrics were used to conduct a systematic review of these considerations for the 31 Skeena and Nass Sockeye stocks:

* *Contrast*: low contrast in spawner data is flagged if $Max(Spn)/Min(Spn) < 4$, using the threshold from @Clarketal2014PercBM.  
* *Number of observations*: insufficient data for fitting SR models is flagged if the number of brood years with estimates of both spawners and recruits is less than 10. This trigger value was selected based on general experience with other Sockeye stocks. This is intended to identify stocks with "little" data using a consistent definition.
* *Large/small estimates not in model fit*: large estimates of spawners or recruits outside of the SR data set are flagged if the largest observed value is more than double the largest value for the brood years with estimates of both spawners and recruits. Small estimates of spawners or recruits outside of the SR data set are flagged if the smallest observed value is less than half the largest value for the brood years with estimates of both spawners and recruits. These trigger values were selected to identify extreme values.
* *Large expansion factor*: the expansion from index spawners to total spawner estimate were flagged if the average expansion for the whole time series is larger than 3 (i.e., observations are multiplied by more than 3).

Qualitative commentaries  were compiled to describe spawner data, catch data, age composition data, recruitment estimates, and lake survey data, and included the following considerations for each of these categories:

* *Indicator quality*: commentary on quality of spawner surveys (i.e., sum of estimates from indicator streams), based on survey types and coverage. Weirs and fishways were generally categorized as highly accurate, but if they capture multiple stocks then quality of stock composition estimates and relative abundance of the component stocks was also considered. 
* *Expansions*: categorizes the total expansion factor applied to the estimate from indicator streams into 4 categories. Expansion factors were taken from the previously published run reconstruction estimates [e.g., @Englishetal2019NCCReview].
* *Total spawner estimate quality*: Commentary on overall quality of the spawner estimate, considering the quality of the index estimate and the expansion factor.
* *Overall rating for spawner estimate*: The quality of spawner estimates was assessed on a 5-point scale from Very Good to Very Poor, based on the commentary for *TotalSpn*.


Quality of catch estimates by stock: 


* *Marine*: commentary on whether the marine migration route for the stock is likely similar to the aggregate migration route used in the NBSRR model, considering life history (e.g., sea type vs. lake type) and stock size (i.e., the model better captures they dynamics of major stocks: Meziadin on Nass and Babine stocks on the Skeena); this in turn affects whether the proportion of aggregate marine catch in the major fisheries for this stock is likely similar to the stock composition in the lower river assessment project (i.e., Nass fishwheels, Tyee test fishery), considering migration behaviour and stock size.
* *In-river*: commentary covering 2 considerations: (1) the quality of run timing and migration speed estimates; and (2) quality of catch estimates in different modelled river sections.
* *Total catch estimate quality*: commentary on overall quality of the total catch estimates, considering the quality of the above components 
* *Overall rating for catch estimate*: The quality of catch estimates was assessed on a 5-point scale from Very Good to Very Poor, based on the commentary for *TotalCt*.


Quality of recruitment estimates by stock: 

* *Run rating*: Describes the quality of run size estimates on a 5-point scale from Very Good to Very Poor, based on the commentary ratings for expanded spawner estimates and total catch estimates, and the relative magnitude of catch and spawner abundance (e.g., very poor catch estimate has little effect on quality of run size estimate if catches are very small).
* *Age structure*: categorizes the age structure of the stock as either *stable* (very little change from one year to the next), *variable* (some change on relative proportions, but consistent dominant age class), or *highly variable* (dominant age class varies). 
* *Age data*: categorizes the age composition estimates as either *Annual* (estimates available for most years), *Infill* (estimates available for many years, and remaining years infilled with average), or *Average* (a few years of data, using average for all years).
* *Total recruitment estimate quality*: commentary on overall quality of the recruitment estimates, considering the quality of the estimates for total run size and age composition.
* *Overall rating for recruitment estimate*: The quality of recruitment estimates was assessed on a 5-point scale from Very Good to Very Poor, based on the commentary for *TotalRec*.

Sensitivity tests were used to assess whether the potential data issues identified in the previous section were likely to affect estimates of standard biological benchmarks (e.g., $S_{MSY}$, $S_{MAX}$), in order to assist model scoping and  to identify which priority areas of uncertainty that will need to be considered in subsequent analyses [@SkeenaNassSkDataRep]. 

We performed three sets of sensitivity tests: data variations, uncertainty in the data (bootstrap), and uncertainty in the model fit (Bayesian estimates). Sensitivity tests were implemented with the *RapidRicker* package [@RapidRicker], which iterates through a comprehensive set of data variations to calculate standard biological benchmarks. Model fits and benchmark calculations are generated using both a simple linear regression fit and a Bayesian fit using JAGS code adapted from @MillerPestalTakuSk. 

The results from these initial sensitivity tests of SR model fitting, documented in the data report [@SkeenaNassSkDataRep],  were used to identify analytical priorities for this Research Document (Section \@ref(EGProcess)).

### Available Spawner-Recruit Data {#AvailableSRData}


Complete time series of spawners and recruits are available for the Nass aggregate since 1982 and for the Skeena aggregate since 1970. Stock-level run reconstructions for the Skeena are available back to 1960.

The 31 stocks were organized into three groups based on relative abundance and available data. Group 1 includes 14 larger stocks with long time series of spawner-recruit data which account for about 98% of the combined total returns for Nass and Skeena Sockeye, Group 2 includes 9 smaller stocks with some spawner-recruit data which together account for about 2% of combined total returns, and Group 3 includes 8 stocks without any spawner-recruit data.

Nass and Skeena Sockeye were organized into 31 stocks (Table \@ref(tab:StockOverview)), including 7 Nass and 24 Skeena stocks [@SkeenaNassSkDataRep]. Stocks can be grouped together based on life history  and adaptive zone (*LHAZ*), as well as watershed. Life history variations include lake type (*LT*), river-type (*RT*), and sea-type (*ST*). Exploitation rates are estimated for indicator systems (stocks or stock groups with similar run timing and reliable estimates of catch and spawner abundance). Under the Wild Salmon Policy [@WSP], Canadian anadromous salmon are grouped into distinct conservation units (CU). For Nass and Skeena Sockeye, most of the stocks match 
up with a single CU. Some of the smaller stocks combine 2-3 CUs, either because they rear in cojoined lakes, or they are assessed together and data can't be separated. Babine/Nilkitkwa, the largest Skeena CU, was divided into 5 distinct stocks based on enhancement and run timing.

The length and quality of spawner-recruit time series vary across stocks (Table \@ref(tab:DataOverviewSkeena), Figure \@ref(fig:SRDataOverview)). Larger stocks generally have more years of higher quality data. The TWG developed a consistent quality rating for spawner, run size, and recruitment estimates based on the types of data and calculations @SkeenaNassSkDataRep. Spawner estimate ratings incorporate the quality of the index survey as well as the expansion factor (e.g., a fence-based census of all spawners is rated as *very good*, but an aerial survey covering less than 1/5th of the stock is rated as poor).  Run size estimates ratings consider whether a stock is well represented in the catch accounting and run reconstruction analyses (e.g., a major stock with reliable stock identification and resulting timing estimates is rated as *good*, but a small stock uncertain timing is rated as *poor*). Recruitment estimate ratings combine quality of the run size estimate with considerations of the quality and amount of age composition data (e.g., stock-specific annual age data vs. average age composition from a proxy stock).

We filtered out implausible spawner-recruit observations and infilled gaps to allow fitting model forms that require complete time series. Specifically, we excluded brood years where estimated recruits/spawner exceeded 45, infilled 1-yr gaps in spawner estimates using the average of previous and subsequent estimates, infilled the corresponding run size using the year-specific exploitation rate estimate from the run reconstruction models, and then used the infilled data in the recruit calculation. Filtering and infilling procedures were only applied to some stocks (Table \@ref(tab:DataOverviewSkeena), Figure \@ref(fig:SRDataOverview)). We tested the effect of these data treatment steps using the basic Ricker model: Removing outliers through filtering generally had a larger effect on the parameter estimates than the infilling step (Appendix \@ref(AltSRTest)).

New information has recently become available for some stocks, which has not been incorporated into the current version of the analyses, but we consider these a high priority for updating in future work:

* *Bowser*: Bowser Lake was likely a major contributor to the Nass aggregate Sockeye return in some years. Visual escapement estimates, which are confounded by high glacial turbidity, have not been regularly conducted for Bowser Lake Sockeye, which are primarily a lakeshore spawning population. Previous abundance estimates for Bowser Sockeye have been derived using different methods including stock identification using scale pattern analyses, and more recently, GSI applied to Nass aggregate escapements. The different methods have produced divergent estimates for some years, and further assessment is required to reconcile these estimates before spawner recruit time series can be developed. 
* *Bear/Azuklotz*: Preliminary results from a new assessment program (video weir installed in 2021 on Bear River downstream of Bear Lake) suggest that the combined visual spawner escapements based on aerial surveys may underestimate the actual spawning population by a much larger factor than what has been accounted for in the expansion factors that are currently used in run reconstruction procedures. For our analyses, we used the existing time series of reconstructed abundances that do not account for new information from the camera weir program, with the understanding that these data may change in the near future.
* *Skeena river-type*: This stock is currently considered data deficient, because there is not enough information about spawning abundance or distribution of Skeena river-type Sockeye to estimate total watershed abundance for these populations. While there are  small persistent river-type spawning populations that are enumerated annually in the Kispiox watershed and Bulkley River, it is not known whether these populations account for most or only a small portion of river-type spawners in the Skeena watershed. Anecdotal information from historic and recent surveys suggest that persistent or ephemeral populations are also present in Upper Skeena tributaries. The population structure of river-type spawners in the Skeena watershed is unclear, with few samples in the genetic baseline and poor differentiation between some Skeena and Nass river-type populations. It is not known whether Skeena river-types represent one or multiple populations, or a single population for Skeena and Upper Nass river-types. 





\clearpage
(ref:StockOverview) Nass and Skeena Sockeye population structure. The 31 stocks fall into 7 distinct groups based on life history type and freshwater adaptive zone (LHAZ) and 21 watersheds. We use short stock labels (Stock) for tables and figures throughout the Research Document. Exploitation rate indicators (ERInd) are available for most of the stocks. Stocks match up with one or more conservation units (CU). Babine is currently designated as a single CU, but assessed and analyzed as five distinct stocks (marked with *).

```{r StockOverview, echo = FALSE, results = "asis"}

n.nass <- sum(stock.info$Basin == "Nass")
n.skeena <- sum(stock.info$Basin == "Skeena")

table.in <- stock.info %>% #dplyr::filter(Basin == "Nass") %>%
   select(LHAZShort,Watershed,Stock,StkNmS,ERInd,NumCU) %>%
   dplyr::rename(LHAZ = LHAZShort, Label = StkNmS,CU= NumCU)

table.in[table.in$Watershed == "Babine","CU"] <- "*"


table.in$LHAZ[duplicated(table.in$LHAZ)] <- ""
table.in$Watershed[duplicated(table.in$Watershed)] <- ""
   
table.in %>%   
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l","l","l","r"),
                  caption = "(ref:StockOverview)") %>%
   kableExtra::row_spec(rep(n.nass,2), hline_after = TRUE) %>%
      kableExtra::row_spec(c(1,6,13,22,30), hline_after = TRUE) %>%
    kableExtra::row_spec(c(2,4,5,9:12,14,16,17,23:25,28,29), extra_latex_after = "\\cmidrule(l){2-6}") 

```





\clearpage

(ref:DataOverviewSkeena) Summary of available spawner-recruit data by stock. Stocks are sorted based on relative size (pSpn), calculated as the the percent of cumulative spawner abundance since 2000 across both stock aggregates. Quality ratings for spawner (Spn), run (Run), and recruit (Rec) estimates on a 5-point scale from *very poor* (VP) to *very good* (VG) are based on TWG consensus using a set of qualitative criteria [@SkeenaNassSkDataRep].  nS is the number of spawner estimates. The number of brood years with spawner-recruit data varies based on data treatment for some stocks: Orig = original data set from @SkeenaNassSkDataRep, Filter  = number of brood years filtered due to R/S > 45, Infill = number of years for which a 1-yr gap in estimates of spawners and run size could be infilled, Use = number of brood years with spawner and recruit estimates after filtering and infilling.

```{r DataOverviewSkeena, echo = FALSE, results = "asis"}



table.in <- stock.info %>% arrange(desc(PercEffSpn)) %>%
   left_join(data.notes.tab %>%  select(Stock,SpnRating,RunRating,RecRating), by= "Stock") %>%
   left_join(alt.sr.data %>% dplyr::filter(Label == "Main") %>% group_by(Stock) %>%
   summarize(Orig = sum(!is.na(RpS))), by = "Stock") %>%
      left_join(alt.sr.data %>% dplyr::filter(Label == "Filter45_Infill") %>% group_by(Stock) %>%
   summarize(Use = sum(!is.na(RpS)),NumFiltered = sum(Filter), NumInfilled = sum(Infill)), by = "Stock") %>%
   # left_join(table.in <- alt.sr.test1 %>% dplyr::filter(DataVersion == "Main") %>% mutate_all(as.character) %>%
    #         dplyr::rename(nSRo = NumBrYr) %>% select(Stock, nSRo), by = "Stock") %>%
   #left_join(table.in <- alt.sr.test1 %>% dplyr::filter(DataVersion == "Filter45_Infill") %>% mutate_all(as.character) %>%
   #          dplyr::rename(nSR = NumBrYr) %>% select(Stock, NumFiltered,NumInfilled,nSR), by = "Stock") %>%
   select(LHAZShort,StkNmS,PercSpnLabel, NumSpn, SpnRating,RunRating,RecRating,
          Orig, NumFiltered,NumInfilled, Use) %>%
   dplyr::rename(LHAZ = LHAZShort, Stock = StkNmS,pSpn =  PercSpnLabel, nS = NumSpn, 
                 Spn = SpnRating,Run = RunRating,Rec =RecRating,Filter = NumFiltered,
                 Infill =NumInfilled)

table.in[table.in == "Data Deficient"] <- "DD"
table.in[table.in == "Good"] <- "G"
table.in[table.in == "Moderate"] <- "M"
table.in[table.in == "Poor"] <- "P"
table.in[table.in == "Very Good"] <- "VG"
table.in[table.in == "Good to V. Gd"] <- "G to VG"
table.in[table.in == "Very poor"] <- "VP"


table.in[is.na(table.in)] <- "-"
table.in[table.in == "NA"] <- "-"
 
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","r","r","l","l","l","r","r","r","r"),
                  caption = "(ref:DataOverviewSkeena)") %>%
   add_header_above(c(" " = 4, "Quality" = 3, "SR Data" = 4),bold=TRUE) #%>% 
    # kableExtra::row_spec(c(6,15,23), hline_after = TRUE) %>%
     #kableExtra::column_spec(3, width = "10em") %>%
        #kableExtra::row_spec(c(1:5,7:14,16:22), extra_latex_after = "\\cmidrule(l){2-8}") 

```







\clearpage
(ref:SRDataOverview) Spawner-recruit data availability - By stock. Timeline of available data by brood year, with stocks grouped based on life history and adaptive zone. Dark blue circles are brood years with both spawner and recruit estimates. Light blue points are brood years with only spawner estimates. Light red diamonds mark brood years where a 1yr gap in spawner estimates was infilled. Dark red diamonds mark infilled brood years where a corresponding recruit estimate could be calculated. Red "x" mark filtered observations (R/S > 45) that could not be infilled. Numbers in brackets are the share of cumulative spawner abundance since 2000 across both stock aggregates.

```{r SRDataOverview,  out.width= 415, fig.cap="(ref:SRDataOverview)"}
include_graphics("data/DataOverview_ResDoc_Filter45Infill.png")
```




\clearpage
## SPAWNER-RECRUIT MODEL FITTING {#SRFitting}

### Ricker Model Forms {#ModelForms}

Sockeye population dynamics are commonly modelled with the Ricker curve, which assumes that productivity, expressed as ln(Rec/Spn), decreases as the abundance of spawners increases, resulting in a dome-shaped relationship between spawners and total recruits. Key milestones in previous work on Skeena and Nass Sockeye (Table \@ref(tab:PastWorkTable)) all used the Ricker curve, but implementation details varied widely across these analyses:

* @BockingetalMeziadinBM used a deterministic Ricker model for a single stock (Meziadin, the largest Nass stock). 
* @Waltersetal2008ISRP and @Hawkshaw2018Diss developed state-space Ricker models for 9 Skeena CUs.
* @Hawkshaw2018Diss also modeled Skeena Sockeye as a single stock with Ricker dynamics in a multi-species simulation.
* @CoxRogersetal2010 used nursery lake capacity estimates based on photosynthetic rate to develop Ricker parameters for 28 nursery lakes, with Babine treated as a single stock. 
* @KormanEnglish2013 used hierarchical Bayesian Ricker fits for 17 Skeena conservation units, splitting Babine into one enhanced and three wild CUs, and using PR-based lake rearing capacity as priors for Smax for all CUs except for Babine-Nilkitkwa. 
* @PacificSalmonExplorer updated the @KormanEnglish2013 analysis, but modified the Babine split to one enhanced and two wild CUs, and added four Nass Sockeye CUs.
* @Atlasetal2021HabitatDyn used Hierarchical Bayesian Ricker model fit to SR data for 54 North Coast Sockeye CUs (1 model for each biogeoclimactic zone).


@Fleischmanetal2013CJFASStateSpace observed that "*The Ricker model is, by far, the most common choice for Pacific
salmon SR analyses, probably because (i) it can accommodate overcompensation, which is evident in many Pacific salmon data
sets, and (ii) it is conservative with respect to optimal escapement levels (for fixed values of the productivity parameter and carrying capacity, SMSY is always higher under the assumption of a Ricker model than under a Beverton–Holt model).*"

We used three alternative model forms for the Ricker SR relationship to assess the sensitivity of biological benchmark estimates to alternative assumptions about observed residuals.

*Basic Ricker (BR)*

A standard Bayesian Ricker fit, based on a fixed linear relationship between $ln(R/S)$ (productivity) and $S$ (spawner abundance). The basic Ricker model, like any basic linear regression, assumes that residuals have a random normal distribution $N$ with mean 0 and sample-based variance, without any pattern in the deviations over time.  The basic Ricker fit serves as a good baseline, even if the observed pattern in productivity violates that assumption (i.e., What would benchmark estimates look like if changes over time were disregarded?). For brood year $i$

\begin{equation} 
  ln(R_{i}/S_{i}) = ln(\alpha) - \beta * S_{i} + \varepsilon_{i}
  (\#eq:BasicRicker)
\end{equation} 

\begin{equation} 
  \epsilon_i \sim N(0,\sigma^2)
  (\#eq:BasicRickerResid)
\end{equation} 



*Ricker with lag-1 autocorrelation (AR1)*

An extension to the Ricker model, which is again based on fixed linear relationship between $ln(R/S)$ (productivity) and $S$ (spawner abundance), but also looks for an underlying pattern in the residuals (i.e., estimate a specific residual for each brood year $i$) 

\begin{equation} 
  ln(R_{i}/S_{i}) = ln(\alpha) - \beta * S_{i} + \varepsilon_{i}
  (\#eq:AR1Ricker)
\end{equation} 


\begin{equation} 
  \varepsilon_i = \phi \varepsilon_{i-1} + \nu_i
  (\#eq:AR1RickerResid1)
\end{equation} 

\begin{equation} 
  \nu_i \sim N(0,\sigma_{\nu}^2)
  (\#eq:AR1RickerResid2)
\end{equation} 



*Ricker with Time Varying Productivity (TVP)* 

An extension to the Ricker model, which is based on a *time-varying* linear relationship between $ln(R/S)$ (productivity) and $S$ (spawner abundance) [e.g., @PetermanPyperGrout2000ParEst;  @PetermanPyperMacGregor2003KF; @HoltMichielsens2020RecBayes; @Freshwateretal2020Selectivity; @FraserSkRPASAR; @Huangetal2021FraserSkRPA]. For each brood year, the productivity parameter $ln(\alpha)$ is estimated based on the observed productivity for that year and the estimated $ln(\alpha)$ for the previous brood year, to generate a more or less smooth series of $\alpha$ parameters. 


\begin{equation} 
  ln(R_{i}/S_{i}) = ln(\alpha)_i - \beta * S + \varepsilon_{i}
  (\#eq:RecBayesRicker1)
\end{equation} 

\begin{equation} 
  ln(\alpha)_{i} = n(\alpha)_{i-1} + \omega_{i}
  (\#eq:RecBayesRicker2)
\end{equation} 


\begin{equation} 
  \omega_i \sim N(0,\sigma_{\omega}^2)
  (\#eq:RecBayesRicker3)
\end{equation} 

\begin{equation} 
  \epsilon_i \sim N(0,\sigma^2)
  (\#eq:RecBayesRicker4)
\end{equation} 

Two alternative approaches for fitting a time-varying productivity Ricker model have been applied for Pacific salmon data, (1) Kalman Filter [@PetermanPyperGrout2000ParEst;  @PetermanPyperMacGregor2003KF] and (2) Recursive Bayes [@HoltMichielsens2020RecBayes; @Freshwateretal2020Selectivity; @FraserSkRPASAR; @Huangetal2021FraserSkRPA]. While the mathematical structure of these models are the same (Eq. \@ref(eq:RecBayesRicker1),  \@ref(eq:RecBayesRicker4)), the computational implementation of the estimation step is very different. A key difference is that the Kalman Filter implementations included a smoothing step, whereas the Recursive Bayes implementations did not.  

The TVP model described here uses the Recursive Bayes version, consistent with recent work on Fraser Sockeye [@FraserSkRPASAR; @Huangetal2021FraserSkRPA].


*Summary*

The Basic Ricker and AR1 Ricker both fit a single spawner-recruit relationship which is assumed to describe inherent properties of the stock that remain constant over time. The AR1 model form has previously been used in escapement goal analyses for Alaskan and northern transboundary Sockeye stocks [e.g., @MillerPestalTakuSk; @Connorsetal2022]. The time-varying productivity version of the Ricker model assumes that there are real changes in productivity over time, and tries to extract a more-or-less smoothed pattern, identifying high and low productivity periods. The time-varying productivity model has been used for some northern transboundary salmon stocks [e.g., @PestalJohnstonTakuCo] and in several Fraser Sockeye applications [e.g., @FrSkWSPBM;@Huangetal2021FraserSkRPA; @PetermanDorner2011Fraser].


### Bayesian Parameter Estimation Using Markov Chain Monte Carlo (MCMC) 

We derived parameter estimates with Bayesian methods for candidate stock-level and aggregate-level SR models with Markov chain Monte Carlo (MCMC) using the JAGS sampling engine [@Plummer03jags] via the *jags()* function from the *R2jags* package [@R2jags]. Appendix \@ref(SingleStockSRFits)  describes the code set-up and  lists the JAGS code for the three model forms.

MCMC estimation combines prior assumptions about each parameter with the likelihood of different parameter values based on the data to generate a posterior sample of parameter values. Prior assumptions can be uninformative (e.g., productivity for the stocks can be anywhere from very high to very low, and we don't specify a preference) or informative (e.g., we think productivity of the stock is similar to the average productivity from several near-by stocks with similar life history).

The sampling engine starts with some random values sampled from the prior distribution, then searches through variations of all the parameters to identify values that plausibly link the observed data to the specified relationship (e.g., a Ricker function). Sampling should be set up such that parameter values stabilize (*convergence*), and earlier parts of the sampling chain are discarded (*burn-in*).

MCMC implementations require careful testing of prior assumptions and verification of sampling behaviour to assess the quality of resulting estimates. We compiled a checklist of MCMC diagnostics and used it to select a short-list of SR model fits for each stock (Sec. \@ref(ModelSelection)).


### Priors {#Priors}

Bayesian fits for all three model forms (Basic Ricker, AR1 Ricker, and time-varying productivity Ricker require prior distributions for the productivity parameter $ln(\alpha)$  and the capacity parameter $S_{max}$.

We used uninformative productivity priors for all single-stock SR model fits, implemented as a normal distribution with a mean of 0 and a very wide spread:

\begin{equation} 
  ln.alpha \sim normal(0,100)
  (\#eq:ProductivityPrior)
\end{equation} 

For the capacity prior, we tested uniform and lognormal prior distributions for $S_{max}$, with either wide (Scalar = 3) or capped (Scalar = 1.5) upper bounds:

\begin{equation} 
  Smax \sim uniform(0,Spn_{ref}*Scalar)
  (\#eq:UniformCapacityPrior)
\end{equation} 

\begin{equation} 
  Smax \sim lognormal(Spn_{ref},CV) [0,Spn_{ref} * Scalar]
  (\#eq:LognormalCapacityPrior)
\end{equation} 

Informative capacity priors based on the photosynthetic rate observed in rearing lakes can improve the precision of capacity estimates (i.e., narrower posterior distribution of Smax)  for stocks where they are appropriate given the life history, lake properties, number of stocks rearing in a lake, and plausibility of the PR-based Smax estimate [e.g., @Bodtkeretal2007; @Atlasetal2021HabitatDyn; @Atlasetal2020Limno]. We used lake-based capacity estimates as the initial stock-specific values for $Spn_{ref}$ where available and applicable, and used the largest observed spawner abundance as the reference value for the remaining stocks. 

\clearpage
@SkeenaNassSkDataRep compiled published and unpublished PR-based Smax estimates for 26 Sockeye rearing lakes in the Skeena and Nass basins (their Appendix B.4), and we used the estimates for 20 of the lakes to specify informative $Spn_{ref}$ values for 15 stocks (Table \@ref(tab:PRPriorsTable)), based on the following considerations:

* Informative capacity priors using the sum of PR-based Smax estimates for major rearing lakes are not applicable for aggregate-level model fits, given the mixture of life histories and lake properties across the component stocks.
* PR-based capacity priors are not applicable to river type or sea type Sockeye, which do not rear in a lake.
* We did not use PR-based capacity priors for Babine stocks, due to (1) the size of the lake and (2) the challenge of allocating lake capacity estimates among five stocks, including the two channel-enhanced stocks (Pinkut, Fulton).
* For stocks with multiple rearing lakes, we generally summed the available PR-based Smax estimates for the main spawning lakes (Bear / Azuklotz, Fred Wright / Kwinageese, Swan / Stephens / Club, Sustut / Johansen). For Mcdonell, we used only the Mcdonell lake capacity estimate, but excluded Aldrich and Dennis, because all spawners observed surveys rear in Mcdonell Lake. The Slamgeesh stock includes Slamgeesh and Damshilgwit lakes, but PR-based estimates are only available for Slamgeesh. 
* For some stocks with PR-based capacity priors there is insufficient data for fitting single-stock SR models (Bowser).
* Capacity priors for some stocks were adjusted based on a review of posterior distributions from preliminary model fits.

Overall, we tested four alternative capacity priors (Table \@ref(tab:AltCapPriorsTable)) and used the *capped uniform* prior as the base case for the model fits reported in this paper. 

Where PR-based capacity estimates were available, these were used to bound the SR model fit, but in a bounded uniform prior the lake-based estimate carries less weight than in a lognormal prior, unless the lognormal prior is used with a large CV, in which case it behaves almost like a uniform prior. We chose to downweight the lake-based information this way because (1) most of the available PR-based estimates are from 20 or more years ago, and (2) a consistent stock-by-stock review of limiting factors has not been completed for Skeena and Nass Sockeye.

The potential issues with using PR-based capacity estimates are illustrated by Kitwanga Sockeye: The PR-based estimate of  $S_{max}$ from 2003 is 36,984 (Table \@ref(tab:PRPriorsTable)), but median observed spawner abundance since 1960 is 1,258. The largest observed spawner abundance was 20,804 in 2010, and the second largest was 13,699 in 2014. All other observations have been below 6,000 spawners. There are several potential explanations for this discrepancy: Either the spawner-recruit data is biased low, or the capacity estimate is biased high, or the stock has been severely depleted since before 1960, or Kitwanga production is not lake-limited. In addition, lake conditions have likely changed in the 20 years since the estimate was generated. In an escapement goal review focusing on one or two stocks, these alternative hypotheses could be explored and weighed to determine whether the PR-based capacity estimate is valid. However, this was not feasible here, given the number of stocks covered in the current analysis.  



\clearpage

(ref:PRPriorsTable) PR-based Smax estimates used to specify informative capacity priors. Table lists the year of the last limnological survey (*LastLim*) used to derive the PR-based Smax (*Est*). 95% confidence intervals (*Lower*, *Upper*) were based on assumed 20% coefficient of variation and a normal distribution (Cox-Rogers and Hume, pers. comm., DFO, 2012). Estimates for Skeena lakes are from Cox-Rogers and Hume (pers. comm., DFO, 2012, from datasets maintained by Cultus Lake Salmon Research Laboratory), which include lake-specific adjustments for non-Sockeye competitors (e.g., stickleback) and juvenile competition. Estimates for Nass lakes are from @Atlasetal2020Limno,  which do not include adjustments. However, adjustments would likely be small for the Nass nursery lakes. Updates or sensitivity tests of the PR-based Smax capacity estimates developed in the 1990s and early 2000s (e.g., the 20% CV assumption) were outside the scope of the current project.

```{r PRPriorsTable, echo = FALSE, results = "asis"}


table.in <- read.csv("data/Reference Tables/PRbasedPrior_Info.csv",stringsAsFactors = FALSE, fileEncoding = "UTF-8-BOM") %>% arrange(Basin,Stock,-Smax_Spn) %>% select(-CV, - Cap) %>%
   mutate(Smax_Spn = prettyNum(round(Smax_Spn),big.mark = ",")) %>%
   mutate(Smax_Spn_Lower = prettyNum(round(Smax_Spn_Lower),big.mark = ",")) %>%
   mutate(Smax_Spn_Upper = prettyNum(round(Smax_Spn_Upper),big.mark = ",")) %>%
   #mutate(Cap = prettyNum(round(Cap),big.mark = ",")) %>%
   dplyr::rename(Est = Smax_Spn,Lower = Smax_Spn_Lower,Upper = Smax_Spn_Upper) #%>% 
   #select(-EstCV)
   

table.in$Basin[duplicated(table.in$Basin)] <- ""
table.in$Stock[duplicated(table.in$Stock)] <- ""
table.in[is.na(table.in)] <- ""
table.in[table.in == "NA"] <- ""
   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l","r","r","r","r","r"),
                  caption = "(ref:PRPriorsTable)") %>%
add_header_above(c(" " = 4, "PR-based Smax" = 3),bold=TRUE) %>%
        kableExtra::row_spec(c(5), hline_after = TRUE) %>%
       kableExtra::row_spec(c(1,4,6,9:17,20), extra_latex_after = "\\cmidrule(l){2-7}") 

```


\clearpage

(ref:AltCapPriorsTable) Alternative priors for the capacity parameter Smax. All four versions were tested with the Basic Ricker model fit, and the two versions of the uniform prior were tested with the AR1 and TVP Ricker model fits. The capped uniform prior (*CU*) was selected as the default for results presented in this paper.

```{r AltCapPriorsTable, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Reference Tables/AltCapPriors.csv")
   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l","r","r","r","r","r"),
                  caption = "(ref:AltCapPriorsTable)") %>%
        kableExtra::row_spec(2:dim(table.in)[1]-1, hline_after = TRUE) %>%
	kableExtra::column_spec(3, width = "31em") 

```


\clearpage
### Candidate Aggregate Spawner-Recruit Model Fits {#CandidateAggModels}


Key considerations for spawner-recruit modeling for the two aggregate stocks are time-varying productivity and the contribution of channel-enhanced stocks to Skeena Sockeye returns. Because long time series of continuous SR data are available for both the Skeena and Nass aggregates, all three candidate model forms (Section \@ref(ModelForms)) can be applied, which allows for an explicit evaluation of changes in productivity over time. The main challenge for aggregate-level SR fits is determining whether the analysis method is appropriate at that scale.

*Nass aggregate*

For most of the available time series, Meziadin accounts for most of the total spawner abundance. The aggregate data set has good contrast overall, but the early part of the time series accounts for most of the contrast in the data. Since the mid-1990s contrast has been much lower (<4), but this is partly due to changing stock composition, specifically the recent increase in the abundance and relative contribution of Lower Nass Sea and River Type Sockeye. Given their different life histories, we consider it more appropriate to fit SR models separately to these two main stocks, but included the aggregate model fits for comparison.


*Skeena aggregate*

Limited contrast in spawner data and noisy scatter of data points create large uncertainty in SR model fits, because a high proportion of the aggregate originates from the BLDP enhancement facilities. It is inappropriate to fit a density-dependent SR relationship to the resulting data, because the fit is highly sensitive to small changes in the data treatment choices (e.g., including or excluding a few earlier or recent brood years; Figure \@ref(fig:AltFitPlotSkeena)). Bayesian priors can used to force the model fit to a particular productivity or capacity considered plausible, but we have here chosen to exclude the enhanced stocks and fit SR models to the wild component of the aggregate (Section \@ref(DataSources)). An overview of available information on enhanced production is provided in Section \@ref(ChannelReview).


\clearpage
(ref:AltFitPlotSkeena) Simple deterministic Ricker fits for total Skeena aggregate, including enhanced Pinkut and Fulton, using all available brood years compared to various subsets of the data. The regression fit varies drastically and even reverses direction, depending on whether the 1994 and 2013 brood years are included in the analysis. Solid points are the data used for the model fit. Open circles are the excluded observations. Fits might be more stable if additional information can be incorporated, such as environmental covariates (e.g., ocean conditions during smolt outmigration) or covariation in productivity across river systems. However, this simple illustration shows that the SR data by themselves provide little information about a density-dependent relationship between spawners and productivity.  

```{r AltFitPlotSkeena,   fig.cap="(ref:AltFitPlotSkeena)"}
include_graphics("data/CaseSpecificExtraPlots/SkeenaScatterVariations.PNG")
```





\clearpage
### Candidate Single-Stock Spawner-Recruit Model Fits  {#CandidateStockModels}

Key considerations for SR modeling for each of the Skeena and Nass Sockeye stocks include stock characteristics, available data, and observed changes over time (e.g., data quality, productivity). We developed a checklist of considerations to identify an initial set of candidate SR model variations for each stock (Figure \@ref(fig:CandidateModels)). 

The minimum number of SR data points required for model fitting was discussed during the peer review process. If there are relatively few SR data points, then parameter estimates and resulting biological benchmarks can be highly uncertain and systematically biased, particularly where observation error is relatively high and there is strong year-to-year correlation in survival. A consensus was reached that the threshold should be at least 10, because it was participants' experience that SR model fits to fewer than 10 observations are vulnerable to severe biases in parameter estimates and resulting benchmarks. Participants also considered a higher threshold based on unpublished work by Brendan Connors (pers. comm., DFO, 2022, documented in a [github repository](https://github.com/brendanmichaelconnors/PSE-pop-SAC/blob/master/How-many-data-points/2020-07-30_How-many-SR-pairs-are-too_few.pdf)), who explored the amount of bias in estimates of Smsy for various numbers of data points included in the analysis and found that at least 13 years of stock-recruitment data pairs are needed, in general, to get unbiased Smsy estimates. Most concerning was that fewer than 13 points generally produced underestimates of Smsy, with the largest bias produced by the least productive populations. This bias in Smsy was generally smaller in an HBM analysis than in the single-CU analyses. 

We maintained the threshold of at least 10 data points as part of the checklist in Figure \@ref(fig:CandidateModels), but the higher threshold of at least 13 data points would not affect our analyses (Table \@ref(tab:DataOverviewSkeena)). Upper Skeena RT with 4 brood years of data are excluded regardless, and all the other stocks have more than 13 brood years of data. Slamgeesh and Johnston have 14 brood years through 2019 return data, and will have 16 as soon as the next update of the run reconstruction (up to the 2022 return year) is implemented. 


SR model fits were only applied to *wild* stocks with at least 10 brood years of SR data:

* For eight stocks with gaps in the data series (after filter and infill, Section \@ref(AvailableSRData)) only the Basic Ricker model was fitted. 
* For 12 stocks with at least 25 continuous brood years of SR data, all three model forms were fitted (Basic, AR1, TVP). 
* The two enhanced stocks (Pinkut, Fulton) were excluded because of fitting issues as illustrated in Figure \@ref(fig:AltFitPlotSkeena) for the Skeena aggregate. 
* The eight data-deficient stocks were also excluded. Note that for one of the stocks that are here considered to be data deficient, Bowser (Nass), there is on-going discussion regarding the usability of available estimates, and it may be included in future updates to this analysis. 

\clearpage

(ref:CandidateModels) Checklist for identifying a base set of candidate models for single-stock SR model fits. We focused on Ricker model variations for wild stocks with at least 10 brood years of spawner-recruit data after infilling any 1-year gaps in spawner abundance or run size. For stocks with at least 25 continuous brood years of spawner-recruit data we tested three alternative model forms. For stocks not meeting that requirement we fitted only a basic Ricker model.


```{r CandidateModels,  fig.cap="(ref:CandidateModels)"}
include_graphics("diagrams/Diagram_CandidateModels_REV.PNG")
```

\clearpage

We completed two sets of sensitivity tests for the Basic Ricker fit:

* *Full vs. truncated data*: Compare fits using all available data to fits with truncated data, excluding earlier brood years. The cut-off for the truncated data differed  by stock, but we generally used the mid- to late 1990s. For example, Alastair has SR data back to to 1960, but the truncated model fit uses only brood years starting in 1998. Note that for Kitsumkalum we used SR data truncated at 1990 as the base case, and all years of data in a sensitivity test, due to the observed drastic changes in production dynamics since a spawning channel was built in the late 1980s. Note that the Kitsumkalum channel differs from Pinkut and Fulton because spawner abundances are not actively managed to a target, and data since 1990 show a clear density-dependent pattern.
* *Alternative capacity priors*: Compare benchmark estimates using four alternative capacity priors: capped uniform, wide uniform, capped lognormal, wide lognormal (Section \@ref(Priors). Where  available and applicable we used capacity estimates based on lake photosynthetic rate to bound the capacity priors. 


### Exploration of Hierarchical Spawner-Recruit Model Fit for Skeena Sockeye Stocks  {#HBMExploration}

As part of the TWG process, McAllister and Challenger (Appendix \@ref(app:HBMFits)) updated the hierarchical Bayesian model (HBM) fitting approach for Skeena Sockeye stocks from @KormanEnglish2013 to provide a comparison of previous estimates generated using the same methodology but with an updated spawner-recruit data set. An advantage of hierarchical Bayesian models is that information can be shared between stocks, drawing on similarities in the available data to extract shared underlying patterns (e.g., similar intrinsic productivity across stocks with similar life history, common patterns in changing productivity), which may improve the precision of estimates for stocks with noisy or missing data.

Details of the HBM methods, model fits, and results are provided in Appendix \@ref(app:HBMFits). Briefly, the approach is to model stock-level productivity with two components: (1) a common underlying distribution with a shared central tendency across the stocks (called a *hyperparameter*), and (2) a stock-specific deviation from that shared distribution. For stocks with highly informative data, the resulting productivity estimates can shift further away from the common productivity parameter. For stocks with noisy or missing data, the parameter estimate will get pulled more strongly towards the overall centre of the distribution for the group of stocks. This *shrinkage* occurs for all stocks whose productivity parameters differ from the mean productivity of the group of stocks that was included in the HBM, but the level of shrinkage differs by stock depending on how strong the signal in the data is (Section \@ref(HBMShrinkage)).

Known challenges for hierarchical Bayesian fits include:

* *Model complexity*: Many parameters are being estimated simultaneously. Parameter estimates may be highly sensitive to alternative settings and unexpected interactions could skew the results. While this is the case for all Bayesian model fits, the potential issue grows with the number of parameters.
* *Assumed similarities between stocks*: In its simplest form, an HBM implementation estimates productivity for all component stocks relative to a single underlying hyperparameter, but more nuanced stock structures can be incorporated (e.g., group stocks to match the spatial structure of the basin). Given that information is exchanged between stocks, it is important to consider the life histories and observed productivity patterns of stocks linked together in a hierarchical model structure.


In addition to providing a comparison with estimates that were previously developed using a similar modeling framework for Skeena Sockeye, the updated HBM model results confirm the overall pattern of basin-wide declines in productivity for Skeena Sockeye in the shape of the shared year effect curve, and the HBM results reported in Appendix \@ref(app:HBMFits) also support other objectives for this Research Document, including:

* Contributing a fully independent cross-check of the single-stock parameter estimates for Skeena Sockeye stocks (Objective 3)
* Providing an opportunity to explore sources of observed differences (i.e., model form, prior assumptions) (Objective 6)

To support these objectives,  the HBM was implemented using  the same data sets and incorporated some sensitivity tests designed to be similar to the single-stock implementation. The intent was that observed differences in results should be mostly due to the hierarchical structure, but it was challenging to clearly isolate the effect of the hierarchical assumption from other methodological nuances for the stocks where differences between single stock and HBM model outputs were observed.

## SINGLE-STOCK SR MODEL SELECTION AND PRODUCTIVITY SCENARIOS {#ModelSelection}

We fit a total number of 163 candidate model fits, due to the alternative model forms (Basic, AR1, TVP), sensitivity testing (i.e., alternative priors, full vs. truncated time series), and large number of stocks (20 wild, 2 enhanced, 3 versions of aggregate fit). To improve consistency, we developed guidelines for first selecting a short list of model fits for each stock or aggregate and then developing alternative productivity scenarios based on the short-listed model fits (Figure \@ref(fig:ModelSelection)).

Given that *"all models are wrong but some are useful"* [@BoxModelsWrong], the approach for short-listing model fits needs to be adapted to their purpose. For example, in applied SR analyses for the same stocks of Fraser River Sockeye, using the same data, the annual forecasting process [e.g., @Grantetal2013FC] uses a different set of candidate models and a different model selection approach than the simulation used for a recovery potential assessment [@Huangetal2021FraserSkRPA]. Both approaches combine quantitative criteria for model selection (e.g., MCMC convergence, mean absolute percent error from a retrospective test) with expert judgment regarding the plausibility and usefulness of the candidate SR model fits.

For the Skeena and Nass Sockeye escapement goal review, the TWG and independent reviewers identified changes in productivity over time, and differences in productivity between stocks, as the main analytical priorities (Section \@ref(PaperObj)). Accordingly, we framed the fundamental question for model selection as *"Of the SR model fits that converged on biologically plausible parameter estimates, which ones are useful for describing alternative productivity scenarios that are relevant to subsequent decision processes"*, where we define "useful" as  *"helping to demonstrate the magnitude of changes in  biological benchmarks and subsequent analyses resulting from different productivity assumptions"*. This emphasizes the contrast between productivity scenarios, and is a very different approach from looking for the single model with the "best" fit. These productivity scenarios are not predictions or recommendations for the best model fit per se. Upcoming decision processes will need to identify scenarios they consider plausible, and then evaluate the implications for the specific building blocks they choose to focus on (e.g., status assessments vs. equilibrium profiles vs. harvest strategy simulations).

We used three steps to short-list candidate model fits for each stock (Figure \@ref(fig:ModelSelection)):

1. *Statistical considerations*: Models that fit very poorly or didn't converge, using the criteria listed in Table \@ref(tab:MCMCDiagnostics) were screened out. 
2. *Capacity considerations*: We compared capacity estimates across model fits to screen out any that were considered highly implausible. If the remaining plausible alternative were substantially different, we examined whether the difference was most likely explained by model form, choice of informative/uninformative capacity prior, data truncation, or scatter of data points, and made case-specific choices. Where available, we generally selected model fits with uniform capacity priors capped based on lake photosynthetic rate.
3. *Productivity considerations*: We compared productivity estimates across model fits for a stock, and across stocks, to screen out any that were considered implausible. Where AR1 and TVP models could be fitted, we compared the time-varying parameter estimates to Basic Ricker estimates and made case-specific choices. We generally selected model fits using all available data unless there were clear data issues. Where data and model fits indicated recent changes in population dynamics, we generally selected AR1 or TVP fits over Basic Ricker fits, and fits using all available data rather than truncated data.

The following general guidelines were used to generate alternative productivity scenarios based on subsampling the posterior distributions from the short-listed model fits (Figure \@ref(fig:ModelSelection)):

* To describe long-term average productivity, we sampled from AR1 fit where available, and from Basic Ricker fit otherwise. Using the TVP fit would require averaging or subsampling across all brood years, and so we considered it more appropriate to just use the AR1 parameter estimates, if both AR1 and TVP were available.
* To describe recent productivity and high/low productivity bookends, we sampled from the TVP fit where available, and from the Basic Ricker fit otherwise.
   * Where a TVP fit was available, we subsampled from the annual ln.alpha samples for a full generation, using the most recent generation for the recent productivity scenario, and the generation centred on the lowest/highest productivity for the bookends. As a sensitivity test, we also generated two alternative versions of the recent productivity scenario, using the last two or three full generations (i.e., 8 and 12 brood years for a stock with mainly age 4 returns).
   * Where only a Basic Ricker fit was available, we checked the pattern of Ricker residuals and identified a rough productivity adjustment in terms of a percentile of the posterior. Then we selected half the sample from above and below that percentile to generate a recent scenario. For high/low bookends, we subsampled such that the median ln.alpha corresponds to the 10th and 90th percentiles of the original posterior distribution.
   
\clearpage
(ref:ModelSelection) Considerations for model selection and guidelines for generating productivity scenarios. 

```{r ModelSelection,   fig.cap="(ref:ModelSelection)", out.width = "90%"}
include_graphics("diagrams/Diagram_ModelSelection.PNG")
```



\clearpage

(ref:MCMCDiagnostics) Checklist of MCMC diagnostics. The following standard diagnostics were used to assess MCMC sampling and model fit. Table adapted and expanded from @PestalJohnstonTakuCo. 

```{r MCMCDiagnostics, echo = FALSE, results = "asis"}


table.in <- read.csv("data/Reference Tables/MCMC_Diagnostics.csv",
                     stringsAsFactors = FALSE, fileEncoding = "UTF-8-BOM")

table.in$Examples <- gsub("None","-",table.in$Examples)

   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l","l"),
                  caption = "(ref:MCMCDiagnostics)") %>%
    kableExtra::column_spec(1, width = "7em") %>%
       kableExtra::column_spec(2, width = "10em") %>%
       kableExtra::column_spec(3, width = "17em") %>%
       kableExtra::column_spec(4, width = "5em") %>%
     kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) 

```




\clearpage
## BIOLOGICAL BENCHMARKS AND STATUS BENCHMARKS {#BMMethods}

We calculated four standard biological benchmarks and  two related WSP status benchmarks (Table \@ref(tab:BMDefs)). Preliminary benchmark estimates were reviewed through the TWG process to identify potential errors and anomalies. During the first pass through the TWG review process, Sgen values were flagged as seeming too low for several of the stocks. This triggered a detailed review and testing of all the benchmark calculation steps.

Smax and Seq can be calculated directly from the SR parameters. Smsy and Sgen require a more complex solution, and we tested four alternative implementations for each. Based on the tests summarized in Appendix \@ref(BMCalcTest), we decided to use (1) the  @Scheuerell2016 method for Smsy, because it is the only exact solution, and (2) the @Connorsetal2022 version of the Sgen optimizer, because it was the only optimization approach that generated solutions for all tested combinations of parameters (Table \@ref(tab:BMCalcs)).  Appendix \@ref(BMFuns) shows the corresponding R code.

Some previous escapement goal analyses have used a log-normal bias correction on the productivity parameter (Table \@ref(tab:BMCalcs)), but implementation has varied between agencies, regions, and projects. WSP status assessments  used benchmarks without the bias correction [@FrSkWSPStatus2012; @FrSkWSPStatus2017; @SBCCkWSPStatus2012SAR;@IFCohoWSPStatus2013SAR]. Alaskan escapement goal analyses typically included the bias correction  [@EggersBernard2011Alsek; @FleishmanEvenson2010; @McPhersonetal2010; @Fairetal2011]. Escapement goal analyses for northern transboundary salmon stocks used to include both versions a few years ago [e.g., @PestalJohnstonTakuCo] but have recently shifted to only reporting the bias-corrected version [@Connorsetal2022; @MillerPestalTakuSk]. 

This is not unique to Pacific salmon. In their review of stock-recruit modelling,  @Subbeyetal2014SRReview note that both versions have been widely used and that the choice for a particular applications should consider how the estimates are used afterwards. The general guidelines are:

* Use values *with* bias correction when the management objective is defined in terms of mean values  (e.g., mean Smsy).
* Use values *without* bias correction when the management objective is defined in terms of median values (e.g., median Smsy) or when using the parameter estimates as inputs to other models (e.g., forward simulation).


Systematic testing of Skeena and Nass Sockeye SR data (Appendix \@ref(BiasCorrtest)) demonstrated that the effect of bias correction is generally small for productive stocks with good SR model fits (i.e., sigma is small relative to ln.alpha), but can be substantial for stocks with low productivity and poor SR model fits (i.e., sigma is larger relative to ln.alpha). The bias correction generally increases Smsy estimates and decreases Sgen estimates.

Given these observed effects, and the differences in approach over recent years, we chose to report medians and percentiles of posterior parameter estimate without log-normal bias correction throughout this paper, but included the bias-corrected version in Appendix \@ref(BiasCorrectedBM). 
 


\clearpage
(ref:BMDefs) Definition of standard biological benchmarks and Wild Salmon Policy status benchmarks for the relative abundance metric. Note that we define benchmarks in terms of median recruits and median yield, and therefore present benchmark estimates without log-normal bias correction throughout most of the paper. Bias-corrected mean estimates of biological benchmarks are included as Appendix \@ref(BiasCorrectedBM).


```{r BMDefs, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Reference Tables/BM_Definitions.csv")

table.in$Type[duplicated(table.in$Type)] <- ""


table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:BMDefs)")  %>%
	kableExtra::row_spec(4, hline_after = TRUE) %>%
   #  kableExtra::column_spec(1, width = "8em") %>%
   #     kableExtra::column_spec(2, width = "8em") %>%
        kableExtra::column_spec(3, width = "30em") %>%
   kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```


(ref:BMCalcs) Calculation approach for biological benchmarks.

```{r BMCalcs, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Reference Tables/BM_Calc_Equations.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:BMCalcs)")  #%>%
	#kableExtra::row_spec(4, hline_after = TRUE) %>%
     #kableExtra::column_spec(1, width = "8em") %>%
       # kableExtra::column_spec(2, width = "8em") %>%
        #kableExtra::column_spec(3, width = "30em") %>%
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



(ref:BiasCorrCalcs) Log-normal bias correction for the productivity parameter by model form.

```{r BiasCorrCalcs, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Reference Tables/Bias_Corr_Equations.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:BiasCorrCalcs)") %>%
	kableExtra::row_spec(1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "12em")  %>%
       kableExtra::column_spec(2, width = "12em") %>%
        kableExtra::column_spec(3, width = "20em")# %>%
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```







\clearpage

## ALTERNATIVE APPROACHES FOR DEVELOPING MANAGEMENT REFERENCE POINTS FOR STOCK AGGREGATES {#AltApproachesComp}

### Overview of Alternative Approaches

This paper and the underlying R code were structured to clearly separate the steps of (1) biological analyses to fit SR models and generate alternative productivity scenarios, and (2) using the SR parameter sets to develop management reference points for the Skeena and Nass Sockeye stock aggregates. These steps are fundamentally different in terms of the information and process they require. Keeping the analyses modular has allowed us to set up a framework for future updates and collaborative processes. 

We provide worked examples of eight alternative aggregation approaches for the second step, and a rationale for why these examples are included in this paper in Section \@ref(AnalysisOverview). These examples use the specific SR fits and productivity scenarios described above, but can be quickly regenerated with alternative parameter sets (e.g., if participants in a planning workshop suggest a different productivity scenario, or contribute alternative SR parameter sets based on alternative analyses).

Table \@ref(tab:TableAltApproaches) summarizes the alternative approaches and defines the short labels we use throughout the rest of the paper. The approaches are presented in order of increasing complexity, where complexity can be due to analytical requirements, process requirements, or both. The simplest approaches directly use estimates of biological benchmarks like Smsy or Umsy (Agg Smsy, Sum Smsy, Umsy Comp). Next are approaches that can be calculated directly from  SR parameters using assumptions about long-term equilibrium (Equ. Prof, Agg TradeOff). Aggregation approaches that explicitly consider stock status (Status, Log Reg), are computationally simple but require a collaborative process to agree on status criteria. Forward simulation (Sim) is the most complex approach, because in addition to the SR fitting, many iterations of scoping, prototyping, and review need to occur through a collaborative process. To provide worked examples for each approach, we assumed quantitative objectives, and used examples consistent with previous work (Table \@ref(tab:TableAltApprObj)).   

Six of the eight alternative approaches have been previously used for Skeena or Nass Sockeye analyses (Table \@ref(tab:PastWorkTable)):

* *Agg Smsy*: The current escapement goals for Skeena and Nass Sockeye are based on aggregate-level Smsy estimates developed in 1958 for the Skeena, prior to the implementation of Babine spawning channels, and in the 1990s for the Nass. 
* *Sum Smsy*: In 2016, the Skeena First Nations Technical Committee recommended increasing the limit reference point for aggregate Skeena Sockeye from 400,000 to 600,000 based on the sum of stock-level Smsy estimates and the observed stock composition [@NCIFMP2019]. 
* *Umsy Comp*: @Waltersetal2008ISRP included a comparison of stock-specific estimates of Fmax, the maximum exploitation rates (ER) that can be applied sustainably
without causing extinction (their Figure 14). The worked example we include here compares stock-specific estimates of an ER benchmark. 
* *Equ. Prof*: These have not been previously published for Skeena or Nass Sockeye, but are a standard output for the escapement goal reviews completed for northern transboundary stocks [e.g., @MillerPestalTakuSk].
* *Agg Tradeoff*: This was a key result of @Waltersetal2008ISRP, and triggered changes to the Canadian domestic harvest management approach. 
* *Status*: @KormanEnglish2013 and  @PacificSalmonExplorer included synoptic, or “first-cut”, status assessments taking a broad-brush and consistent approach based on a single status metric. While this approach does not cover all the considerations captured in the integrated multi-criteria status assessments completed under WSP [@FrSkWSPStatus2012; @FrSkWSPStatus2017; @SBCCkWSPStatus2012SAR; @IFCohoWSPStatus2013SAR], it uses the same benchmarks for the relative abundance metric (Sgen, 80% Smsy), and gives comparable results for those CUs where integrated status assessments were driven by that metric.
* *Log Reg*:  This is one of two candidate approaches described for developing limit reference points (LRP) for stock management units (SMU) under the modernized *Fisheries Act* (2019). @LRPGuidelinesSAR summarizes three case studies and concludes that *"Logistic regression LRPs’ have several limitations and should only be used when (i) supplemental aggregate abundance LRPs are required and (ii) all assumptions of the logistic regression model can be met"*. We included a worked example for this method to check whether the challenges identified by @LRPGuidelinesSAR are encountered for Skeena and Nass Sockeye.
* *Sim*: @CoxRogersetal2010 tested the effect of different harvest rates (i.e., *open loop*) over 15 years (*short simulation*) and 100 years (*long simulation*).  @Hawkshaw2018Diss used optimization techniques (*long simulation*) to compare alternative harvest strategy types (*open* and *closed* loop). The harvest rates in @CoxRogersetal2010 were applied equally to all stocks.  @Hawkshaw2018Diss explored alternative harvest control rules and fishing plans for multi-species mixed-stock fishery (i.e., 5 Pacific salmon species and steelhead, each modeled as a single stock).







\clearpage
(ref:TableAltApproaches) Alternative Approaches for Developing Aggregate Biological Reference Points. 

```{r TableAltApproaches, echo = FALSE, results = "asis"}


table.in <- read_csv("data/AggregationApproachTables/AggregationTable_AltApproaches.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableAltApproaches)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "8em") %>%
     kableExtra::column_spec(2, width = "6em") %>%
     kableExtra::column_spec(3, width = "32em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```


\clearpage

(ref:TableAltApprObj) Objectives used in the worked examples for each aggregation approach. 

```{r TableAltApprObj, echo = FALSE, results = "asis"}


table.in <- read_csv("data/AggregationApproachTables/AggregationTable_ExampleObjectives.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableAltApprObj)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "8em") %>%
     kableExtra::column_spec(2, width = "38em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage
### Evaluation of Alternative Approaches

There are advantages and disadvantages for each of the different approaches for developing management reference points described here. For example, calculating Smsy for aggregate stocks using the full time series of available data is the most computationally straightforward method to produce a single estimate and comparatively simple to implement in a management framework, but may not meet conservation objective for smaller, less productive stocks within each aggregate. A status-based approach, which may better address WSP or other Canadian legislative requirements, does not provide explicit target abundances as reference points and may not meet requirements for developing international harvest sharing agreements. Although simulation modeling is the most computationally intense approach, it may better address considerations about variability in future productivity than the sum of lower benchmarks developed assuming average long-term productivity for the different stocks.  

The initial version of this Research Document did not make a clear recommendation for which approach should be used to inform aggregate escapement goals for Skeena and Nass Sockeye. Likewise, the CSAS review committee did not reach a consensus during the Regional Peer Review. A subgroup of participants was convened in a follow-up process to develop this advice, which included (1) identifying criteria for evaluating the alternative approaches, (2) completing a detailed evaluation of each approach, and (3) generating a summary table of comparisons, along with an overview of practical challenges for the alternative approaches. This structured comparison of approaches is a key product of the peer review process.

11 evaluation criteria were identified and grouped into three types (Table \@ref(tab:TableCriteria)). Five of the criteria relate to parameter estimation, four relate to the type of outcome the approach produces, and two reflect practical considerations for implementation. In a virtual workshop, we scored each approach against all 11 criteria (Table \@ref(tab:TableSummary)) and drafted a brief rationale for each score (Appendix \@ref(AggregationAppendix)). Key challenges for each approach were also identified (Table \@ref(tab:TableChallenges)).

Appropriate aggregation approaches can be selected depending on which criteria are identified as critical for a specific application. For example:

* If abundance-based aggregate escapement goals that consider stock level diversity are required, then the only approaches that meet these criteria are aggregate equilibrium tradeoff plots, logistic regression, and a forward simulation approach. 
* Logistic regression is not  appropriate for Nass stocks, because past aggregate abundance was found to be not correlated with stock-level performance measures. 
* This leaves the aggregate equilibrium tradeoff plots and forward simulation as the only viable options within this example. 
* Of these, closed loop forward simulation within a Management Strategy Evaluation (MSE) framework is the only aggregation approach identified that meets all of the criteria identified by CSAS review committee, while the aggregate equilibrium trade-off approach can be implemented within a relatively short time frame.

The aggregate equilibrium trade-off approach was recommended for evaluating alternative goals and harvest management rules for Skeena Sockeye in the report prepared by the 2008 Skeena Independent Science Review Panel (ISRP) (Walters et al. 2008). At the time, the ISRP report and preliminary trade-off analyses led to changes in the harvest rule for Canadian marine commercial fisheries for Skeena Sockeye implemented in 2009, which substantially reduced the harvest rate in these fisheries.

A full simulation model and associated MSE framework and process would require a considerable investment of time to develop (1) agreed-upon objectives, (2) agreed-upon model scope, and (3) agreed-upon scenarios for testing through a structured process. Depending on the available time to select an escapement goal, evaluating aggregate tradeoff plots may be the best option for developing an aggregate escapement goal in the short term. If a full MSE is not feasible within the available time frame, a simplified forward simulation can also be used to provide a complementary set of results for aggregate tradeoff considerations in a relatively short period of time.



\clearpage

(ref:TableCriteria) Description of criteria for evaluating the alternative approaches described in Table \@ref(tab:TableAltApproaches). An initial list of criteria was identified during the peer review meeting, then modified as the evaluations were being filled in during the follow-up process. Criteria can be grouped into three distinct types: Estimation criteria are relevant to SR model fitting or simulation model scoping. Outcome criteria relate to the type of end-product generated by the aggregation method. Implementation criteria relate to how the end-product can be used, and when it could be available.

```{r TableCriteria, echo = FALSE, results = "asis"}


table.in <- read_csv("data/AggregationApproachTables/AggregationTable_Criteria.csv")

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteria)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "4em") %>%
     kableExtra::column_spec(2, width = "14em") %>%
     kableExtra::column_spec(3, width = "28em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage

(ref:TableSummary) Summary of characteristics of 8 alternative methods for developing aggregate reference points. The peer-review process compared alternative approaches for developing aggregate reference points (Table \@ref(tab:TableAltApproaches)) based on a set of 10 criteria (Table \@ref(tab:TableCriteria)). A YES/NO/MAYBE rating was assigned for each criterion to provide a comparison of aggregation methods. YES identifies that the aggregation approach meets the criterion. MAYBE means that current approach could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.  The Critical column values are provided by the review participants and identify criteria that are critical (Yes) or to be determined (TBD). Appendix \@ref(AggregationAppendix) briefly summarizes the rationale for each rating.

\begin{landscapepage}



```{r TableSummary, echo = FALSE, results = "asis"}



table.in <- read_csv("data/AggregationApproachTables/AggregationTable_Summary.csv")

col.names.use <- c("Criterion","Crit?","Agg\nSmsy","Sum\nSmsy","Umsy\nComp","Equ.\nProf",
"Agg\nEqu.\nProf.","Status","Log\nReg","Sim")


#table.cols <- table.in
#table.cols[,] <- "white"
#table.cols[table.in == "YES"] <- "green" 
#table.cols[table.in == "NO"] <- "orange" 

col3.cols <- rep("white", 11)
col3.cols[table.in[,3]  == "YES"]  <- '#b8e186'
#col3.cols[table.in[,3]  == "NO"] 	 <- '#d01c8b'
col3.cols[table.in[,3]  == "MAYBE"] 	 <- "#F1CF31"	  #'#f1b6da'

col4.cols <- rep("white", 11)
col4.cols[table.in[,4]  == "YES"]  <- '#b8e186'
col4.cols[table.in[,4]  == "MAYBE"] 	 <- "#F1CF31"	 

col5.cols <- rep("white", 11)
col5.cols[table.in[,5]  == "YES"]  <- '#b8e186'
col5.cols[table.in[,5]  == "MAYBE"] 	 <- "#F1CF31"	 

col6.cols <- rep("white", 11)
col6.cols[table.in[,6]  == "YES"]  <- '#b8e186'
col6.cols[table.in[,6]  == "MAYBE"] 	 <- "#F1CF31"	 

col7.cols <- rep("white", 11)
col7.cols[table.in[,7]  == "YES"]  <- '#b8e186'
col7.cols[table.in[,7]  == "MAYBE"] 	 <- "#F1CF31"	
	
col8.cols <- rep("white", 11)
col8.cols[table.in[,8]  == "YES"]  <- '#b8e186'
col8.cols[table.in[,8]  == "MAYBE"] 	 <- "#F1CF31"	
	
col9.cols <- rep("white", 11)
col9.cols[table.in[,9]  == "YES"]  <- '#b8e186'
col9.cols[table.in[,9]  == "MAYBE"] 	 <- "#F1CF31"	
	
col10.cols <- rep("white", 11)
col10.cols[table.in[,10]  == "YES"]  <- '#b8e186'
col10.cols[table.in[,10]  == "MAYBE"] 	 <- "#F1CF31"	
	
#col11.cols <- rep("white", 11)
#col11.cols[table.in[,11]  == "YES"]  <- '#b8e186'
#col11.cols[table.in[,11]  == "MAYBE"] 	 <- "#F1CF31"	

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = "l",
              col.names = linebreak(col.names.use), #landscape = TRUE,
                  caption = "(ref:TableSummary)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "20em") %>%
  kableExtra::column_spec(3, background =  col3.cols) %>%
  kableExtra::column_spec(4, background =  col4.cols) %>%
	  kableExtra::column_spec(5, background =  col5.cols) %>%
	  kableExtra::column_spec(6, background =  col6.cols) %>%
	  kableExtra::column_spec(7, background =  col7.cols) %>%
	  kableExtra::column_spec(8, background =  col8.cols) %>%
	  kableExtra::column_spec(9, background =  col9.cols) %>%
	  kableExtra::column_spec(10, background =  col10.cols) #%>%
	  #kableExtra::column_spec(11, background =  col11.cols)
	
	
	
	
	
	
	
	#table.cols[,3]) #%>%	

# COLOR NOT WORKING! WHY??????


```

\end{landscapepage}

\clearpage
(ref:TableChallenges) Key challenges for alternative aggregation approaches. Aggregation approaches are grouped into 4 types based on their fundamental ingredient (i.e., benchmarks, status, equilibrium profiles, or forward simulations).

```{r TableChallenges, echo = FALSE, results = "asis"}


table.in <- read_csv("data/AggregationApproachTables/AggregationTable_KeyChallenges.csv")


table.in$Type[duplicated(table.in$Type)] <- ""


table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 9,align = "l",
              #col.names = linebreak(col.names.use),
                  caption = "(ref:TableChallenges)")  %>%
	kableExtra::row_spec(c(3,5,7), hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "8em") %>%
         kableExtra::column_spec(2, width = "15em") %>%
        kableExtra::column_spec(3, width = "20 em") %>%
	kableExtra::row_spec(c(1,2,4,6,8), extra_latex_after = "\\cmidrule(l){2-3}") 


```







\clearpage

## IMPLEMENTATION OF ALTERNATIVE APPROACHES FOR DEVELOPING MANAGEMENT REFERENCE POINTS FOR STOCK AGGREGATES {#AltApproachesMethods}


This section briefly describes the implementation of the alternative approaches for developing management reference points that were discussed in the initial version of this Research Document and during the CSAS RPR.

### Aggregate Estimates of Biological Benchmarks (Agg Smsy)

This approach simply calculates Smsy estimates for aggregate-level SR model fits (Sec. \@ref(CandidateAggModels)). 


### Sum of Stock-Level Biological Benchmarks for Abundance (Sum Smsy)

Calculate the sum of Smsy estimtes for wild stocks. For the example presented here, we compare the sum of mean and median Smsy and Smax estimates across modelled stocks to the corresponding estimates for aggregate-level SR model fits. Percentiles of the distributions are shown for the single-stock and aggregate fits. If percentiles for the sum of stock-level estimates are required, they can be calculated by summing the individual MCMC samples, then calculating the percentiles. 

### Compare Stock-level Biological Benchmarks for Exploitation Rate (Umsy Comp)

Although estimates of Umsy cannot be summed across stocks, but it is informative to compare them. We include two types of comparison:

* Visual comparison of posterior distributions for stock-level and aggregate-level Umsy estimates. 
* Frequency distribution of median Umsy across stocks, adapting the approach from Figure 14 in @Waltersetal2008ISRP.

### Calculation of Spawner-based Equilibrium Profiles (Equ. Prof) {#EqProfilesMethods}

Recent escapement goal analyses for Alaskan salmon stocks have equilibrium  probability profiles as a standard part of the results. Initial applications focused on yield profiles that capture the notion of “pretty good yield” (PGY) as defined by @HilbornPGY, but other types of profiles have also been explored (e.g., recovery profiles). While implementation methods continue to evolve, these profiles were generated with the same basic approach.

For example, the "80-60 range" for a yield profile is derived as follows:

* specify a reference value $Ref$ for yield at 60% of MSY.
*  for each spawner abundance $S_i$, calculate the percent of MCMC samples for which the expected number of recruits is at least $S_i + Ref$, which captures the expected yield under equilibrium conditions if the stock were managed to a fixed escapement goal at $S_i$ and all returns above $S_i$ were harvested. 
* Identify the range of $S_i$ for which the percent of samples meeting the criterion is at least 80%.

Examples include summer Chum Salmon in the East Fork of the Andreafksy River (Fleishman and Evenson 2010), Taku River Chinook Salmon (McPherson et al. 2010), Alsek River Sockeye (Eggers and Bernard 2011), and salmon stocks in the Copper and Bering rivers (Fair et al. 2011). Equilibrium probability profiles have also been included in recent escapement goal analyses for northern transboundary salmon stocks [e.g., @PestalJohnstonTakuCo; @MillerPestalTakuSk].

We implemented the yield profiles as follows: at each increment of spawner abundance, we compare the distribution of yields (Rec-Spn) across parameter samples to the specified % of the median yield at median Smsy, and count the proportion that are larger. The resulting profile shows the probability of meeting or exceeding this average target, which is an anchor point for subsequent planning processes tasked with choosing spawning goals. 

These yield profiles differ from the version included in recent ADFG and transboundary analyses [e.g., @EggersBernard2011Alsek; @MillerPestalTakuSk], which plot the probability of meeting the implied target for each parameter set (i.e., at each spawner increment, compare yield to a chosen % of MSY for that parameter set). @PestalJohnstonTakuCo compared the two approaches. Both have the same intent, and we consider them equally valid. They simply differ in the details of the calculation. We did not include a side-by-side comparison in this paper, but the alternate version can easily be calculated if a future planning process requests it.

We included three alternative yield profiles to illustrate the importance of specifying the exact objectives:

* Probability that equilibrium yield > 80% MSY
* Probability that equilibrium yield > 60% MSY
* Probability that equilibrium yield > stock-specific reference value (e.g., 1,000, 10,000, etc.)

For each profile, we show two curves corresponding to the long-term average and recent productivity scenarios, using the *same* reference value (i.e., both are compared to the long-term average MSY). The intent is to highlight the difference in expected yield between the two productivity scenarios.
 

### Calculation of ER-based Equilibrium Profiles (Agg Tradeoff)


Using the approach by Walters et al. (2008), the equilibrium state for each component stock was calculated at different levels of fixed exploitation rate (i.e., what spawner abundance and catch would the stock eventually settle down to, if each ER were applied for many years, in the absence of inter-annual variation?). Equilibrium spawner abundances and catches were then summed across stocks to calculate aggregate equilibrium spawners and catch under the assumption that all component stocks are harvested at the same fixed ER, and all are at equilibrium. This simplifying assumption allows the aggregate trade-off profiles to be calculated directly from the SR parameter estimates.  This approach is described in Section 2.3 of Walters and Martell (2004), Walters et al. (2019), Staton et al. (2020), and Connors et al. (2020).

At a fixed exploitation rate $U_q$ the equilibrium calculation for spawner abundance $S_{q}$  is:


\begin{equation}
S_{q} = S_{\mathrm{MSY}} \frac{U_{MSY} - \ln\left(\frac{1 - U_{MSY}}{1 - U_{q}}\right)}{U_{MSY}}
\end{equation}

and for equilibrium harvest ($H_{q}$) is:

\begin{equation}
H_{q} = \frac{U_{q}S_{q}}{1 - U_{q}}
\end{equation}

Appendix \@ref(EquProfFuns) shows the corresponding R code.

### Status-based Aggregate Limit Reference Points (Status) {#StatusMethods}

Canada's modernized *Fisheries Act* (2019) requires that limit reference points (LRP) are developed for stock management units (SMU). Pacific salmon present a challenge due to their complex population structure, and guidelines for developing aggregate salmon LRPs were just published [@LRPGuidelinesSAR]. The recommended approach is to assess status of the CUs in the SMU according to WSP criteria, and then determine whether an SMU meets the LRP based on the CU statuses (i.e., number of CUs in the red status zone, changes in CU statuses over time).

Status assessments under the WSP integrate multiple metrics, where available [@Holtetal2009BM]: 

* abundance relative to biological benchmarks (Sgen, 80% Smsy), where available
* absolute abundance relative to a small population threshold of 1,000 spawners, for consistency with COSEWIC criteria [@CosewicMetrics2021]
* long-term trend
* short-term trend (probability of decline)
* distribution of spawners across sites

Integrated WSP status assessments have been completed for Fraser River Sockeye [@FrSkWSPStatus2012; @FrSkWSPStatus2017], Southern BC Chinook [@SBCCkWSPStatus2012SAR], and Interior Fraser Coho [@IFCohoWSPStatus2013SAR]. Each of these status assessments was a multi-year process, culminating in a multi-day workshop where 30-40 experts reviewed available information (quality-controlled data, biological benchmarks, status metrics) and assigned a consensus status designation to each CU. This process has not been completed for Skeena and Nass Sockeye, but considering stock status in harvest management decisions is required under the WSP [@WSPImplementation]. 

Ongoing work to develop an algorithm-based rapid approximation of the expert status designations will use the data and biological benchmarks generated through the Skeena and Nass Sockeye escapement goal review process. Pending completion of these multi-criteria status assessments for Skeena and Nass Sockeye, we illustrate the building blocks of the status-based approach using one of the status metrics, but we do not attempt to complete a comprehensive status assessment here.

Specifically, we compared the running generational geometric mean of spawner abundance to the lower benchmark at Sgen and the upper benchmark at 80% Smsy, then summarized the annual proportion of stocks in the red, amber, and green status zones *on that single metric*.  We used the median Sgen and Smsy values for the long-term average productivity scenario (Section \@ref(ModelSelection)), which is consistent with the benchmarks used in past WSP status assessments (Section \@ref(BMMethods)).


### Aggregate Abundance Reference Points Based on Logistic Regression (Log Reg)

A candidate approach for developing aggregate abundance reference points is to define a success/failure criterion, plot observed success/failure vs. observed aggregate abundance, fit a logistic regression, and select a reference point based on a chosen probability threshold [@LRPGuidelinesSAR]. This approach is only applicable under certain conditions, and formal guidelines for its use in the development of limit reference points have not been finalized. 

We include an example of this approach using a criterion linked to the lower WSP benchmark for the relative abundance metric, which is Sgen. Specifically, we defined success as *"At least 80% of the stocks in the aggregate are above Sgen"*.  We used the median Sgen value for the long-term average productivity scenario, which is consistent with the benchmarks used in past WSP status assessments (Section \@ref(BMMethods)).

<!--chapter:end:02-Methods.Rmd-->


### Simulation-based Aggregate Abundance Reference Points (Sim) {#SimMethodsGeneral}


Forward simulation can be used to explore how different management actions perform over a range of alternative assumptions about future conditions, with risk quantified using the resulting escapement trajectories as part of a formal decision analysis [e.g., @HilbornPeterman1996PrecApp; @deYoungetal1999UncertainWorld; @PuntetalMSEBestPractices].

The key benefit of building forward simulation models is that they allow us to compare the expected performance of alternative strategies and identify strategies that are more robust to uncertainty [e.g., @PuntetalMSEBestPractices], which has been characterized as searching for a *"safe-fail"* strategy that avoids catastrophic outcomes even when things go wrong, rather than identifying a strategy that is optimal under very specific assumptions and conditions (Ann-Marie Huang, DFO, and Mike Staley, pers. comm, 2010).

Building a fully functional simulation model to support Skeena and Nass Sockeye planning will require that many choices be made regarding model scope, biological assumptions, management assumptions,  objectives, and performance measures. 

We include a worked example to illustrate potential benefits and expected challenges, and to initiate the development process for a more comprehensive model. We consider this an urgent first step, because  recently published guidelines [@LRPGuidelinesSAR] identify forward simulations as one of the candidate approaches for developing aggregate reference points for stock management units, and the on-going Canadian domestic consultation process has also focused on simulation results. In addition, the development of a formal management strategy evaluation (MSE) model was identified as an important future step by the two independent reviewers for the escapement goal review process. We describe the current version of the simulation model in the next section.


## IMPLEMENTATION OF FORWARD SIMULATION APPROACH {#SimMethodsImplementation}

### Model Structure


```{r num.stocks.calc, echo = FALSE, results = "asis"}

tmp.df <- read_csv("data/Sims/Generated_StockInfo_SimUsed.csv")

num.nass <- sum(tmp.df$MU == "Nass")
num.skeenawild <-  sum(tmp.df$MU == "SkeenaWild")


```

This simulation example explores the range of near-term responses to alternative harvest strategies under alternative productivity assumptions, starting from recent spawner abundances (i.e., not simulating a long time into the future to explore equilibrium conditions).

This simulation example includes only the  `r  num.nass + num.skeenawild` wild stocks for which spawner-recruit models were fitted in the current round of work (`r num.nass` Nass stocks, `r num.skeenawild` Skeena stocks). Simulations start with the last 8 years of spawner abundance (2012-2019). For a few stocks, missing estimates in this time window were infilled with the mean of available observations. 

Forward simulations generate one 20-yr trajectory for each parameter set sampled from the parameter distributions selected for each productivity scenario (Section \@ref(ModelSelection)).

For each simulated year (Figure \@ref(fig:SimModelFlowchart)):

* calculate stock run size based on recruits by age
* apply the candidate harvest strategy to each aggregate
* calculate the resulting spawner abundance by stock
* calculate the total recruits for each stock based on SR parameter set for the candidate productivity scenario (includes a random error, and a cap on recruitment set at 20% larger than the largest observed recruitment) 
* distribute the recruits across return years based on median observed age composition

Total run size $Run$ for each aggregate $agg$ in year $yr$ with parameter set $par$ is the sum across ages and stocks, with age-specific recruits for each stock from corresponding brood years (e.g., age 4 fish from 4 years ago, age 5 fish from 5 years ago):

\begin{equation} 
  Run_{agg,yr,par} =  \sum_{ages}\sum_{stocks} Rec_{age,stock,yr-age,par}
\end{equation} 

Exploitation rate for each stock is a calculated based on the aggregate run, an aggregate harvest control rule $HCR$, aggregate-level outcome uncertainty $AggOU$, and stock-specific outcome uncertainty $StkOU$:
 
\begin{equation} 
  ER_{stock,yr,par} = fn(Run_{agg,yr,arp}, HCR, AggOU, StkOU)
\end{equation}

Catch $Ct$ and spawner abundance $Spn$ for each stock are then calculated as:

\begin{equation} 
  Ct_{stock,yr,par} = Run_{stock,yr,par} * ( ER_{stock,yr,par}) 
\end{equation}

\begin{equation} 
  Spn_{stock,yr,par} = Run_{stock,yr,par} * (1 - ER_{stock,yr,par}) 
\end{equation}

Finally, total recruits $Rec$ for each stock are calculated as:

\begin{equation} 
  Rec_{stock,yr,par} =  Spn_{stock,yr,par} * exp(ln.alpha_{stock,par} -  beta_{stock,par} * Spn_{stock,yr,par}) 
\end{equation}


The same model structure and code base were used for the Recovery Potential Assessment (RPA) of Fraser River Sockeye [@Huangetal2021FraserSkRPA].  The code is designed for computing efficiency in R, using array calculations and pre-filled arrays where possible. For example, age proportions are pre-generated as a 4- dimensional array (Stocks x  MCMC parameter sets x Simulation Years x Age Classes), and for each simulated brood year the corresponding 3-dimensional sub-array  of age proportions is multiplied with a 2-dimensional slice of the recruitment array (Stock x MCMC parameter sets) to populate a subset of a 3-dimensional run size array (Stocks x  MCMC parameter sets x [Brood Year + Min Age]:[Brood Year + Max Age]). The pre-generated arrays allow maximum flexibility for exploring alternative assumptions (e.g., variable or changing age composition).

\clearpage
(ref:SimModelFlowchart) Simulation model components.  The biological submodel simulates stock-specific population dynamics to generate adult returns for each stock for each brood year and resulting aggregate runs by calendar year. The harvest submodel then determines a target ER given a harvest rule and applies it with outcome uncertainty to calculate harvest and spawner abundance. One example of a harvest rule is a fixed escapement target of 300,000 combined with a minimum ER of 10% and a maximum ER of 65%. Alternative simulations can then test the effect of changing components of the harvest rule, such as varying the maximum ER from 20% to 80% in increments of 10%.


```{r SimModelFlowchart,  fig.cap="(ref:SimModelFlowchart)"}
include_graphics("diagrams/Diagram_SimModelStructure_REV.PNG")
```



\clearpage

Three extensions of the Fraser Sockeye RPA model were implemented for Skeena and Nass Sockeye:

* *Aggregate harvest strategies*: The purpose of the RPA model was to test different levels of fixed exploitation rate, but for Skeena and Nass Sockeye the focus is on testing alternative types of harvest strategies (i.e., fixed aggregate escapement goal, abundance-based rule).
* *Outcome uncertainty*: This captures the difference between management targets and actual realized outcomes, caused by mechanisms like: (1) uncertain estimates of abundance and run timing, (2) physical and biological processes that change the availability of fish to fishing gear within a season, (3) non-compliance with fishing regulations, (4) inappropriate choices of regulations, (5) errors in their implementation [e.g., @HoltPetermanOutcomeUnc]. We model outcome uncertainty in two steps, first as the difference between the aggregate ER target and the actual aggregate ER, and then as the difference between actual aggregate ER and stock-specific ER (Appendix \@ref(OutcomeUncApp)).
* *Covariation in productivity*: This captures the observation that productivity is not independent across salmon stocks, because shared environmental factors affect their life cycle [e.g., @CkCov2017]. While it is very hard to identify the specific biological mechanisms at work on a specific group of stocks, we can identify patterns in the resulting overall productivity (Recruits/Spawner) and reflect that in the forward simulation by sampling the annual random error with covariation, such that better-than-expected recruitment for Stock A tends to happen in the same year as better-than-expected recruitment for Stock B. We model covariation in productivity based on simplified correlations of log-residuals from the Basic Ricker fit within and between groups of stocks with similar life history spawning in a shared freshwater adaptive zone (Appendix \@ref(CovarProdApp)). 

Additional mechanisms could be added to the model, such as:

* en-route or pre-spawn mortality (i.e., not all fish that escape past the fisheries spawn successfully)
* changes over time in productivity (for now, the productivity scenarios differ from each other, but each is assumed to persist over the 20-yr simulation)
* changes over time in age composition of recruits for each stock

Each of these additions has a potentially large effect on the simulation results, but addressing them properly is a complex challenge, and not purely a technical one. Participants in the planning process will have to consider which of them need to be explored, and how to bound the explorations, if the choice is made to pursue simulation-based aggregate reference points.





\clearpage

### Types of Harvest Control Rules

In the current model structure, two types of alternative harvest strategies can be specified for each of the two aggregates (Nass, SkeenaWild):

* *FixedER*: Apply a fixed exploitation rate from 0% to 90% in 10% increments to both aggregates, assuming that all component stocks are harvested at the same rate. 

* *FixedEsc*: Simple abundance-based harvest rule for each aggregate, where target exploitation rate is based on aggregate abundance above the escapement goal, with optional specifications for a minimum ER at low abundance and upper cap on ER at larger abundances. Alternative escapement targets were set at 25% to 250% of the interim escapement goal for Nass at 200,000 and the interim aggregate escapement goal for SkeenaWild at 300,000 (Table \@ref(tab:FixedEscHCR)). These escapement targets were combined with lower bounds on ER of 0%, 10%, or 20%, and upper bounds of 60% or 80%.


(ref:FixedEscHCR) Fixed Escapement Strategy: Alternative Scenarios. Scenarios were specified relative to the interim escapement goals currently in use, so that *Esc100* matches the interim goal, *Esc50* is half the interim goal, and *Esc200* is double the interim goal. Note that the interim goal for the SkeenaWild aggregate was set at 1/3 of total Skeena interim target of 900,000, based on average observed proportion of wild spawners in the total spawner abundance since 2000.


```{r FixedEscHCR, echo = FALSE, results = "asis"}


table.in <- read_csv("data/Sims/FixedEscMin10Max80ER_Variations.csv") %>% 
	mutate(ScenarioValue = as.numeric(gsub("Esc","",Scenario))) %>%
	arrange(ScenarioValue) %>%
	select(Scenario,MU,spn.goal) %>%
	dplyr::rename(SpnGoal = spn.goal) %>%
	mutate(SpnGoal = prettyNum(round(SpnGoal*10^6,0),big.mark=",",scientific=F) )  %>% 
	pivot_wider(id_cols = Scenario,names_from = MU, values_from = SpnGoal)




table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","r","r"),
                  caption = "(ref:FixedEscHCR)")  %>%
	kableExtra::row_spec(3:4, hline_after = TRUE) %>%
	kableExtra::row_spec(4, bold = TRUE) 


```






### Scenarios {#SimScenarios}

What makes forward simulations so powerful is the ability to test many different scenarios and bring the results into a collaborative planning process where plausible outcomes under alternative assumptions can be debated. However, this flexibility also creates the biggest challenge for using simulation models in a decision support setting: how to bound the explorations?

With the current model structure, we have so far explored the following options for key model components:

* *Productivity*: six alternative scenarios (Section \@ref(ModelSelection))
* *Harvest Strategy*: Two main types (Fixed ER vs. Fixed Esc), 10 different levels for each, plus alternative combinations of ER floor and cap for the Fixed Esc strategies (0-90% ER, 10-80%, 20-80%, 20-60%), for a total of 50 alternative strategies
* *Aggregate outcome uncertainty*: Three alternatives - none, narrow, wide (Appendix \@ref(OutcomeUncApp))
* *Stock-specific outcome uncertainty*: Three alternatives - none, all years, 1995-2013 brood years (Appendix \@ref(OutcomeUncApp))
* *Covariation in productivity*: Four alternatives - none, simplified covariation 1984-2013 brood years, simplified covariation 1999-2013 brood years, detailed pairwise covariation 1984-2013 brood years  (Appendix \@ref(CovarProdApp))

Just these model components already yield 6x50x3x3x4 = 10,800 alternative scenarios. Because these components interact (e.g., the effect of the covariation assumption may differ depending on the harvest strategy), we would ideally run and compare all of the alternative scenarios, but in practice this is usually an iterative process guided by participants in a broader planning exercise [e.g., @PuntetalMSEBestPractices].

In this paper we present examples of results for 40 of the alternative scenarios, as well as a high-level summary of sensitivity tests completed so far. The intent is to illustrate the type of information that can be generated by a full-scale implementation of a management strategy evaluation and
set the stage for future rounds of model refinement and scenario exploration. The forty scenarios are:

* 10 levels of fixed ER (0-90%) under long-term average productivity
* 10 levels of fixed ER (0-90%) under recent productivity (1 generation)
* 10 levels of fixed escapement, ranging from 1/4 to 2.5 times the interim EG, with a 10% ER floor and an 80% ER cap, under long-term average productivity
* 10 levels of fixed escapement, ranging from 1/4 to 2.5 times the interim EG, with a 10% ER floor and an 80% ER cap, under recent productivity (1 generation)

We ran these scenarios for 3 generations (15 years), starting with 2020 as the first simulated year. All scenarios used the wide version of the aggregate outcome uncertainty, the 1995-2013 version of the stock-specific outcome uncertainty, and the simplified productivity covariation observed over 1999-2013 brood years. These settings were used as the base case for model explorations following the peer review process in April 2022.

### Objectives, Performance Measures, and Diagnostic Plots

To convert the simulation trajectories into meaningful summaries of expected outcomes, we need to identify aggregate-level and stock-level objectives and develop quantitative performance measures for them. For this illustration, we defined a general aggregate objective as *"most stocks should meet their stock-specific conservation objectives"*, and translated this into a quantitative objective that "*16 of the 20 modelled stocks (80%) should have at least 80% probability of spawner abundance exceeding the upper WSP benchmark for the relative abundance metric, which is set at 80% Smsy, after 3 generations (simulation years 11-15)"*. These objectives are examples selected for this illustration, and are not intended as a recommendation of which management objectives should be evaluated by upcoming planning processes. 

We used the median Smsy value for the long-term average productivity scenario (Section \@ref(ModelSelection)) in this performance measure, which is consistent with the benchmarks used in past WSP status assessments (Section \@ref(BMMethods)). Simulation trajectories based on the current productivity scenario and the high/low productivity bookend scenarios were also compared to the same benchmark, to make results comparable across scenarios and to emphasize how different the outcomes under the different scenarios are compared to expectations based long-term average properties.

As with the other components of this simple illustration, the challenging work of developing an agreed-upon suite of objectives and performance measures specifically for the current management context of Skeena and Nass Sockeye will start in the next phase of the project, which is an engagement process with rights holders and stakeholders. Once this takes shape, additional performance measures can easily be calculated and presented for the simulation trajectories (e.g., probability that aggregate catch meets some minimum level, variability in catch associated with different types of harvest strategy).






<!--chapter:end:02b-MethodsSim.Rmd-->

# RESULTS

This Research Document focuses on SR modelling for wild Sockeye stocks (16 Skeena, 4 Nass) and alternative approaches for developing management reference points for two aggregates (Skeena Wild, Nass). Enhanced Pinkut and Fulton Sockeye present a fundamentally different challenge in terms of population dynamics and management implications. We include a review of available information for Babine Sockeye enhancement facilities (Appendix \@ref(ChannelReview)) and illustrate two candidate methods for expanding a management reference point for wild Skeena Sockeye to a management reference point for the whole Skeena aggregate including enhanced Pinkut and Fulton (Section \@ref(SkeenaExpResults)). Section \@ref(AltApproachEnhanced) explains the rationale for this approach within the scope of the current project. Appendix \@ref(PinkutFultonResults) includes the available SR data and parameter estimates for Pinkut and Fulton as a reference, but resulting benchmark estimates should not be used as they are, given SR model fitting issues and management challenges.


## SINGLE-STOCK SPAWNER-RECRUIT MODEL FITS (STOCK-LEVEL AND AGGREGATE-LEVEL)  {#SingleStockSRResults}


### Convergence {#Convergence}


```{r , echo = FALSE, results = "asis"}

conv.summary.df <- read_csv("data/Convergence/FitsComparison_ConvergenceSummary.csv") 

full.set.post.df <- read_csv("data/Appendix SR Model Outputs/FullSet_Posterior_Summary.csv") 

```


We tested `r conv.summary.df %>% dplyr::filter(DataType == "All") %>% select(Total)` alternative SR model fits for `r length(unique(full.set.post.df$Stock)) -3` stock-level SR data sets (including enhanced Pinkut and Fulton) and three aggregate-level SR data sets (Nass, Skeena, SkeenaWild).

All of the candidate SR model fits met at least two of three convergence criteria, and most model fits met all three criteria (Table \@ref(tab:ConvergenceTab)). Convergence varied depending on the amount of data used, the SR model form, and form of the capacity prior (Smax). Models fit to all available brood years of data converged more reliably than model fits to truncated data sets, but note that not all model forms could be fitted to all stocks, and that the number of available brood years varies across stocks (Table \@ref(tab:DataOverviewSkeena); Figure \@ref(fig:SRDataOverview)). More complex SR model forms were run with more intensive MCMC sampling (longer burn-in, larger samples), but still had a lower rate of meeting all three convergence criteria. Alternative capacity priors had little effect on convergence for the Basic Ricker model, with 88-89% of model fits meeting all three convergence criteria across four alternative capacity priors.


### Alternative SR Model Fits

Posterior estimates of capacity (Smax) were much wider and more skewed than posterior estimates of productivity (ln.alpha) for most stocks using the Basic Ricker model (Figure \@ref(fig:FitsCompProdCap)). Four stock-level fits and one aggregate-level fit were notably more uncertain (i.e., wider posteriors) than the other fits: Lower Nass Sea and River Type, Kitwanga, Motase, and the SkeenaWild aggregate.

For the Basic Ricker model, median Bayesian estimates of Smsy were similar to simple deterministic estimates for most cases (Figure \@ref(fig:FitsCompDetFig)). Differences between Bayesian and deterministic estimates were larger for cases where the Bayesian estimate was more uncertain (i.e., wider posterior, larger SIQR). The two stocks with the largest difference between the Bayesian and deterministic Smsy estimate also had the most uncertain Bayesian estimates (Lower Nass Sea and River Type, Kitwanga).


(ref:ConvergenceTab) Summary of convergence diagnostics for single-stock SR model fits. We considered three convergence diagnostics with quantitative thresholds (Table \@ref(tab:MCMCDiagnostics)). Table shows the total number of stock-level and aggregate-level single-stock SR model fits tested (Total), the number that met each criterion (Rhat, Gelman, Geweke), the number and percent of fits that met at least two of the criteria (Met2, pMet2), and the number and percent of fits that met all three criteria (Met3, pMet3). Convergence thresholds were specified as Rhat < 1.05, Gelman within [0.99,1.01], and Geweke within [-2,2]. For each criterion, the value compared to the threshold was the most extreme value across all estimated parameters. For the TVP model fits with time-varying productivity, this captures the poorest fit across all the brood-year-specific ln.alpha posteriors.

```{r ConvergenceTab, echo = FALSE, results = "asis"}

table.in <- conv.summary.df %>% mutate(ModelType = gsub("Kalman","TVP",ModelType))
table.in$DataType[duplicated(table.in$DataType)] <- ""



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l","l","l",rep("r",8)),
                  caption = "(ref:ConvergenceTab)" ) %>%
    kableExtra::row_spec(c(1,9), hline_after = TRUE) %>%
	add_header_above(c(" " = 4, "Criteria" = 3," " = 4) ) %>%
  kableExtra::row_spec(c(5,7,13,15), extra_latex_after = "\\cmidrule(l){2-11}") 

```

For the Basic Ricker model, the effect of alternative capacity priors was generally larger for cases where capacity estimates were more uncertain (Figure \@ref(fig:FitsCompCapPrior)). Three stock-level fits and one aggregate-level fit were notably more sensitive (i.e., larger differences in median estimates) than the other fits: Lower Nass Sea and River Type, Kitwanga, Babine Late Wild, and the Skeena aggregate. Motase had the widest Smsy posterior across alternative capacity priors.


Posterior parameter distributions were similar for Basic Ricker and AR1 model fits in most cases (Figures \@ref(fig:FitsCompAR1Pars) and \@ref(fig:FitsCompAR1Smsy)). The AR1 model had slightly wider ln.alpha
posteriors for most fits and slightly narrower Smax posteriors for some fits. Estimates of ln.alpha  that were more uncertain in the Basic Ricker fit did not improve with the AR1 model form. The AR1 fit for Swan/Stephens was even poorer than the Basic Ricker fit. Among the cases where AR1 models could be fitted, the posterior capacity estimates were most uncertain for the three aggregate-level fits and for Lower Nass Sea and River Type Sockeye, which also showed the largest improvement due to the change in model form (i.e., Smax posterior much narrower for the AR1 fit than the Basic Ricker fit). Median Smsy estimates were very similar for the two alternative model forms, except for Lower Nass Sea and River Type, where the AR1 estimate has a 26% lower median and a substantially narrower posterior.

A TVP model with time-varying productivity could be fitted to 12 stocks to identify stock-specific changes over time in productivity (Figure \@ref(fig:FitsCompProd1)). Most stocks showed recent declines in productivity, but the time trends differed between stocks.  Five stocks showed a persistent decline in productivity since the 1990s, and these include the two largest wild stocks (Babine Early Wild, Babine Late Wild, Damdochax, Meziadin, and Swan/Stephens). Estimated productivity of Alastair gradually increased since the 1960s, until a drastic drop in the last few brood years. Babine Mid Wild has been mostly stable, with a slight decline. Bear has increased. Productivity is highly variable over time for three stocks (Lakelse, Lower Nass Sea and River Type, and Morice). 

The estimated productivity trends from the TVP model fit generally track the trends over time of residuals generated from a Basic Ricker fit (Figure \@ref(fig:FitsCompProd1) vs. Figure \@ref(fig:FitsCompProd2)), and the TVP residuals therefore change less over time (Figure \@ref(fig:FitsCompProd3) vs. Figure \@ref(fig:FitsCompProd2)). Note the trade-off between the time-trends in time-varying ln.alpha from the TVP model and the TVP residuals (Figure \@ref(fig:FitsCompProd1) vs. Figure \@ref(fig:FitsCompProd3)): For those stocks where the productivity parameter changes smoothly over time, the TVP residuals vary a lot from year to year (i.e., "spiky"), while for those stocks where the productivity parameter changes rapidly between years, the TVP residuals are much smoother. Depending on the stock, the observed variability in the data is allocated either to the time-varying productivity parameter or to the residual error.

For stocks where all three model forms could be fitted, the median posterior estimates of the productivity parameter ln.alpha are very similar between the basic Ricker and AR1 model, but brood-year specific ln.alpha estimates for the TVP model with time-varying productivity span a wide range, from much higher to much lower median estimates  than the basic Ricker and AR1 fits (Table \@ref(tab:ProdCompAcrossModels)).

Figures \@ref(fig:ScatterMezidian) to \@ref(fig:JoinPostBabLW) summarize the SR data and alternative model fits for the largest wild stock from each aggregate: Meziadin on the Nass and Babine Late Wild on the Skeena.

\clearpage
(ref:ProdCompAcrossModels) Comparison of median ln.alpha estimates across SR model forms. Table lists the number of available brood years with SR data (n), the median estimate for the basic Ricker model (BR), and median estimates (Med) and percent difference to the basic Ricker (pDiff) for the AR1 model fit, and the year-specific estimate for the brood years with  lowest and highest median estimate from the time-varying productivity model (TVP Min, TVP Max).

```{r ProdCompAcrossModels, echo = FALSE, results = "asis"}

table.in <- read_csv("data/FitsComparison_ReportTable_lnalpha.csv",
										 col_types = cols(.default = "c"))  %>%
							select(-contains("SIQR"),-SpnContr) %>%
							mutate_all(~ as.character(.x))
	

table.in[is.na(table.in)] <- "-"
table.in$Basin[duplicated(table.in$Basin)] <- ""


col.names.use <-  c("Basin","Stock","n","Med","Med","pDiff","Med","pDiff","Med","pDiff")



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l","l",rep("r",8)),
									col.names = col.names.use ,
                  caption = "(ref:ProdCompAcrossModels)" ) %>%
	 #kableExtra::column_spec(6, bold = T) %>%
     #kableExtra::row_spec(c(lines.idx), hline_after = TRUE) %>%
	add_header_above(c(" " = 3, "BR" = 1,"AR1" = 2,"TVP Min" = 2,"TVP Max" = 2))  

```







\clearpage

(ref:FitsCompProdCap) Comparison of Bayesian posterior distributions for productivity (ln.alpha) and capacity (Smax). Model fits shown are the Basic Ricker fits with capped uniform prior for Smax. Posterior distributions are summarized as the standardized interquartile range SIQR = (p75-p25)/p50. The reference line marks a slope of 2 (i.e., posterior for Smax is twice as wide relative to the median value as the posterior for ln.alpha). Model fits are flagged with red points if the SIQR for Smax is larger than 0.8 or the SIQR for ln.alpha is larger than 0.3. Johnston is labelled because it has the narrowest Smax posterior (lowest SIQR).

```{r FitsCompProdCap,   fig.cap="(ref:FitsCompProdCap)"}
include_graphics("data/Convergence/FitsComparison_ProdvsCapWidth.png")
```




\clearpage
(ref:FitsCompDetFig) Comparison of Bayesian and deterministic estimates for Smsy. Both estimates use the basic Ricker model form. The Bayesian estimate is the version with the capped uniform prior on capacity. Model fits are flagged with red points if the difference is larger than 25%. Panel A compares estimates on a log scale to allow comparison across stocks with very different estimates, but stocks are flagged if the difference in original (i.e., unlogged) values is larger than 25%. Bayesian and deterministic fits are mostly similar. Panel B shows the relationship between the % difference and the width of the Bayesian posterior distribution, expressed as the standardized interquartile range (SIQR). The difference between estimates increases with larger uncertainty in the Bayesian estimates (i.e., larger SIQR).

```{r FitsCompDetFig,   fig.cap="(ref:FitsCompDetFig)"}
include_graphics("data/Convergence/FitsComparison_BRcuVsDet.png")
```




\clearpage
(ref:FitsCompCapPrior) Effect of alternative capacity priors on median Smsy estimates. We tested two to four alternative priors for Smax for the stock-level and aggregate-level fits of the Basic Ricker model (wide vs. capped, uniform vs. lognormal). The effect of alternative capacity priors is larger for more uncertain SR model fits (i.e., larger SIQR). SR model fits with red points are flagged if the difference  between median estimates is larger than 25% or the standardized interquartile range for the most uncertain fit is larger than 1 (i.e., the range between the lower and upper quartiles is larger than the median value).

```{r FitsCompCapPrior,   fig.cap="(ref:FitsCompCapPrior)"}
include_graphics("data/Convergence/FitsComparison_CapPriorEffect.png")
```


\clearpage
(ref:FitsCompAR1Pars) Comparison of posterior distributions for Basic Ricker and AR1 model fits - Parameters. Both model fits used the capped uniform capacity prior. Panels compare the width of posterior distributions (SIQR) between model forms for ln.alpha (A) and Smax (B). SR model fits are flagged with red points for n.alpha if either SIQR > 0.3 and flagged for Smax if either SIQR > 0.6.


```{r FitsCompAR1Pars,   fig.cap="(ref:FitsCompAR1Pars)"}
include_graphics("data/Convergence/FitsComparison_BRcuVsAR1_Pars.png")
```


\clearpage
(ref:FitsCompAR1Smsy) Comparison of posterior distributions for Basic Ricker and AR1 model fits - Smsy. Figures compare median estimates (A) and width of the posteriors (B) of posterior Smsy distributions from the alternative model forms. Model fits are flagged with red points if the difference in median estimates is larger than 25%. Aggregate fits are also labelled in Panel B. Note that Panel A compares estimates on a log scale to allow comparison across stocks with very different estimates, but stocks are flagged if the difference in original (i.e., unlogged) values is larger than 25%.

```{r FitsCompAR1Smsy,   fig.cap="(ref:FitsCompAR1Smsy)"}
include_graphics("data/Convergence/FitsComparison_BRcuVsAR1_Smsy.png")
```



\clearpage
(ref:FitsCompProd1) Time-varying productivity for 12 stocks with complete time series. Each panel shows the median and 80% bounds of year-specific posterior distributions of ln.alpha for the TVP model fit. Reference lines show the corresponding intrinsic productivity in terms of recruits per spawner (R/S) at very low spawner abundance (technically, at 0 spawners). 

```{r FitsCompProd1,   fig.cap="(ref:FitsCompProd1)"}
include_graphics("data/Convergence/FitsComparison_PROD_3_ProdPatterns_KF_All.png")
```




\clearpage
(ref:FitsCompProd2) Log residuals from the Basic Ricker fit for 12 stocks where a time-varying productivity TVP model was also fitted. Each panel shows median and 80% bounds for annual residuals and a 4yr running average trend line.

```{r FitsCompProd2,   fig.cap="(ref:FitsCompProd2)"}
include_graphics("data/Convergence/FitsComparison_PROD_3_RickerResids_Basic_All.png")
```



\clearpage
(ref:FitsCompProd3) Log residuals from the TVP model fit with time varying productivity for 12 stocks with complete time series. Each panel shows median and 80% bounds for annual residuals and a 4yr running average trend line.

```{r FitsCompProd3,   fig.cap="(ref:FitsCompProd3)"}
include_graphics("data/Convergence/FitsComparison_PROD_3_RickerResids_KF_All.png")
```






\clearpage



(ref:ScatterMezidian) Scatterplot of log productivity ln(R/S) vs. spawner abundance - Meziadin. Observations are colour-coded, with earlier data in fainter shading. The secondary axis illustrates the corresponding raw R/S values. 

```{r ScatterMezidian,   fig.cap="(ref:ScatterMezidian)"}
include_graphics("data/StockSampleFigs/Meziadin_RpS_ScatterPlot.png")
```


(ref:JoinPostMezidian) Joint posterior distributions for productivity parameter ln(alpha) and capacity parameter Smax (1/b): Meziadin, with PR-based capacity priors. Each panel shows the scatter of MCMC samples, contour lines for the joint distribution, and two depictions of the marginal distributions for each parameter: boxplot with median, quartiles, and 80% whiskers, and a kernel density plot. Dashed reference lines show the medians of the marginal distributions. Figure includes 1 panel for the Basic Ricker model fit (top left), one panel for the Ricker AR1 fit (top right), and two panels showing the brood years with the lowest and highest productivity (median ln.alpha) for the TVP model fit (bottom panels). The parameter samples for the TVP model have 1 set of posterior samples for Smax, and 1 set of ln.alpha posterior samples for each brood year. The bottom panels show the joint distribution of Smax and ln.alpha for 2 brood years, selected to capture the brood year with the lowest median ln.alpha and the highest median ln.alpha.

```{r JoinPostMezidian,   fig.cap="(ref:JoinPostMezidian)"}
include_graphics("data/StockSampleFigs/Meziadin_SR_Par_JointPosterior_3Modelcu.png")
```



(ref:ScatterBabLW) Scatterplot of log productivity ln(R/S) vs. spawner abundance - Babine Late Wild. Observations are colour-coded, with earlier data in fainter shading. The secondary axis illustrates the corresponding raw R/S values.

```{r ScatterBabLW,   fig.cap="(ref:ScatterBabLW)"}
include_graphics("data/StockSampleFigs/BabLW_RpS_ScatterPlot.png")
```


(ref:JoinPostBabLW) Joint posterior distributions for ln.a and b: Babine Late Wild. Each panel shows the scatter of MCMC samples, contour lines for the joint distribution, and two depictions of the marginal distributions for each parameter: boxplot with median, quartiles, and 80% whiskers, and a kernel density plot. Dashed reference lines show the medians of the marginal distributions. Figure includes 1 panel for the Basic Ricker model fit (top left), one panel for the Ricker AR1 fit (top right), and two panels showing the brood years with the lowest and highest productivity (median ln.alpha) for the TVP model fit (bottom panels). The parameter samples for the TVP model have 1 set of posterior samples for Smax, and 1 set of ln.alpha posterior samples for each brood year. The bottom panels show the joint distribution of Smax and ln.alpha for 2 brood years, selected to capture the brood year with the lowest median ln.alpha and the highest median ln.alpha.

```{r JoinPostBabLW,   fig.cap="(ref:JoinPostBabLW)"}
include_graphics("data/StockSampleFigs/Bab-LW_SR_Par_JointPosterior_3Modelcu.png")
```



\clearpage
## HIERARCHICAL SPAWNER-RECRUIT MODEL FITS (STOCK-LEVEL) {#HBMResultsComp}

### Model Versions Used for Comparison

The purpose of including the HBM analyses by McAllister and Challenger in this paper is to explore the potential benefits of including the hierarchical structure and sharing information across stocks. For this comparison to single-stock fits, all other elements of the model fits should be kept as similar as possible, at least for the starting point of the comparison. McAllister and Challenger document sensitivity tests of the Hierarchical Bayesian Model (HBM) in Appendix \@ref(app:HBMFits). Here we focus on comparing results between the single-stock fits and two versions of the HBM model fits:

* *HBM Base Case*: Used all stocks with data, including enhanced Pinkut and Fulton, with a shared year effect of productivity across all stocks, and more informative lognormal priors on Smax for several of the stocks (CV = 0.3).
* *HBM Senstivity Run 26*: Excluded Pinkut and Fulton, less informative lognormal priors for Bear, Kitwanga, and Sustut (CV = 2).


### Comparison of Biological Benchmark Estimates

We compared HBM estimates of biological benchmarks to single stock fits with the Basic Ricker model (capped uniform Smax prior). This comparison covered the largest number of stocks, because single-stock AR1 and TVP model fits could only be applied to stocks with complete time series.  

For most stocks, median Smsy estimates from the HBM Base Case were similar to the single stock Basic Ricker estimates, but for six stocks the difference was more than 25% (Figure \@ref(fig:HBMComp1)). After excluding the enhanced stocks and relaxing the HBM capacity prior on three stocks (Bear, Kitwanga, Sustut), all the estimates were similar between HBM and single-stock model fits (Figure \@ref(fig:HBMComp2)). In fact, Smsy estimates for HBM Run 26 and the single-stock Basic Ricker with capped uniform prior (Model BRcu) were more similar to each other than the Bayesian single stock estimate was to the simple deterministic estimate (Figure \@ref(fig:FitsCompDetFig)). 

Widths of posterior Smsy distributions were similar between HBM and single-stock model fits for many stocks, tighter (i.e., more precise) in the HBM estimates for some stocks, and wider in the HBM estimates for a few stocks (Figure \@ref(fig:HBMComp3)). Smsy estimates for three stocks stood out in both HBM and single-stock model fits as particularly uncertain (i.e., very wide posteriors): Kitwanga, Asitka, and Motase. 

While Smsy estimates were much more similar between HBM and single-stock model fits after the capacity prior was relaxed for some stocks (HBM Run 26), median estimates of Umsy remained quite different between the two types of model (Figure \@ref(fig:HBMComp4)). Median Umsy differed by more than 5% for seven stocks. HBM Run 26 estimated higher median Umsy (i.e., higher productivity) than the single stock Basic Ricker fit for Johnston, Asitka, Kitwanga, and Swan/Stephens. HBM Run 26 estimated lower median Umsy for Sustut, Bear, and Kitsumkalum. The range of median Umsy estimates across stocks was narrower for HBM Run 26 (44%-72%) than for the single stock Basic Ricker fit (41%-79%).



\clearpage
(ref:HBMComp1) Difference in median Smsy estimates for the HBM Base Case and the single stock Basic Ricker fit with capped uniform capacity priors. Stocks with differences larger than 25% are highlighted.

```{r HBMComp1,   fig.cap="(ref:HBMComp1)"}
include_graphics("data/HBM/FitsComparisonHBM_BRcuvsBaseCase_Smsy_PercDiff.png")
```



(ref:HBMComp2) Difference in median Smsy estimates for HBM Run 26 and the single stock Basic Ricker fit with capped uniform capacity priors. Stocks with differences larger than 25% are highlighted.

```{r HBMComp2,   fig.cap="(ref:HBMComp2)"}
include_graphics("data/HBM/FitsComparisonHBM_BRcuvsSensitivityRun26_Smsy_PercDiff.png")
```

\clearpage

(ref:HBMComp3) Comparison of Smsy estimates for HBM Run 26 and the single stock Basic Ricker fit with capped uniform capacity priors. (A) Comparison of median estimates. Stocks with differences in median estimates larger than 25% are highlighted. Panel A compares estimates on a log scale to allow comparison across stocks with very different estimates, but stocks are flagged if the difference in original (i.e., unlogged) values is larger than 25%. (B) Comparison of the standardized interquartile range (SIQR), which captures half of the posterior samples. Stocks with SIQR > 0.4 in either estimate are highlighted.

```{r HBMComp3,   fig.cap="(ref:HBMComp3)"}
include_graphics("data/HBM/FitsComparisonHBM_BRcuvsSensitivityRun26_Smsy.png")
```



(ref:HBMComp4) Comparison of Umsy estimates for HBM Run 26 and the single stock Basic Ricker fit with capped uniform capacity priors. Sidebars identify the range of median estimates from each model type. Stocks are highlighted if median Umsy differ by more than 5%. Note that the difference in actual values is used here, not the relative % difference used for Smsy comparisons in previous plots.

```{r HBMComp4,   fig.cap="(ref:HBMComp4)"}
include_graphics("data/HBM/FitsComparisonHBM_BRcuvsSensitivityRun26_Umsy.png")
```


\clearpage
### Comparison of eEstimated Productivity Changes Over Time (Single-stock vs. HBM Fits)

We compared HBM estimates of productivity over time to single stock fits with the TVP model (capped uniform Smax prior) for the nine wild Skeena stocks where both estimates were available. 

The shared year effect identified by the HBM model was very similar between the HBM Base Case, which included Pinkut and Fulton, and HBM Run 26, which excluded the enhanced stocks (Figure \@ref(fig:HBMComp5), Panel A). Both versions identified a sharp productivity drop in 1994, followed by a spike in 1995. The 4-year running mean of the HBM Base Case shared year effect identified a period of higher-than-average productivity in the 1980s and early 1990s, followed by a general decline since then (Panel B). Single stock TVP fits identified a similar decline in productivity since the 1990s for Babine Late Wild, the largest wild Skeena stock,  but other stocks had very different productivity trends over time (e.g., Panel D of Figure \@ref(fig:HBMComp5), Figure \@ref(fig:HBMComp6)). The three wild Babine stocks had very similar productivity trend over time in the single stock TVP fits. 

Both the HBM shared year effect and the single-stock TVP estimates of changes in productivity over time link back to the residuals from the Basic Ricker fit. Based on the residuals, the  1994/1995 dip and spike in the HBM shared year effect was driven by the residuals from the Babine stocks (Figure \@ref(fig:HBMComp6)). In the HBM Base Case the change over time was very pronounced, because it included Pinkut and Fulton, which had the disease outbreak. In HBM Run 26, without the enhanced stocks, this was less pronounced, but it still picked up the same signal from the 3 wild Babine stock residuals, and assumed that all the non-Babine stocks experienced the same dramatic changes in those 2 years. The residual 1994/1995 signal from the three wild Babine stocks needs to be interpreted carefully, because spawner and recruit estimates for all five Babine stocks are derived together from a total weir count, based on estimated stock proportions from tagging studies in the 1970s.


(ref:HBMComp5) Comparison of estimated productivity trends over time from HBM and single stock TVP model fits. (A) Annual shared year effect from the HBM model fits. Blue line and shaded area show median and 80% of the shared year effect from the HBM Base Case, the red overlayed line shows the shared year effect from HBM Run 26. (B) 4yr running mean of the shared year effect from the HBM Base Case. (C, D) Estimates of the time-varying productivity parameter ln.alpha from a single stock TVP model fit with capped uniform capacity prior for Babine Late Wild and Alastair. Medians and 80% bounds are shown.

```{r HBMComp5,   fig.cap="(ref:HBMComp5)"}
include_graphics("data/HBM/FitsComparison_PROD_1_SinglevsHBM.png")
```


(ref:HBMComp6) Changes in productivity parameter ln.alpha over time for nine wild Skeena stocks with complete SR time series. Estimates of the time-varying productivity parameter ln.alpha from a single stock TVP model fit with capped uniform capacity prior. Medians and 80% bounds are shown. 

```{r HBMComp6,   fig.cap="(ref:HBMComp6)"}
include_graphics("data/HBM/FitsComparison_PROD_2_ProdPatterns_KF_Skeena.png")
```




\clearpage
##  PRODUCTIVITY SCENARIOS (STOCK-LEVEL AND AGGREGATE-LEVEL)

### Scenario Descriptions

We worked through the considerations and steps outlined in Section \@ref(ModelSelection) to select spawner-recruit parameter sets for four alternative productivity scenarios from the suite of candidate single-stock SR model fits.  These scenarios were used for the remaining analyses in this report (Tables \@ref(tab:SelectedModelsTab) and \@ref(tab:SelectedModelsTabAgg)). 

Key considerations were:

* We wanted to highlight differences in productivity between stocks, so focused on single-stock model fits, rather than the hierarchical model fits that start with the assumption of a similar underlying productivity across stocks.
* Given that all candidate single-stock model fits converged for at least two of the three criteria (Section \@ref(Convergence)), we did not reject any of the fits based on statistical considerations.
* We wanted to highlight the implications of alternative productivity assumptions for each stock, so we generated three alternatives to the base case with long-term average productivity. 
* Where available, we used the AR1 model fit for the long-term average scenario. Otherwise, we used the Basic Ricker fit.
* Where available, we used year-specific ln.alpha estimates from the time-varying productivity (TVP) model for the alternative productivity scenarios. Otherwise, we subsampled parameter sets from the Basic Ricker fit.
* For stocks with time-varying productivity (TVP) model fits, we used the last available generation for the *Recent* productivity scenario. However, given discussions during the peer-review meeting [@SkeenaNassSkPRO;@SkeenaNassSkSAR], we also tested the effect of expanding the number of brood-years included in the *Recent* scenario to either two generations or 3 generations.

We anticipate that future processes, such as the on-going Canadian domestic engagement process, will identify additional candidate model fits,  explore alternative parameter selection criteria, and request additional productivity scenarios.  The analysis framework is set up to rapidly respond to these requests. All of the results shown in subsequent sections can be  easily re-generated with alternative MCMC parameter sets (e.g., different productivity assumptions, Hierarchical Bayesian Model fits), but we have decided to limit the number of examples included in this Research Document. 

\clearpage


(ref:SelectedModelsTab) Stock-level SR models selected for alternative scenarios.  Model fits are BR = Basic Ricker, AR1 = Ricker with lag-1 autoregression correction, TVP = Ricker model with time-varying productivity parameter alpha. All selected model fits used the capped uniform (*cu*) capacity prior. The range of brood years from which parameter sets were sampled is listed for TVP fits. For example, TVPcu2010-2014 denotes that 1/5th of the parameter samples were taken from each year in the period 2010 to 2014. For BR and AR1 fits, alternative scenarios are identified based on the percentile used for the adjusted median of the sample distribution. For example, BRcu(0.1) denotes that half the parameter samples for that scenario were taken from below the 10th percentile of the original distribution, and half from above. For stocks without TVP fits the recent productivity scenario matched the low productivity scenario if observed R/S clearly decreased in recent years, matched the high productivity scenarios if they increased, and matched the long-term average scenario if there was no clear trend in either direction. Note that three alternative versions of *Recent* productivity scenario for stocks with TVP fits were explored. All use the model fit, generation length, and end year specified in this table, but they include either one, two, or three generations. Section \@ref(AltProdResults) compares the parameter distributions. The examples in the rest of this Research Document are based on the 1-generation version of the *Recent* productivity scenario.

```{r SelectedModelsTab, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_SelectedModels.csv") %>% arrange(Aggregate,StkSeq) %>%
									select(-StkSeq,-Stock) %>% dplyr::filter(!is.na(Aggregate)) %>%
									dplyr::rename(Stock = StkNmS,Recent = Now,MU=Aggregate) %>%
									mutate_all(funs(str_replace(., "KF","TVP")))




table.in$MU[duplicated(table.in$MU)] <- ""

table.in[is.na(table.in)] <- "-"


table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "l",
                  caption = "(ref:SelectedModelsTab)" ) %>%
    kableExtra::row_spec(c(7,9), hline_after = TRUE) %>%
	add_header_above(c(" " = 2, "Scenarios" = 4)) 

```




\clearpage

(ref:SelectedModelsTabAgg) Aggregate-level SR models selected for alternative scenarios. Layout as per Table \@ref(tab:SelectedModelsTab). 

```{r SelectedModelsTabAgg, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_SelectedModelsAgg.csv") %>%			
	dplyr::rename(Recent = Now) %>%
									mutate_all(funs(str_replace(., "KF","TVP")))

table.in[is.na(table.in)] <- "-"




table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "l",
                  caption = "(ref:SelectedModelsTabAgg)" ) %>%
    #kableExtra::row_spec(c(7,9), hline_after = TRUE) %>%
	add_header_above(c(" " = 1, "Scenarios" = 4)) 

```



### Comparison of Parameter Distributions for Productivity Scenarios {#AltProdResults}

Productivity parameters for the *recent* scenario, generated as per Table \@ref(tab:SelectedModelsTab), have lower median productivity for about 1/3 of the stocks, and wider distributions (i.e., larger uncertainty, lower confidence) for most of the stocks (Table \@ref(tab:ProdCompTab1), Figures \@ref(fig:ProdComp1) and \@ref(fig:ProdComp2)). Stocks with lower productivity in the *Recent* scenario include Meziadin, the largest Nass stock, and all three wild Babine stocks (Early, Mid, and Late Wild). Four small stocks, each accounting for less than 2% of cumulative spawner abundance since 2000, have higher *Recent* productivity: Alastair, Bear, Slamgeesh, and Johnston. Three stocks are notable for having both much lower productivity, with a % difference in median ln.alpha between scenarios larger than -35%, and a distribution that is more than twice as wide (i.e., % difference in SIQR > 100): Babine Late Wild, Babine Early Wild, Meziadin, and Kitwanga (Figure \@ref(fig:ProdComp2)).


Alternative versions of the *recent* productivity scenario, using either two or three generations instead of only the last generation, have similar median productivity and similar spread for about 1/2 of the stocks (Table \@ref(tab:ProdCompTab2), Figures \@ref(fig:ProdComp3) and \@ref(fig:ProdComp3)). This includes two of the three wild Babine stocks (Mid and Late Wild). For 1/3 of the stocks, median productivity is higher, and the distribution is narrower if additional brood years are included in the definition of the *recent* scenario. These include Meziadin, the largest Nass stock, and one of the wild Babine stocks (Babine Early Wild). For two stocks, median productivity is lower and more uncertain: Morice and Swan/Stephens. Corresponding Smsy estimates change similarly due to the alternative time windows used for the recent productivity scenario (Table \@ref(tab:ProdCompBMRatioSmsy)).






\clearpage
(ref:ProdCompTab1) Estimated productivity parameters ln.alpha for the long-term average and recent productivity scenarios. For each scenario, the table lists median (Med) and standardized inter-quartile range (SIQR, range between p25 and p75) for the Bayesian posterior parameter subsamples generated as per Table \@ref(tab:SelectedModelsTab). The last two columns show percent difference (*PercDiff*) between the scenarios. Stocks are sorted by *PercDiff* in median estimate. Horizontal lines separate the stocks into three groups based on percent difference in median ln.alpha: more than ~ 10% decrease (top), more than ~ 10% increase  (bottom), or less than ~ 10% change in either direction (middle). Note that table includes only wild stocks.

```{r ProdCompTab1, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_ProdChange.csv") %>%		
	dplyr::filter(!(Stock %in% c("Skeena","SkeenaWild","Nass","Pinkut","Fulton"))) %>%
	dplyr::select(Stock,Median.LTAvg,SIQR.LTAvg,Median.Curr,SIQR.Curr,
								Median.PercChange,	SIQR.PercChange) %>%
	arrange(Median.PercChange) %>%
	mutate_if(is.numeric, round,2)
	

lines.idx <- c( max(which(table.in$Median.PercChange < -9.5)),
								min(which(table.in$Median.PercChange > 9.5))-1)

	
table.in$Stock <- gsub("Lower Nass Sea & River Type","L Nass SRT",table.in$Stock)

table.in[is.na(table.in)] <- "-"


col.names.use <-  c("Stock",rep(c("Med","SIQR"),3))



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
									col.names = col.names.use ,
                  caption = "(ref:ProdCompTab1)" ) %>%
	 kableExtra::column_spec(6, bold = T) %>%
    kableExtra::row_spec(c(lines.idx), hline_after = TRUE) %>%
	add_header_above(c(" " = 1, "LTAvg" = 2,"Recent" = 2,"PercDiff" = 2)) 

```




\clearpage

(ref:ProdComp1) Estimated median productivity (ln.alpha) for the long-term average and recent productivity scenarios. Medians are for the Bayesian posterior parameter subsamples generated as per Table \@ref(tab:SelectedModelsTab). Stocks falling on the solid red line have the same median for both scenarios. For stocks below the solid red line, recent productivity is lower than long-term average productivity. For stocks between the dashed red lines, the difference in productivity scenarios is less than 25%. Note that figure includes only wild stocks.

```{r ProdComp1,   fig.cap="(ref:ProdComp1)"}
include_graphics("data/RecentProd_Comparison_LTavgvs1genv_Values.png")
```




\clearpage

(ref:ProdComp2) Differences in median and spread of the productivity parameter (ln.alpha) for the long-term average and recent productivity scenarios. Points show the percent change in the standardized inter-quartile range (SIQR, range between p25 and p75) vs. the change in median ln.alpha. Horizontal and vertical red lines mark "no change". Note that figure includes only wild stocks.


```{r ProdComp2,   fig.cap="(ref:ProdComp2)"}
include_graphics("data/RecentProd_Comparison_LTavgvs1gen_Diffs.png")
```




\clearpage
(ref:ProdCompTab2) Estimated productivity parameters ln.alpha for three versions of the recent productivity scenario (1,2, or 3 generations). For each scenario, the table lists median (Med) and standardized inter-quartile range (SIQR, range between p25 and p75) for the Bayesian posterior parameter subsamples generated as per Table \@ref(tab:SelectedModelsTab). Table also shows percent difference (*PercDiff*) between the alternative versions. Stocks are sorted by *PercDiff* in median estimate between the 2-generation and 1-generation versions. Horizontal lines separate the stocks into three groups based on percent difference in median ln.alpha between the 2-generation and 1-generation versions: more than ~ 5% decrease (top), more than ~ 5% increase  (bottom), or less than ~ 5% change in either direction (middle). Note that table includes only wild stocks. Table \@ref(tab:ProdCompBMRatioSmsy) compares the corresponding Smsy estimates.

```{r ProdCompTab2, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_ProdChange.csv") %>%		
	dplyr::filter(!(Stock %in% c("Skeena","SkeenaWild","Nass","Pinkut","Fulton"))) %>%
	dplyr::select(Stock,Median.CurrLS,SIQR.CurrLS,Median.CurrLS2,SIQR.CurrLS2,
								Median.PercChange2,	SIQR.PercChange2,
								Median.CurrLS3,SIQR.CurrLS3,
								Median.PercChange3,	SIQR.PercChange3) %>%
	arrange(Median.PercChange2) %>%
	mutate_if(is.numeric, round,2)
	
	
table.in$Stock <- gsub("Lower Nass Sea & River Type","L Nass SRT",table.in$Stock)

table.in[is.na(table.in)] <- "-"


lines.idx <- c( max(which(table.in$Median.PercChange2 < -4.99)),
								min(which(table.in$Median.PercChange2 > 4.99))-1)

	

col.names.use <-  c("Stock",rep(c("Med","SIQR"),5))



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
									col.names = col.names.use ,
                  caption = "(ref:ProdCompTab2)" ) %>%
	 kableExtra::column_spec(6, bold = T) %>%
     kableExtra::row_spec(c(lines.idx), hline_after = TRUE) %>%
	add_header_above(c(" " = 1, "1 Gen" = 2,"2 Gen" = 2,"PercDiff\n(2 Gen vs 1 Gen)" = 2,
										 "3 Gen" = 2,"PercDiff\n(3 Gen vs 1 Gen)" = 2)) 

```





\clearpage
(ref:ProdCompBMRatioSmsy) Comparison of Median Smsy estimates across productivity scenarios. Layout as per Table \@ref(tab:ProdCompTab2). Median Smsy estimates are in 1,000s (columns 2,4,and 8).

```{r ProdCompBMRatioSmsy, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ReportTable_SmsyChange.csv") %>%		
	dplyr::filter(!(Stock %in% c("Skeena","SkeenaWild","Nass","Pinkut","Fulton"))) %>%
	dplyr::select(Stock,Median.CurrLS,SIQR.CurrLS,Median.CurrLS2,SIQR.CurrLS2,
								Median.PercChange2,	SIQR.PercChange2,
								Median.CurrLS3,SIQR.CurrLS3,
								Median.PercChange3,	SIQR.PercChange3) %>%
	arrange(Median.PercChange2) %>%
	mutate_if(is.numeric, round,2)
	
	
table.in$Stock <- gsub("Lower Nass Sea & River Type","L Nass SRT",table.in$Stock)

table.in[is.na(table.in)] <- "-"


lines.idx <- c( max(which(table.in$Median.PercChange2 < -4.99)),
								min(which(table.in$Median.PercChange2 > 4.99))-1)

	

col.names.use <-  c("Stock",rep(c("Med","SIQR"),5))



table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
									col.names = col.names.use ,
                  caption = "(ref:ProdCompBMRatioSmsy)" ) %>%
	 kableExtra::column_spec(6, bold = T) %>%
     kableExtra::row_spec(c(lines.idx), hline_after = TRUE) %>%
	add_header_above(c(" " = 1, "1 Gen" = 2,"2 Gen" = 2,"PercDiff\n(2 Gen vs 1 Gen)" = 2,
										 "3 Gen" = 2,"PercDiff\n(3 Gen vs 1 Gen)" = 2)) 

```







\clearpage

(ref:ProdComp3) Estimated median productivity (ln.alpha) for the 1-generation and 2-generation versions of the recent productivity scenario. Medians are for the Bayesian posterior parameter subsamples generated as per Table \@ref(tab:SelectedModelsTab). Stocks falling on the solid red line have the same median for both scenarios. For stocks below the solid red line, the 1-generation version has lower productivity than the 2-generation version. For stocks between the dashed red lines, the difference between alternative versions is less than 25%. Note that figure includes only wild stocks.

```{r ProdComp3,   fig.cap="(ref:ProdComp3)"}
include_graphics("data/RecentProd_Comparison_1genvs2Gen_Values.png")
```



\clearpage

(ref:ProdComp4) Differences in median and spread of the productivity parameter (ln.alpha) for the  1-generation and 2-generation versions of the recent productivity scenario. Points show the percent change in the standardized inter-quartile range (SIQR, range between p25 and p75) vs. the change in median ln.alpha. Horizontal and vertical red lines mark "no change". Note that figure includes only wild stocks.

```{r ProdComp4,   fig.cap="(ref:ProdComp4)"}
include_graphics("data/RecentProd_Comparison_1genvs2Gen_Diffs.png")
```


\clearpage
##  BIOLOGICAL BENCHMARK ESTIMATES (STOCK-LEVEL AND AGGREGATE-LEVEL) {#BMResults}


### Illustration of Stock-level Results: Meziadin and Babine Late Wild

Standard biological benchmarks for the largest wild stock from each aggregate differed substantially between alternative productivity scenarios. 

Comparing the recent productivity scenario to the long-term average  productivity scenario for Meziadin (Figure \@ref(fig:BMMezidian)) and Babine Late Wild (Figure \@ref(fig:BMBabLW)):

* The recent productivity scenario had  lower and more uncertain estimates of productivity (ln.alpha) and lower estimates of capacity (Smax), which resulted in lower estimates of Smsy, Seq, and Umsy. 
* Estimates of Sgen, which is linked to both the productivity estimate and the value of Smsy, increased for Meziadin and decreased for Babine Late Wild, but note that in both cases Sgen increased as a relative proportion of Smsy (i.e., with lower productivity need more spawners to build back to Smsy in one generation, but if Smsy is much lower, then Sgen can actually drop relative to the long-term average scenario).


(ref:BMMezidian) Posterior distributions of productivity parameter ln.alpha and biological benchmark estimates: Meziadin. Each panel shows the posterior distribution (median, quartiles, 80% bounds) for two productivity scenarios: long-term average (LTAvg) and recent productivity. Two versions of the parameter estimates are shown: regular (R) and with log-normal bias correction (C) on the productivity parameter ln.alpha.

```{r BMMezidian,   fig.cap="(ref:BMMezidian)"}
include_graphics("data/StockSampleFigs/Meziadin_BM_SummaryPlot.png")
```



\clearpage
(ref:BMBabLW) Posterior distributions of biological benchmark estimates: Babine Late Wild. Layout as per Figure \@ref(fig:BMMezidian).

```{r BMBabLW,   fig.cap="(ref:BMBabLW)"}
include_graphics("data/StockSampleFigs/Bab-LW_BM_SummaryPlot.png")
```

\clearpage
### Stock-level and Aggregate-level Biological Benchmarks for Nass Sockeye  

*Abundance benchmarks*

Comparing aggregate to stock-level Smsy estimates for Nass Sockeye under long-term average productivity (Table \@ref(tab:SmsyLtAvgNass)):

* The aggregate-level Smsy estimate for Nass Sockeye was substantially larger than the sum of stock-level estimates for those stocks where SR models were fitted. The four stocks with stock-level estimates are assumed to account for most of the Sockeye production from the Nass, but see notes regarding Bowser in Section \@ref(PrioritiesFuture). 

Comparing aggregate to stock-level Smsy estimates for Nass Sockeye under recent productivity (Table \@ref(tab:SmsyRecentNass)):

* The aggregate-level Smsy estimate was larger than the sum of stock-level estimates for four modelled stocks, but the difference was smaller than under the long-term average productivity scenario.

Comparing aggregate and stock-level benchmark estimates under recent productivity to the long-term average productivity scenario  (Table \@ref(tab:SmsyLtAvgNass) vs. Table \@ref(tab:SmsyRecentNass), Table \@ref(tab:SmaxLtAvgNass) vs. Table \@ref(tab:SmaxRecentNass), Table \@ref(tab:SgenLtAvgNass) vs. Table \@ref(tab:SgenRecentNass)):

* The aggregate estimate, and the Meziadin Smsy estimate are much lower under recent productivity. 
* The Smsy estimate for the second-largest stock, Lower Nass Sea and River Type Sockeye, also dropped under recent productivity, but much less.  
* The same general differences were observed for estimates of Smax. 
* Under the recent productivity scenario, Sgen estimates increased for Meziadin and Kwinageese and decreased for the other two stocks with SR data, so that there was little change in the sum of Sgen estimates due to alternative productivity assumptions. 


Sgen is not applicable for stock aggregates, so we did not include a comparison between aggregate and stock-level Sgen estimates. 

*Umsy*

Aggregate-level estimates of Umsy closely matched the median of stock-level median estimates under both productivity scenarios, but differed from the median Umsy estimate for Meziadin, the largest stock (Tables \@ref(tab:UmsyLtAvgNass) and \@ref(tab:UmsyRecentNass)). Under the recent productivity scenario, Umsy for Meziadin is the lowest among the 4 modelled Nass stocks and about 10% lower than the aggregate fit (55% for the aggregate, 45% for Meziadin).


\clearpage

(ref:SmsyLtAvgNass) Comparison of aggregate and stock-level Smsy estimates: Nass / Long-term average productivity. Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit. 

```{r SmsyLtAvgNass, echo = FALSE, results = "asis"}

abd.bm.tab.src <- read_csv("data/SummaryTables_AbundanceBM.csv") %>% select(-Stock)

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Smsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyLtAvgNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SmsyRecentNass) Comparison of aggregate and stock-level Smsy estimates: Nass / Recent productivity. Stocks are sorted based on median estimate. 

```{r SmsyRecentNass, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Smsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyRecentNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```




\clearpage

(ref:SmaxLtAvgNass) Comparison of aggregate and stock-level Smax estimates: Nass / Long-term average productivity.  Stocks are sorted based on median estimate. 

```{r SmaxLtAvgNass, echo = FALSE, results = "asis"}



table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Smax") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmaxLtAvgNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SmaxRecentNass) Comparison of aggregate and stock-level Smax estimates: Nass / Recent productivity.  Stocks are sorted based on median estimate. 

```{r SmaxRecentNass, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Smax") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmaxRecentNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```






\clearpage

(ref:SgenLtAvgNass) Comparison of aggregate and stock-level Sgen estimates: Nass / Long-term average productivity.  Stocks are sorted based on median estimate. 

```{r SgenLtAvgNass, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Sgen") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenLtAvgNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SgenRecentNass) Comparison of aggregate and stock-level Sgen estimates: Nass / Recent productivity.  Stocks are sorted based on median estimate. 

```{r SgenRecentNass, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Sgen") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenRecentNass)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



\clearpage

(ref:UmsyLtAvgNass) Comparison of aggregate and stock-level Umsy estimates: Nass / Long-term average productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyLtAvgNass, echo = FALSE, results = "asis"}

umsy.bm.tab.src <- read_csv("data/SummaryTables_UMSY.csv") %>% select(-Stock)

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Umsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"


table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyLtAvgNass)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```



(ref:UmsyRecentNass) Comparison of aggregate and stock-level Umsy estimates: Nass / Recent productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyRecentNass, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Umsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyRecentNass)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```





\clearpage
### Stock-level and Aggregate-level Biological Benchmarks for Skeena Wild Sockeye

*Abundance benchmarks*

Comparing aggregate to stock-level Smsy estimates for Skeena Sockeye under long-term average productivity (Table \@ref(tab:SmsyLtAvgSkeenaWild)):

* The aggregate-level Smsy estimate for Skeena Sockeye was lower than the sum of stock-level estimates for those stocks where SR models were fitted. The 16 stocks with stock-level estimates are assumed to account for most of the Sockeye production from the Skeena. 

Comparing aggregate to stock-level Smsy estimates for Skeena Sockeye under recent productivity (Table \@ref(tab:SmsyRecentNass)):

* The aggregate-level Smsy estimate was much lower than the sum of stock-level estimates for 16 modelled stocks, and the difference was larger than under the long-term average productivity scenario.

Comparing aggregate and stock-level benchmark estimates under recent productivity to the long-term average productivity scenario  (Table \@ref(tab:SmsyLtAvgSkeenaWild) vs. Table \@ref(tab:SmsyRecentSkeenaWild), Table \@ref(tab:SmaxLtAvgSkeenaWild) vs. Table \@ref(tab:SmaxRecentSkeenaWild), Table \@ref(tab:SgenLtAvgSkeenaWild) vs. Table \@ref(tab:SgenRecentSkeenaWild)):

* The aggregate estimate, and the Babine Late Wild Smsy estimate, are much lower under recent productivity. 
* The same general differences were observed for estimates of Smax.
* Under the recent productivity scenario, Sgen estimates decreased for most of the stocks, and the sum of Sgen estimates decreased substantially. 

Sgen is not applicable for stock aggregates, so we did not include a comparison between aggregate and stock-level Sgen estimates. 

*Umsy*

Aggregate-level estimates of Umsy differed substantially from the median of stock-level median estimates under both productivity scenarios (Tables \@ref(tab:UmsyLtAvgSkeenaWild) and \@ref(tab:UmsyRecentSkeenaWild)). The median of stock-level estimates was much higher, because large stocks and small stocks were weighted equally in the median calculcation. Aggregate Umsy estimates were closer to the Umsy estimates for the largest stocks (Babine Early, Mid, and Late Wild).



\clearpage

(ref:SmsyLtAvgSkeenaWild) Comparison of aggregate and stock-level Smsy estimates: SkeenaWild / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyLtAvgSkeenaWild, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Smsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyLtAvgSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
(ref:SmsyRecentSkeenaWild) Comparison of aggregate and stock-level Smsy estimates: SkeenaWild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyRecentSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Smsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyRecentSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```




\clearpage

(ref:SmaxLtAvgSkeenaWild) Comparison of aggregate and stock-level Smax estimates: SkeenaWild / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmaxLtAvgSkeenaWild, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Smax") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmaxLtAvgSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
(ref:SmaxRecentSkeenaWild) Comparison of aggregate and stock-level Smax estimates: SkeenaWild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmaxRecentSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Smax") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmaxRecentSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```






\clearpage

(ref:SgenLtAvgSkeenaWild) Comparison of aggregate and stock-level Sgen estimates: SkeenaWild / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SgenLtAvgSkeenaWild, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Sgen") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenLtAvgSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```

\clearpage

(ref:SgenRecentSkeenaWild) Comparison of aggregate and stock-level Sgen estimates: SkeenaWild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added.  

```{r SgenRecentSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Sgen") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenRecentSkeenaWild)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```




\clearpage

(ref:UmsyLtAvgSkeenaWild) Comparison of aggregate and stock-level Umsy estimates: Skeena Wild / Long-term average productivity.  Table also lists the range and median across stock-level estimates. 

```{r UmsyLtAvgSkeenaWild, echo = FALSE, results = "asis"}


table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Umsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyLtAvgSkeenaWild)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```

\clearpage

(ref:UmsyRecentSkeenaWild) Comparison of aggregate and stock-level Umsy estimates: Skeena Wild / Recent productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyRecentSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Umsy") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyRecentSkeenaWild)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```




\clearpage
## COMPARISON OF SUSTAINABLE EXPLOITATION RATE ACROSS STOCKS AND SCENARIOS {#UmsyComp}

Observed differences in estimated productivity (ln.alpha) between stocks and between scenarios translate into large differences in estimates of Umsy, defined as the sustainable harvest mortality rate at MSY (Figure \@ref(fig:AggUmsy)). Aggregate-level Umsy estimates were higher for the Nass aggregate than for the Skeena Wild aggregate, and Umsy under recent productivity was much lower than under long-term average productivity for both aggregates. Umsy estimates varied substantially between the component stocks for each aggregate. The SkeenaWild aggregate includes more stocks than the Nass aggregate, and median stock-level Umsy estimates spanned a wider range. Umsy under recent productivity was lower  and more uncertain for most stocks. The ranking of stocks by productictivity also differed between the long-term average and recent productivity scenario. Tables \@ref(tab:UmsyLtAvgNass), \@ref(tab:UmsyRecentNass), \@ref(tab:UmsyLtAvgSkeenaWild), and \@ref(tab:UmsyRecentSkeenaWild) above list the corresponding estimates. Appendix \@ref(BiasCorrectedBM) lists the bias-corrected versions of the estimates.

Umsy distributions can be considered relative to different levels of target exploitation rate for a stock aggregate:

* Figure \@ref(fig:UmsyProfiles1) shows the Prop(ER < Umsy), the proportion of the posterior Umsy distribution for each stock which exceeds ERs from 0 to 100% (i.e., the probability that a particular aggregate ER is sustainable over the long run for each component stock). This is the same information as Figure \@ref(fig:AggUmsy), just expressed in a different way: this version shows how much of the boxplots in Figure \@ref(fig:AggUmsy) falls to the right of each ER level. At 40% ER applied over the long run, two wild stocks have less than 50% probability of being harvested sustainably (two lines below the horizontal red line) under the long-term average productivity scenario, which increases to four wild stocks under the recent productivity scenario.
* Figure \@ref(fig:UmsyProfiles2) summarizes this information across stocks. It shows how many stocks have at least 50% probability of being sustainably harvested at each ER. At 40% ER, two wild stocks don't meet this objective under the long-term average productivity scenario (18 of 20 stocks do meet it), which increases to four wild stocks under the recent productivity scenario (16 of 20 wild stocks = 80%  of stocks meet the objective).
* Figures \@ref(fig:UmsyHeatmapLTAvg) and \@ref(fig:UmsyHeatmapRecent) show the details by stock. At 40% ER, the two stocks with less than 50% probability of meeting this objective under the long-term average productivity scenario are Kitwanga and Swan/Stephens. Under the recent productivity scenario, the list of stocks not meeting this objective also includes Babine Early Wild and Babine Late Wild.
* Figure  \@ref(fig:UmsyHisto) shows the frequency distribution of stock-specific median Umsy estimates. This type of plot was presented by @Waltersetal2008ISRP in their review of Skeena Sockeye (their Figure 14), so we've included it here for comparison. For Skeena Wild,  the spread of median Umsy estimates across component stocks is wider under the recent productivity scenario (i.e., the mixed-stock fishery challenge is more pronounced).  For Nass, there is an overall shift to lower median Umsy for the component stocks.

The different types of plots in Figures \@ref(fig:AggUmsy) to \@ref(fig:UmsyHisto) all show the same underlying information, just presented differently. However, these alternative displays capture alternative perspectives, and no single version will be informative for all participants in a planning process.

\clearpage
(ref:AggUmsy) Comparison of aggregate and stock-level Umsy estimates across stocks and aggregates for two alternative productivity assumptions. Stocks within an aggregate are sorted based on median Umsy. The largest stock in each aggregate is highlighted with red horizontal lines.

```{r AggUmsy,   fig.cap="(ref:AggUmsy)"}
include_graphics("data/UmsyPlots/Agg_Umsy_Comparison.png")
```


\clearpage
(ref:UmsyProfiles1) Probability profiles of sustainable ER. Proportion of Bayesian posterior Umsy estimates that exceeds different levels of ER for 20 wild stocks. Red horizontal line marks 50% probability. The ER where a line for a stock crosses the red line corresponds to the median estimate of Umsy. The largest stock from each aggregate is highlighted. 

```{r UmsyProfiles1,   fig.cap="(ref:UmsyProfiles1)"}
include_graphics("data/UmsyPlots/ResDoc_Umsy_Profile_Figure.png")
```


\clearpage
(ref:UmsyProfiles2) Summary of sustainable ER across 20 wild stocks. Number of stocks with at least 50% probability of being harvested sustainably.

```{r UmsyProfiles2,   fig.cap="(ref:UmsyProfiles2)"}
include_graphics("data/UmsyPlots/ResDoc_Umsy_Profile_Figure2.png")
```



\clearpage
(ref:UmsyHeatmapLTAvg) Umsy comparison across stocks - Long-term average productivity. Each column shows estimates for a fixed aggregate ER rate. Each cell in the table shows the probability of that ER being sustainable over the long run (i.e., ER < Umsy). Probabilities are categorized using the Intergovernmental Panel on Climate Change (IPCC) Likelihood Scale to facilitate discussion of results (Table \@ref(tab:IPPCLikelihoodTab)). Note that this figure shows the same information as the top panel of Figure \@ref(fig:UmsyProfiles1), just in more detail. Stocks are grouped by aggregate, and roughly sorted within aggregate from mouth of the river upstream. Grey shading indicates stocks that either lack SR data (e.g., Oweegee, Sicintine) or are enhanced (Pinkut, Fulton).


```{r UmsyHeatmapLTAvg,   fig.cap="(ref:UmsyHeatmapLTAvg)"}
include_graphics("data/UmsyPlots/Agg_Heatmap_FixedER_UmsyComparison_LTAvgProd.png")
```



\clearpage
(ref:IPCCscale) IPCC Likelihood scale from @IPCCscale and colour coding used in this paper.


```{r IPPCLikelihoodTab, echo = FALSE, results = "asis"}

ipcc.df <- read.csv("data/Reference Tables/IPCC_LikelihoodScale.csv",stringsAsFactors = FALSE)


ipcc.df  %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)})%>%
   mutate_all(function(x){gsub("NA", "", x)})%>%
   csas_table(format = "latex", escape = FALSE, font_size = 9,
                  caption = "(ref:IPCCscale)") %>%
   #kableExtra::column_spec(1, width = "15em") %>%
   kableExtra::row_spec(1:(dim(ipcc.df)[1] -1), hline_after = TRUE)
   #kableExtra::column_spec(2, width = "8em") %>%
   #kableExtra::column_spec(3, width = "5em") %>%
   #kableExtra::column_spec(4, width = "7em") %>%
   #kableExtra::column_spec(5, width = "4em") %>%
   #kableExtra::column_spec(6, width = "14em") 


```





\clearpage
(ref:UmsyHeatmapRecent) Umsy comparison across stocks - Recent productivity. Layout as per Figure \@ref(fig:UmsyHeatmapLTAvg).

```{r UmsyHeatmapRecent,   fig.cap="(ref:UmsyHeatmapRecent)"}
include_graphics("data/UmsyPlots/Agg_Heatmap_FixedER_UmsyComparison_RecentProd.png")
```



\clearpage
(ref:UmsyHisto) Umsy frequency plot. Each panel shows the distribution of stock-specific median Umsy estimates, rounded to the nearest 5%, with the 16 modelled stock in the Skeena Wild aggregate  in the top row, and the 4 modelled stocks in the Nass aggregate in the bottom row. Adapted from Figure 14 in @Waltersetal2008ISRP.

```{r UmsyHisto,   fig.cap="(ref:UmsyHisto)"}
include_graphics("data/UmsyPlots/Umsy_Histograms.png")
```









\clearpage
##  EQUILIBRIUM PROFILES (STOCK-LEVEL AND AGGREGATE-LEVEL)  {#ProfileResults}

### Examples of Spawner-based Equilibrium Profiles

Equilibrium profiles of expected yield or recruitment at different levels of spawner abundance (assumed to be a fixed escapement target) have been used as a building block for setting escapement goals in Alaskan and northern transboundary salmon fisheries (Section \@ref(EqProfilesMethods)). 

We illustrate the approach for the largest stock in each aggregate, using three alternative versions of a yield profile, and then summarize the aggregate-level and stock-level results for one commonly used version, the "80-60 range", which captures the range of spawner abundances with an 80% probability of achieving at least 60% of median MSY on average over the long term, if the stock were managed to a fixed escapement goal in that range.

*Meziadin*: The all-year median spawner abundance is very close to the median estimate of Smsy under the long-term average productivity scenario, but most years since 2000 were at or below the long-term average Smsy (Figure \@ref(fig:ProfileMezidian), Panel A). Under long-term average productivity, spawner abundances near the median Smsy estimate had a roughly 80% probability of achieving at least 80% of long-term average MSY, but under the recent productivity scenario no spawner abundance was likely to achieve that objective (Panel B). For objectives with lower targets (achieve at least 60% of MSY, achieve an equilibrium yield of 100,000 fish), the ranges of spawner abundances with an 80% probability of meeting the objective under long-term average productivity was wider (Panels C, D).  Probabilities of achieving these lower objectives were higher under recent productivity, but still didn't meet the 80% threshold we used for illustration.

*Babine Late Wild*: The all-year median spawner abundance is below the median estimate of Smsy under the long-term average productivity scenario, and most years since 2000 were below the long-term average Smsy (Figure \@ref(fig:ProfileBabineLW), Panel A). The observed differences in yield profiles across alternative productivity scenarios and objectives for Babine Late Wild were similar to the Meziadin profiles (Panels B-D).

 "80-60" yield ranges could be calculated for both aggregates and most of the stocks under the long-term average productivity, but only a few stocks met the 80% threshold under the recent productivity scenario (Tables \@ref(tab:ProfTab8060Nass) and \@ref(tab:ProfTab8060SkeenaWild)).


(ref:ProfileMezidian) Sample yield profiles: Meziadin (Largest Nass stock). Observed spawner abundances (A) and three alternative equlibrium yield profiles (B, C, D). Yield profiles are shown for the long-term average productivity scenario (LtAvg) and the recent productivity scenario (Recent). Boxplots in panel A show distributions for either all years or by decade, with number of observations in brackets. Each boxplot shows median (vertical line), lower and upper quartiles (box) and the range between smallest and largest observations (x).  

```{r ProfileMezidian,   fig.cap="(ref:ProfileMezidian)"}
include_graphics("data/StockSampleFigs/Meziadin_EqProbProfiles.png")
```





\clearpage
(ref:ProfileBabineLW) Sample Yield Profiles: Babine Late Wild (Largest wild Skeena stock). Layout as per Figure \@ref(fig:ProfileMezidian).

```{r ProfileBabineLW,   fig.cap="(ref:ProfileBabineLW)"}
include_graphics("data/StockSampleFigs/Bab-LW_EqProbProfiles.png")
```



\clearpage



(ref:ProfTab8060Nass) Summary of "80-60" yield profiles - Nass. For the long-term average productivity scenario, the table lists median Smsy (Smsy), median yield at Smsy (MSY), 60% of MSY, the range of spawner abundances with 80% probability of achieving 60% MSY (Lower, Upper), as well as median recruits and median productivity in that spawner range (Rec, RpS). For the recent productivity scenario, spawner range, recruits, and productivity are also listed for those cases where there is at least an 80% of achieving 60% of the long-term average MSY.

```{r ProfTab8060Nass, echo = FALSE, results = "asis"}

prof.tab.src <- read_csv("data/SummaryTables_Profiles8060.csv")

table.in <-  prof.tab.src %>% dplyr::filter(Basin=="Nass") %>% arrange(StkSeq) %>% select(-StkSeq,-Basin,-Stock) %>%
														mutate_at(c(2:7,9:11),function(x){prettyNum(round(x), big.mark=",")}) %>%
														mutate_at(c(8,12),function(x){format(round(x,1),n.small =1)}) 
	
col.names.use <- c("Stock","Smsy","MSY","60\\% MSY","Lower","Upper","Rec","RpS","Lower","Upper","Rec","RpS")
table.in[table.in == "NA"] <- "-"
table.in[table.in == " NA"] <- "-"
	

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",11)),
                  caption = "(ref:ProfTab8060Nass)", col.names = col.names.use ) %>%
    kableExtra::row_spec(c(1), hline_after = TRUE) %>%
	add_header_above(c(" ", "LtAvg" = 7, "Recent" = 4)) 

```



(ref:ProfTab8060SkeenaWild) Summary of sample yield profiles - SkeenaWild.  Layout as per Table \@ref(tab:ProfTab8060Nass).

```{r ProfTab8060SkeenaWild, echo = FALSE, results = "asis"}

table.in <-  prof.tab.src %>% dplyr::filter(Basin=="Skeena",!(Stock %in% c("Pinkut", "Fulton","Skeena"))) %>% arrange(StkSeq) %>% select(-StkSeq,-Basin,-Stock) %>%
							mutate_at(c(2:7,9:11),function(x){prettyNum(round(x), big.mark=",")}) %>%
														mutate_at(c(8,12),function(x){format(round(x,1),n.small =1)}) 
	
col.names.use <- c("Stock","Smsy","MSY","60\\% MSY","Lower","Upper","Rec","RpS","Lower","Upper","Rec","RpS")
table.in[table.in == "NA"] <- "-"
table.in[table.in == " NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",11)),
                  caption = "(ref:ProfTab8060SkeenaWild)", col.names = col.names.use ) %>%
    kableExtra::row_spec(c(1), hline_after = TRUE) %>%
	add_header_above(c(" ", "LtAvg" = 7, "Recent" = 4)) 

```




\clearpage
### Examples of ER-based Equilibrium Profiles


Equilibrium yield and spawner abundance at different levels of exploitation rate (assumed to be a fixed ER target) have been used to develop aggregate trade-off plots for various salmon stock aggregates and mixed-stock fisheries, including Skeena Sockeye (Section \@ref(EqProfilesMethods)). 

We illustrate the approach for the 16 modelled wild stocks in the Skeena aggregate  and the 4 modelled wild stocks in the Nass aggregate.


*Skeena Wild* (Figure \@ref(fig:ERBasedProfileSkeenaWild), Table \@ref(tab:ProfTabERbasedNass))


Aggregate equilibrium catch is largest for exploitation rate around 50% under the long-term average productivity scenario, and around 40% under the recent productivity scenario. The number of spawners and catch at equilibrium under these exploitation rates are very different depending on the productivity assumption. Under long-term average productivity and 50% ER every year, the aggregate is assumed to settle into a stable state of around 390,000 spawners and around 390,000 catch, with 6 stocks harvested above their sustainable harvest rate (Umsy), and one of these stocks extirpated. Under recent productivity and 41% ER every year, the aggregate is assumed to settle into a stable state of around 226,000 spawners and around 157,000 catch, with two stocks harvested above their stock-specific Umsy, and two of those extirpated.

If all stocks are at equilibrium, then managing to a fixed ER of 50% under long-term average productivity is the same as managing, on average, to an aggregate spawning target of 390,000.  In practice, however, aggregate run sizes and stock composition vary from year to year, and parameter estimates are uncertain.  In any given year, these two strategies can therefore have very different implications, both in terms of aggregate outcomes and stock-level outcomes. For example, consider a year with an aggregate run of 400,000. Though mathematically equivalent at equilibrium, in that year a fixed ER strategy of 50% implies a spawning target of 200,000 and a catch target of 200,000, while a fixed escapement strategy of 390,000 implies a spawning target of 390,000 and a catch target of 10,000 (ER = 0.25%). Now imagine another year with an aggregate run of 1 million. In that year a fixed ER strategy of 50% implies a spawning target of 500,000 and a catch target of 500,000, while a fixed escapement strategy of 390,000 implies a spawning target of 390,000 and a catch target of 610,000 (ER = 61%).  At 61% ER over many years, about 10 of the 16 stocks would be overfished or extirpated. In that specific year, however, the stock-level implications of a 61% ER depend on stock composition (i.e., similar to equilibrium stock composition, or disproportionate contribution of a few stocks?). Forward simulations can be used to explore the expected long-term effect of alternative strategies for these types of contingencies (Section \@ref(ProjBesdResults)).  


*Nass* (Figure \@ref(fig:ERBasedProfileNass), Table \@ref(tab:ProfTabERbasedNass))

Aggregate equilibrium catch is largest for exploitation rate around 60% under the long-term average productivity scenario, and around 50% under the recent productivity scenario. The number of spawners and catch at equilibrium under these exploitations are very different depending on the productivity assumption. Under long-term average productivity and 60% ER every year, the aggregate is assumed to settle into a stable state of around 220,000 spawners and around 330,000 catch, with 2 stocks harvested above their sustainable harvest rate (Umsy), and none of the stocks extirpated. Under recent productivity and 50% ER every year, the aggregate is assumed to settle into a stable state of around 125,000 spawners and around 125,000 catch, with two stocks harvested above their stock-specific Umsy, and none of the stocks extirpated.




\clearpage
(ref:ERBasedProfileSkeenaWild) Example of aggregate equilibrium trade off plots for the SkeenaWild aggregate with 16 modelled stocks. For 5% increments of aggregate exploitation rate (ER; top axis), the figure shows median estimates (points) and interquartile range along the vertical axes (shaded area, p25 to p75) for aggregate spawner abundance (bottom axis), aggregate catch (left axis), and number of stocks where aggregate ER exceeds stock-specific median estimates of Umsy, the exploitation rate at maximum sustainable yield. Note  that the ranges of spawner abundances and catch levels differ substantially between long-term average productivity (Panel A) and recent productivity  (Panel B), but the ranges of ER and number of stocks are the same in both panels.

```{r ERBasedProfileSkeenaWild,   fig.cap="(ref:ERBasedProfileSkeenaWild)"}
include_graphics("data/AggProfiles_SkeenaWild_Dual_2Panel.png")
```



\clearpage
(ref:ProfTabERbasedSkeenaWild) Summary of aggregate equilibrium trade-offs under alternative exploitation rates - Skeena Wild. Table shows values from Figure \@ref(fig:ERBasedProfileSkeenaWild) at 10% increments of fixed harvest rate *(U)*.

```{r ProfTabERbasedSkeenaWild, echo = FALSE, results = "asis"}

er.based.prof.tab.src <- read_csv("data/Agg_Tradeoff_SummaryByAgg.csv") %>% mutate(U2= round(U*100))

table.in <-  er.based.prof.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild", U2 %in% seq(0,100,by=10)) %>%
							select(Prod,U2, -Aggregate,
										 NumStksOverfished_Med,	NumStksBelowSgen_Med, NumStksExtirpated_Med, 	
										 EqSpn_p25,	EqSpn_Med,	EqSpn_p75,	EqCt_p25,	EqCt_Med,	EqCt_p75) %>%
							mutate_at(c(6:11),function(x){format(round(x/1000,1), big.mark=",",n.small =1)}) 
	
col.names.use <- c("Prod","U (\\%)", "Above\nUmsy","Below\nSgen","Ext","p25","Med","p75","p25","Med","p75")


table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",11)),
                  caption = "(ref:ProfTabERbasedSkeenaWild)", col.names = linebreak(col.names.use) ) %>%
    kableExtra::row_spec(c(11), hline_after = TRUE) %>%
	add_header_above(c(" ", " ", "Median Num Stocks" = 3, "Spawners (1000s)" = 3, "Catch (1000s)" = 3)) 

```




\clearpage
(ref:ERBasedProfileNass) Example of aggregate equilibrium trade off plots for the Nass aggregate with 16 modelled stocks. For 5% increments of aggregate exploitation rate (ER; top axis), the figure shows median estimates (points) and interquartile range along the vertical axes (shaded area, p25 to p75) for aggregate spawner abundance (bottom axis), aggregate catch (left axis), and number of stocks where aggregate ER exceeds stock-specific median estimates of Umsy, the exploitation rate at maximum sustainable yield. Note  that the ranges of spawner abundances and catch levels differ substantially between long-term average productivity (Panel A) and recent productivity  (Panel B), but the ranges of ER and number of stocks are the same in both panels.

```{r ERBasedProfileNass,   fig.cap="(ref:ERBasedProfileNass)"}
include_graphics("data/AggProfiles_Nass_Dual_2Panel.png")
```


\clearpage
(ref:ProfTabERbasedNass)  Summary of aggregate equilibrium trade-offs under alternative exploitation rates - Skeena Wild. Table shows values from Figure \@ref(fig:ERBasedProfileSkeenaWild) at 10% increments of fixed harvest rate *(U)*.

```{r ProfTabERbasedNass, echo = FALSE, results = "asis"}

table.in <-  er.based.prof.tab.src %>% dplyr::filter(Aggregate=="Nass", U2 %in% seq(0,100,by=10)) %>%
							select(Prod,U2, -Aggregate,
										 NumStksOverfished_Med,	NumStksBelowSgen_Med, NumStksExtirpated_Med, 	
										 EqSpn_p25,	EqSpn_Med,	EqSpn_p75,	EqCt_p25,	EqCt_Med,	EqCt_p75) %>%
							mutate_at(c(6:11),function(x){format(round(x/1000,1), big.mark=",",n.small =1)}) 
	
col.names.use <- c("Prod","U (\\%)", "Above\nUmsy","Below\nSgen","Ext","p25","Med","p75","p25","Med","p75")


table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",11)),
                  caption = "(ref:ProfTabERbasedNass)", col.names = linebreak(col.names.use) ) %>%
    kableExtra::row_spec(c(11), hline_after = TRUE) %>%
	add_header_above(c(" ", " ", "Median Num Stocks" = 3, "Spawners (1000s)" = 3, "Catch (1000s)" = 3)) 

```




\clearpage
##  STATUS-BASED AGGREGATE LIMIT REFERENCE POINTS {#StatusRPResults}

```{r , echo = FALSE, results = "asis"}

rel.abd.metric.all.df  <- read_csv("data/SummaryTables_RelAbdMetric_AllYears.csv") 

rel.abd.status.2019 <- rel.abd.metric.all.df %>% 
	dplyr::filter(UpTo == 2019) 

```


Status assessments under Canada's Wild Salmon Policy combine multiple considerations into a single integrated status, including absolute abundance, relative abundance, long-term trend, short-term trend, probability of decline, and spatial distribution (Section \@ref(StatusMethods)). Relative abundance is assessed by comparing the generational average of spawner abundance (geometric mean) to a lower benchmark at Sgen and an upper benchmark at 80% Smsy. If abundance falls below Sgen, status *on this one metric* is Red, above 80% Smsy it is Green, and in-between it is Amber. WSP status has been recommended as the primary consideration for evaluating aggregate limit reference points under the 2019 update of the *Fisheries Act* [@LRPGuidelinesSAR]. 

In a full integrated WSP status assessment [e.g., @FrSkWSPStatus2017], a group of experts would work through a case-by-case review of observed abundance against the full posterior distributions of alternative benchmark estimates, then decide how much weight to give this metric relative to other information, such as trends. A rapid approximation of the integrated assessments has been developed, using an algorithm derived from completed integrated expert assessments to combine median benchmark estimates, where deemed appropriate, with the other metrics [@RapidStatusTechRep1]. Integrated or rapid status assessments have not been completed for Nass and Skeena Sockeye, but we illustrate potential uses of the status information with a single metric. We used benchmark estimates without lognormal bias correction for the long-term average productivity scenario to calculate the relative abundance metric for wild Nass and Skeena stocks.

With data up to 2019, spawner abundances for the largest stock from each aggregate fell into the Amber status zone for the relative abundance metric (Figures \@ref(fig:RelAbdMetricMeziadian) and \@ref(fig:RelAbdMetricBabLW)). For Meziadin, annual abundances and running generational averages have been above 80% of the median long-term Smsy for most years since 1980, but dipped into the Amber zone in the last few years. Babine Late Wild abundances and running generational averages have been in the Amber zone for most years since the late 1990s, and even dipped into the Red zone for some years.

Looking at the average abundance for the generation ending in 2019, `r sum(rel.abd.status.2019$MetricStatus == "Red")` stocks were in the Red zone,  `r sum(rel.abd.status.2019$MetricStatus == "Amber")` stocks were in the Amber zone, `r sum(rel.abd.status.2019$MetricStatus == "Green")` stocks were in the Green zone, and `r sum(rel.abd.status.2019$MetricStatus == "None")` stocks could not be assessed for this metric (Table \@ref(tab:RelAbd2019)). Generational average for some stocks falls near the benchmark value (i.e., lower or upper ratio is near 1), and in these cases a small change in the median benchmark estimate could result in a switch of the status category (e.g., Meziadin).  For other stocks, the generational average is so clearly in the Red zone (e.g., Kitwanga at less than 20% of Sgen) or in the Green zone (e.g., Mcdonell at almost 4 times the upper benchmark), that the metric status wouldn't change for any of the alternative benchmark estimates generated by the various candidate SR models. 

The proportion of stocks from each aggregate that fall into the Red, Amber, or Green status zone on the relative abundance metric has varied over time (Tables \@ref(tab:RelAbdTabNass) and \@ref(tab:RelAbdTabSkeenaWild), Figure \@ref(fig:RelAbdPattern)). Most of the modelled stocks for both aggregates were in the Amber or Green zones for the relative abundance metric for most years since the 1980s, but note the several of the largest stocks fell into the Amber zone in recent years (with data up to 2019), and that incorporating low returns in 2020 and 2021 may push these stocks deeper into the Amber zone or even into the Red zone (e.g., Meziadin, Babine Late Wild, Babine Early Wild). If integrated status assessments were to result in a similar picture, then the proposed aggregate limit reference points would be triggered for most years on the Nass aggregate for both of the illustrated objectives (No Red, < 20% Red), and for most years on the Skeena aggregate for the stricter objective (No Red).


\clearpage
(ref:RelAbdMetricMeziadian) WSP metric for relative abundance: Meziadin (Largest Nass stock). Figure shows estimated spawner abundances (blue line with points) and running generational average (red line) compared to lower and upper benchmarks (boxplots). Each boxplot shows median (horizontal line), half of the posterior distribution (box, p25-p75), and 80% of the posterior distribution (whiskers, p10-p90). Benchmark estimates are shown without (R) and with (C) lognormal bias correction. Horizontal reference lines mark the median benchmark estimate without bias correction, which are the values used in Tables \@ref(tab:RelAbd2019) to \@ref(tab:RelAbdTabSkeenaWild).

```{r RelAbdMetricMeziadian,   fig.cap="(ref:RelAbdMetricMeziadian)"}
include_graphics("data/StockSampleFigs/Meziadin_RelAbd_Plot.png")
```





\clearpage
(ref:RelAbdMetricBabLW) WSP metric for relative abundance: Babine Late Wild (Largest Skeena stock). Layout as per Figure \@ref(fig:RelAbdMetricMeziadian).

```{r RelAbdMetricBabLW,   fig.cap="(ref:RelAbdMetricBabLW)"}
include_graphics("data/StockSampleFigs/Bab-LW_RelAbd_Plot.png")
```




\clearpage

(ref:RelAbd2019) Single-metric statuses using data up to 2019: Relative abundance.  Table lists the average generation (main age class, Gen), the number of observations in the generation ending in 2019 (Obs). The generational avg (geometric mean) is compared to median estimates of Sgen (Lower) and 80% Smsy (Upper) under the long-term average productivity scenario. Note that the resulting status category is only for the relative abundance metric, and a comprehensive status assessment would also consider absolute abundance (i.e., is it less than 1,000 adults?), short-term and long-term trends, probability of decline, and spatial distribution (Section \@ref(StatusMethods)).

```{r RelAbd2019, echo = FALSE, results = "asis"}

table.in <-  rel.abd.metric.all.df %>%
								dplyr::filter(UpTo == 2019) %>% arrange(RatioUBM) %>%
								select(-UpTo,-MetricScore) %>% 
								mutate_at(5:7,function(x){prettyNum(round(x),big.mark=",")}) %>%
								mutate_at(8:9,function(x){format(round(x,2),n.small =2)}) %>%
								mutate(MetricStatus = recode(MetricStatus,"None" = "Unk"))


#table.in[is.na(table.in)] <- "NA"
table.in[table.in == "NA"] <- "-"


col.names.use <- c("Aggregate","Stock","Gen","Obs","GenAvg","Lower","Upper","Lower","Upper","Status\nCategory")


table.in %>%
   mutate_at(2,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(2,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c(rep("l",2),rep("r",8)),
                  caption = "(ref:RelAbd2019)",col.names = linebreak(col.names.use) ) %>%
	add_header_above(c(" " = 5, "BM Value" = 2, "Ratio" = 2," " = 1 )) 

```









\clearpage


(ref:RelAbdTabNass) Retrospective changes over time in annual metric statuses for Nass: Relative abundance.  Table summarizes status categories for the relative abundance metric as described in Table \@ref(tab:RelAbd2019), listing the total number of stock (n), the number and proportion of stocks for which the metric could not be calculated (Unk, pUnk), the number of stock for which the metric could be calculated (nStatus), the number of stocks in each status ctegory (Red, Amber, Green), the proportion of Red or Green among the assessed stocks (pRed, pGreen), and proportion of all stocks not assessed as Green (pNotGreen; includes Red, Amber, and Unk).

```{r RelAbdTabNass, echo = FALSE, results = "asis"}

table.in <-  read_csv("data/SummaryTables_RelAbdMetric_AnnualSummary_Nass.csv")

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
                  caption = "(ref:RelAbdTabNass)") 

```


\clearpage
(ref:RelAbdTabSkeenaWild) Retrospective changes over time in annual metric statuses for Skeena Wild: Relative abundance.  Layout as per Table \@ref(tab:RelAbdTabNass).

```{r RelAbdTabSkeenaWild, echo = FALSE, results = "asis"}

table.in <-  read_csv("data/SummaryTables_RelAbdMetric_AnnualSummary_SkeenaWild.csv")

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "r",
                  caption = "(ref:RelAbdTabSkeenaWild)") 

```


\clearpage
(ref:RelAbdPattern) Changes over time of single-metric statuses: Relative abundance. The first three panels show the retrospective changes over time in status categories for the relative abundance benchmark, which are listed in Tables  \@ref(tab:RelAbdTabNass) and \@ref(tab:RelAbdTabSkeenaWild). The fourth panel shows a timeline of years for which either none or only a few of the assessed stocks were in the Red zone on this metric.

```{r RelAbdPattern,   fig.cap="(ref:RelAbdPattern)"}
include_graphics("data/RelAbdMetric_Patterns_MultiPanel.png")
```




\clearpage
## LOGISTIC-REGRESSION-BASED AGGREGATE ABUNDANCE REFERENCE POINTS {#LogRegResults}

We explored whether the approach of developing aggregate abundance reference points based on logistic regressions would be potentially applicable to Nass or Skeena Sockeye. For the illustration, we defined a "success" as *"At least 80% of the stocks in aggregate are above Sgen"*, checked which past years met the criterion, and plotted success/failure vs. aggregate abundance.

For the Skeena Wild aggregate, there was relationship between aggregate abundance and success (Figure \@ref(fig:LogRegSkeenaWild)). All past years where aggregate abundance exceeded about 350,000 spawners met the criterion, but many years with lower abundance also met the criterion. A logistic regression could be fitted to the resulting data, but the shape was highly sensitive to alternative definitions of the success criterion we tested (variations not included in this paper).

For the Skeena aggregate, any results from the logistic regression approach would also need to be carefully framed in the context of (1) wild stocks, (2) total effective spawners combining wild spawners and  effective spawners from the channel stocks, and (3) total escapement including non-spawning biological surplus on the enhanced stocks (Figure \@ref(fig:LogRegSkeenaWild2)).


For the Nass aggregate, there was no link between aggregate abundance and success (Figure \@ref(fig:LogRegNass)). The years with the two largest spawner abundances did not meet the criterion (1992 and 1993, both with more than 500,000 spawners). Most of the remaining years did not meet the criterion, and the success years spanned a wide range of abundances, from less than 200,000 to more than 400,000. 



\clearpage
(ref:LogRegSkeenaWild) Illustration of log-regression approach for deriving aggregate abundance reference points: Skeena Wild. Panels show aggregate spawner abundance (A), the proportion of stocks in the aggregate that met the criterion "annual spawner abundance larger than than the lower benchmark set at Sgen" (pAnnualSpnLgLBM)  by year (B) and by aggregate abundance (C), and overall success/failure on the aggregate criterion of "80% of stocks above the benchmark" (D). The interim escapement goal for the Skeena aggregate is 900,000 spawners, and the corresponding interim escapement goal for the Skeena Wild aggregate is 300,000 based on average stock composition (Section \@ref(Background)), which is marked in the plots. Wild spawner abundance has been at or above the interim EG for most years since the 1980s (A). The proportion of stocks meeting the success criterion has ranged from about 60%-100% (B). For most years where aggregate spawner abundance was at or above the interim goal, 80% or more of the stocks met the success criterion (C). The data points in panel C are then simplified to whether they meet the 80% threshold (Yes/No) to fit a logistic regression (D). The fitted red regression line shows the increasing probability that at least 80% of stocks meet the success criterion as aggregate spawner abundance increases, with aggregate spawner abundance larger than 500,000 resulting in very little incremental increase in the probability of success for this specific example of a success criterion. 

```{r LogRegSkeenaWild,   fig.cap="(ref:LogRegSkeenaWild)"}
include_graphics("data/PropBasedTargetPlots_SkeenaWild_pAnnualSpnLgLBM.png")
```



\clearpage


(ref:LogRegSkeenaWild2) Illustration of log-regression approach for deriving aggregate abundance reference points: Alternative aggregations. Skeena Wild are only a part of the total annual returns that are currently managed based on aggregate abundance. This figure shows three versions of the aggregate spawner abundance (A): wild spawners (including only wild stocks), total effective spawners (wild plus channel loading plus Pinkut and Fulton spawners below the fence), and total escapement including biological surplus (i.e., run kept out of channels but beyond the capacity of spawning grounds below the fences). The time series of success/failure from Figure \@ref(fig:LogRegSkeenaWild) can be plotted against either one of these aggregate abundance (B-D), and would lead to very different results for an aggregate abundance reference point. Interim EG of 900,000 for the total aggregate and 300,000 for the wild aggregate are marked. 

```{r LogRegSkeenaWild2,   fig.cap="(ref:LogRegSkeenaWild2)"}
include_graphics("data/PropBasedTargetPlots_COMPARE_SkeenaWild_pAnnualSpnLgLBM.png")
```





\clearpage
(ref:LogRegNass) Illustration of log-regression approach for deriving aggregate abundance reference points: Nass. Layout as per Figure \@ref(fig:LogRegSkeenaWild). The interim escapement goal for the Nass aggregate at 200,000 spawners is marked. Aggregate spawner abundance has been at or above the interim EG for most years since the 1980s (A). The proportion of stocks meeting the success criterion has ranged from about 25%-100%, but for most years it was less than 80% (B). For most years where aggregate spawner abundance was at or above the interim goal, less than 80% of the stocks met the success criterion (C). The data points in panel C are then simplified to whether they meet the 80% threshold (Yes/No), but the observed scatter of points does not allow for a logistic regression fit, because even the largest aggregate spawner abundances failed to meet the success criterion (D).

```{r LogRegNass,   fig.cap="(ref:LogRegNass)"}
include_graphics("data/PropBasedTargetPlots_Nass_pAnnualSpnLgLBM.png")
```




\clearpage
## SIMULATION-BASED AGGREGATE ABUNDANCE REFERENCE POINTS {#ProjBesdResults}

### Example Results

The example results shown here are for a small and very specific subset of potential scenarios, as defined in Section \@ref(SimScenarios), and for two specific versions of more general objectives.

We include two types of summaries for the simulation results: 

* *Stock-specific probabilities* (Figures \@ref(fig:HeatmapAggFixedERLtAvg) to \@ref(fig:HeatmapAggFixedEscRecent)): These plots compare 10 different levels of a harvest strategy, showing for each modelled stock the probability of achieving one specific objective.
* *Trade-off plots* (Figures \@ref(fig:TradeoffPlotSkeenaWildLTAvg) to \@ref(fig:TradeoffPlotNassRecent)):  These plots compare two objectives across 10 different levels of  a harvest strategy.


#### Probabilities - Fixed Exploitation Rates

Under long-term productivity, almost all modelled stocks (19/20) met the objective when fixed ER was 10% or less, but over half the modelled stocks (11/20) met the objective for fixed ER greater than 50%. Most modelled stocks failed to meet the objective at fixed ER at 60% or higher (Figure \@ref(fig:HeatmapAggFixedERLtAvg)). These proportions shifted dramatically under the recent productivity scenario: Even under 10% fixed ER a quarter (5/20) of the modelled stocks did not meet the objective (Figure \@ref(fig:HeatmapAggFixedERRecent)). 

#### Probabilities -  Fixed Escapement

Under long-term productivity about half (11/20) of the modelled stocks met the objective for aggregate escapement goals set at least 75% above the current interim escapement goals, or target of 350,000 for Nass Sockeye compared to the interim EG of 200,000, and a 525,000 target for Skeena Wild compared to the assumed interim EG of 300,000 (Figure \@ref(fig:HeatmapAggFixedEscLtAvg)). Performance degraded rapidly for lower escapement goals, with only 6 of 20 modelled stock meeting the objective at the interim EG (4th column). The effect of lower fixed escapement goals was less pronounced under the recent productivity scenario, with twice the number of stocks (12/20) meeting the objective at the interim EG (Figure \@ref(fig:HeatmapAggFixedEscLtAvg); 4th column). This is due to the interaction between aggregate abundance and target exploitation rate: under the recent productivity scenario, run sizes for the largest stocks are lower, leading to a lower aggregate abundance, and a lower target ER with a fixed escapement strategy. Under long-term average productivity, aggregate abundances and resulting target ERs are higher, and the component stocks are less likely to meet conservation objectives if they have lower productivity than the largest stocks. A key benefit of the forward simulation approach is that it allows us to identify and investigate these types of counter-intuitive interactions.



\clearpage
(ref:HeatmapAggFixedERLtAvg) Simulation summary - Alternative fixed ER and long-term average productivity.  Simulation results are summarized for different levels of fixed ER (0% to 90%; columns) and one productivity scenario across all stocks. The numbers in each cell of the grid show the probability of spawner abundance in the 3rd generation exceeding a benchmark set at 80% of median Smsy under long-term average productivity for one stock under one specific level of fixed ER. Probabilities are categorized using the Intergovernmental Panel on Climate Change (IPCC) Likelihood Scale to facilitate discussion of results (Table \@ref(tab:IPPCLikelihoodTab)). Stocks are grouped by aggregate, and roughly sorted within aggregate from mouth of the river upstream. Grey shading indicates stocks that were not modelled in the current project. Bolded blue numbers above the grid show the number of stocks in each column with probability larger than 80%.


```{r HeatmapAggFixedERLtAvg,   fig.cap="(ref:HeatmapAggFixedERLtAvg)"}
include_graphics("data/Sims/Agg_Heatmap_SpnLgSmsyBM_LTAvg_FixedER.png")
```





\clearpage
(ref:HeatmapAggFixedERRecent) Simulation summary - Alternative fixed ER and recent productivity. Layout as per Figure \@ref(fig:HeatmapAggFixedERLtAvg).

```{r HeatmapAggFixedERRecent,   fig.cap="(ref:HeatmapAggFixedERRecent)"}
include_graphics("data/Sims/Agg_Heatmap_SpnLgSmsyBM_Recent_FixedER.png")
```






\clearpage
(ref:HeatmapAggFixedEscLtAvg) Simulation summary - Alternative fixed escapement targets with 10% ER floor and 80% ER cap under long-term average productivity.  Layout as per Figure \@ref(fig:HeatmapAggFixedERLtAvg), except that columns correspond to different levels of fixed escapement, set at increments of the interim escapement goal for each aggregate. The fourth column corresponds to the interim goal (200,000 for Nass, 300,000 for Skeena Wild), the first column to 1/4 of the interim goal, and the last column to 2.5 times the interim goal.

```{r HeatmapAggFixedEscLtAvg,   fig.cap="(ref:HeatmapAggFixedEscLtAvg)"}
include_graphics("data/Sims/Agg_Heatmap_SpnLgSmsyBM_LTAvg_FixedEsc10to80.png")
```






\clearpage
(ref:HeatmapAggFixedEscRecent) Simulation summary - Alternative fixed escapement targets with 10% ER floor and 80% ER cap under recent productivity.  Layout as per Figure \@ref(fig:HeatmapAggFixedEscLtAvg).

```{r HeatmapAggFixedEscRecent,   fig.cap="(ref:HeatmapAggFixedEscRecent)"}
include_graphics("data/Sims/Agg_Heatmap_SpnLgSmsyBM_Recent_FixedEsc10to80.png")
```


\clearpage
#### Trade-off plots

The basic trade-off is the same for both aggregates under both productivity assumptions: The number of stocks meeting the biological objective increases as the spawning target increases, up to a point,  while the average annual catch  over 3 generations peaks at some spawning level and then starts declining with further increases in spawning target. 

The spawning target with peak average catch is much lower under recent productivity (Figures \@ref(fig:TradeoffPlotSkeenaWildRecent),\@ref(fig:TradeoffPlotNassRecent)) than under recent productivity (Figures \@ref(fig:TradeoffPlotSkeenaWildLTAvg), \@ref(fig:TradeoffPlotNassLTAvg)), and the amount of catch is much lower as well. 

The effect of the productivity assumption on the number of stocks meeting the biological objective differs by spawning target:

* At larger spawning targets, more stocks meet the biological objective under long-term average productivity than under recent productivity, because more stocks are sufficiently productive to withstand higher aggregate exploitation rates, even though those rates are higher than under recent productivity. Under recent productivity, several stocks fail to meet the biological objective even with a large spawning target, because their productivity is so low that even with the lower exploitation rates associated with larger spawning targets, they don't reach their stock-specific benchmark within 3 generations from recently observed spawner abundances.

* At spawning targets around the interim EG, more stocks meet the biological objective under recent productivity than under long-term average productivity, because aggregate abundances are lower, resulting in lower exploitation rates for the same spawning target. This is another example of the counter-intuitive interactions discussed above for the probability plots.


\clearpage
(ref:TradeoffPlotSkeenaWildLTAvg) Example of Trade off Plot - SkeenaWild - Long-term average productivity. Compares change for two different performance measures as aggregate spawning target is increased from 1/4 of the current escapement goal (left-most point) to 2.5 times the current goal (right-most point). Performance measures were selected to show the trade-off between an example biological objective (number of stocks for which the 3rd simulated generation exceeds 80% of Smsy with more than 80% probability; blue line with solid points, left axis) and an example harvest objective (trimmed average annual catch over 3 generations, orange line with open circles, right axis). Both performance measures improve as as the aggregate spawning target increases up to around 500,000, but average catch peaks around 500,000 spawning target (much above the interim EG of 300,000), while the number of stocks meeting 80% of Smsy continues to increase.

```{r TradeoffPlotSkeenaWildLTAvg,   fig.cap="(ref:TradeoffPlotSkeenaWildLTAvg)"}
include_graphics("data/Sims/TradeOffPlot_2_ProbSmsyVSAvgCtTr_Skeena_LTAvg.png")
```



\clearpage
(ref:TradeoffPlotSkeenaWildRecent) Example of Trade off Plot - SkeenaWild - Recent productivity.  Layout as per Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg). The basic trade-off is the same as in Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg) for long-term average productivity, with average catch peaking at some spawning level while the number of stocks meeting the biological objective continues to increase. However, with this recent productivity scenario, the average catch peaks at a lower spawning target (around the interim goal of 300,000 vs. 500,000) and peak catch is much lower (around 175,000 vs. almost 500,000). The number of stocks meeting the biological objective is higher at lower spawning targets (because large stock have reduced productivity under the recent scenario, so total run sizes and resulting aggregate ER are lower), but lower at higher spawner targets (because under recent productivity more stocks do not reach the biological objective in 3 generations).

```{r TradeoffPlotSkeenaWildRecent,   fig.cap="(ref:TradeoffPlotSkeenaWildRecent)"}
include_graphics("data/Sims/TradeOffPlot_1_ProbSmsyVSAvgCtTr_Skeena_Recent.png")
```




\clearpage
(ref:TradeoffPlotNassLTAvg) Example of Trade off Plot - Nass - Long-term average productivity.  Layout as per Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg). The basic trade-off is the same as in Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg) for Skeena Wild, with average catch peaking at some spawning level while the number of stocks meeting the biological objective continues to increase. For Nass Sockeye, under long-term productivity, the average catch peaks around the interim goal of 200,000 and peak catch is around 325,000. The number of stocks meeting the biological objective ranges from 0 for spawning targets below the interim EG of 200,000 to all 4 modelled stocks for spawning targets
above about 400,000. 

```{r TradeoffPlotNassLTAvg,   fig.cap="(ref:TradeoffPlotNassLTAvg)"}
include_graphics("data/Sims/TradeOffPlot_4_ProbSmsyVSAvgCtTr_Nass_LTAvg.png")
```




\clearpage
(ref:TradeoffPlotNassRecent) Example of Trade off Plot - Nass - Recent productivity.  Layout as per Figure \@ref(fig:TradeoffPlotSkeenaWildLTAvg). Observed differences between recent and long-term average productivity for Nass are similar to the observed differences for Skeena Wild: average catch peaks at a lower spawning target (around 125,000 vs. 200,000) and reaches a lower peak (about 110,000 vs. 325,000). Only 2 of the 4 stocks reach the biological objective under recent productivity over 3 generations, even for spawning targets more than double the interim EG.

```{r TradeoffPlotNassRecent,   fig.cap="(ref:TradeoffPlotNassRecent)"}
include_graphics("data/Sims/TradeOffPlot_3_ProbSmsyVSAvgCtTr_Nass_Recent.png")
```




\clearpage
### Summary of Sensitivity Tests

The example results in the previous section are for a small, and very specific subset, of potential scenarios, as defined in Section \@ref(SimScenarios), and for two very specific versions of more general objectives. To support scoping discussions for future simulation work and collaborative planning processes, we summarize general observations from sensitivity tests described in Table \@ref(tab:SimSensTestTable).  Figure \@ref(fig:FixedEscVar) illustrates the comment regarding ER floor and cap.



(ref:SimSensTestTable) Observed effects of key model components. 

```{r SimSensTestTable, echo = FALSE, results = "asis"}


sims.obs.df <- read.csv("data/Sims/SimSensTest_Summary.csv",stringsAsFactors = FALSE, fileEncoding="UTF-8-BOM") %>%
								arrange(Seq) %>% select(-Seq)


colnames(sims.obs.df) <- linebreak(c("Model\nComponent","Observations"))



sims.obs.df %>%
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = "l",
                  caption = "(ref:SimSensTestTable)") %>%
  kableExtra::column_spec(1, width = "7em") %>%
  kableExtra::column_spec(2, width = "43em") %>%
   kableExtra::row_spec(1:  (dim(sims.obs.df)[1] -1), hline_after = TRUE)


```


\clearpage
(ref:FixedEscVar) Variations of a fixed escapement goal strategy. Both panels compare a 300,000 escapement goal (A,C) and a 600,000 escapement goal (B,D), with the resulting ER at different aggregate run sizes shown in grey for strict fixed escapement policy (i.e., harvest every fish above the goal, harvest is 0 below the goal). Top panel shows the corresponding harvest control rules (HCR) if ER floor is 10% (i.e., harvest 10% of run regardless of run size, increase ER as run size increases) and ER cap is 80% (i.e., never harvest more than 80%, even at very large aggregate run size). Bottom panel shows the HCR for the same two escapement goals, but with ER floor at 25% and ER cap at 60%. Even though escapement goals of 300,000 and 600,000 are very different, with the higher floor and lower cap, the target ER for the two strategies is the same for a wide range of run sizes. Differences in simulated outcomes between the two strategies are even less pronounced if aggregate-level outcome uncertainty is added to the model (e.g., a difference of 5% in the ER target has little effect if the actual ER is modelled as target ± 15%).



```{r FixedEscVar,   fig.cap="(ref:FixedEscVar)"}
include_graphics("data/Sims/FixedEscRule_Illustration.png")
```


\clearpage
## EXPANDING AGGREGATE REFERENCE POINTS FOR WILD SKEENA SOCKEYE TO ACCOUNT FOR ENHANCED CONTRIBUTION {#SkeenaExpResults}

### Context

Enhanced Pinkut and Fulton present two distinct challenges for managing the total Skeena stock aggregate: 

* *Enhanced contribution to returns* (Figure \@ref(fig:ChannelContribution)): The contributions of Sockeye originating from the enhanced Babine tributaries to the aggregate returns of Babine and Skeena Sockeye have increased over time. Pinkut and Fulton Sockeye together accounted for about 30% of Babine returns in the 1950s and 1960s, before the start of BLDP enhancement, but consistently contribute 80% or more in recent years. Enhanced contribution to total Skeena returns has ranged from about 40% to more than 80% since the 1980s, with a median of 66%. 

* *Biological surplus* (Figure \@ref(fig:SurplusProduction)): Returning Pinkut and Fulton fish that exceed the capacity of natural spawning habitat below the fence and are locked out of the channels are considered a non-spawning surplus. The method for estimating the annual surplus is described in Section \@ref(SurplusEst). The surplus amount has declined from over 400,000 fish in the 1980s and 1990s to under 200,000 in the 2000s and 2010s. However, the proportion of the surplus relative to the total Skeena Sockeye abundance has increased with decreasing returns of wild Skeena Sockeye. From 2016-2019, the surplus represented around 20 percent of the total Sockeye return and up to 50% of total escapements for the Skeena aggregate. In 2 recent years, the surplus matched or exceeded the total harvest.  

(ref:ChannelContribution) Contribution of BLDP-enhanced Sockeye stocks (Pinkut and Fulton). Figure shows % contribution of Pinkut and Fulton to Babine returns (A) and total Skeena returns (B). Vertical red line indicates start of construction of BLDP enhancement facilities in 1965. Trend line shows the running 4-yr average. 

```{r ChannelContribution,  fig.cap="(ref:ChannelContribution)" }
include_graphics("data/ChannelReview/ChannelContribution.PNG")
```


(ref:SurplusProduction) Estimates of channel surplus over time - Total Skeena. Top left panel shows the time series of estimated surplus spawners from both channel-enhanced stocks, excluding ESSR harvests. The remaining panels show the magnitude of the surplus relative to total Skeena Sockeye run size, total escapement (i.e., effective spawner abundance), and total catch including ESSR harvest. 


```{r SurplusProduction,  fig.cap="(ref:SurplusProduction)" }
include_graphics("data/ChannelReview/ChannelSurplus_TotalSkeena_TimeSeries.PNG")
```

\clearpage


### Alternative Expansion Approaches 

We tested two alternative approaches for expanding a wild Skeena management reference point to a total Skeena management reference point. Both used the observed composition of the total Skeena returns from 1982 to 2019, specifically the % of wild spawners in the terminal return, and they both assume that future stock composition is similar to the range of stock compositions observed since the 1980s. 

However, they differ in how those data are used:

* *Simple expansion* (Figure \@ref(fig:SkeenaExp1)): Use the median or upper/lower quartiles for the % wild spawners as a direct scalar on the wild reference point. For example, with a median scalar of 3.58, a terminal run of a bit more than 1 million had a 50:50 chance of resulting in about 300,000 wild spawners. To increase the probability of meeting 300,000 spawners to 75% (3/4 chance), the terminal run would need to be around 1.4 million. Similarly, for a 50:50 chance of meeting a wild spawning goal of 400,000, the terminal run would need to be around 1.5 million. 
* *Logistic regression* (Figure \@ref(fig:SkeenaExp2)): For each candidate terminal run target, calculate the probability of meeting alternative wild spawning goals. The logistic regression approach classifies the annual observations as a success (i.e., wild spawning goal was met) or failure (i.e., wild spawning goal was not met), then calculates the probability of success for different combinations of terminal return target and wild spawning goal. For example, in 1982 the terminal return was 1,447,330 and the wild spawner abundance was	303,954, so 1982 is classified as a success for a wild goal of 300,000, and as a failure for a wild goal of 400,000. The current terminal run target of 1.05 million is as likely as not to meet the current interim EG for wild Skeena Sockeye. This result is similar to the 50:50 chance identified with simple expansion in Figure \@ref(fig:SkeenaExp1). The current terminal run target of 1.05 million is unlikely to meet a wild spawning goal of 350,000 or larger.


Both of these approaches could be applied using forecasted stock composition for a specific year. Given pre-season forecasts for the enhanced stocks and largest wild stocks, in combination with target harvests, an in-season scalar could be approximated. However, this  information is not currently part of the annual in-season planning process.

Note that the % of wild *spawners* used for Figures \@ref(fig:SkeenaExp1) and \@ref(fig:SkeenaExp2) differs slightly from the % of wild *returns* plotted in Panel B of Figure \@ref(fig:ChannelContribution), due to differential in-river harvest of wild and enhanced Sockeye (e.g., timing and location of in-river fisheries below Babine fence, ESSR fisheries targeting Pinkut and Fulton in Babine Lake).


(ref:SkeenaExp1) Simple expansion of wild Skeena management reference points. Plot shows three alternative scalars applied to wild spawning goals ranging from 250,000 to 600,000. Current interim escapement goal and terminal run management target are shown for reference. Note that the scalars are estimated from observed historical stock composition, independent of alternative SR model fits or productivity assumptions.

```{r SkeenaExp1,   fig.cap="(ref:SkeenaExp1)"}
include_graphics("data/SkeenaExpansions/SkeenaExpansion_ScalarPlot_For_DiscPaper_v1.png")
```



\clearpage
(ref:SkeenaExp2) Probability of meeting wild spawning goals at different management targets for terminal run. Logistic regression was used to estimate the probability of success (i.e., wild spawning goal is met) for different combinations of terminal run target and wild spawning goal. Probabilities are categorized using the Intergovernmental Panel on Climate Change (IPCC) Likelihood Scale to facilitate interpretation of results (Table \@ref(tab:IPPCLikelihoodTab)). 

```{r SkeenaExp2,   fig.cap="(ref:SkeenaExp1)"}
include_graphics("data/SkeenaExpansions/SkeenaExpansion_ProbSuccessPlot_For_DiscPaper.png")
```



<!--chapter:end:03-Results.Rmd-->

\clearpage
# DISCUSSION

This chapter is organized into three sections. In Section \@ref(KeyConsiderations) we discuss key considerations for the three analytical priorities that shaped the work presented in this paper: (1) enhanced production, (2) salmon population diversity, and (3) time-varying productivity. In Section \@ref(KeyConclusions) we highlight key conclusions from (1) spawner-recruit analyses, and (2) comparison of alternative approaches for developing aggregate management reference points. Section \@ref(PrioritiesFuture) discusses priorities for future work.

## KEY CONSIDERATIONS {#KeyConsiderations}


Key  considerations for developing management targets for Skeena and Nass Sockeye include the high proportion of enhanced Sockeye within the Skeena aggregate, the large number of component stocks with different production dynamics within each aggregate, and the question of how to address temporal shifts in productivity which is becoming increasingly common for salmon stocks throughout the North Pacific.


###  Enhanced Production

Enhanced Skeena Sockeye now account for a large proportion of the total Skeena Sockeye return in most years. From 1970-2020, the enhanced stocks (Pinkut and Fulton) accounted for an average of 67% of the total Skeena return (range 33-83%). We considered the wild (non-Pinkut and Fulton) and enhanced Skeena Sockeye stocks separately in our analyses, which focused on developing biological benchmarks at the stock and aggregate level for wild Skeena and Nass Sockeye stocks. While total Skeena Sockeye returns have increased considerably since the implementation of the BLDP, the realized benefits to fisheries were less than predicted (Hilborn 1992). 

One fundamental challenge for managing Skeena Sockeye is the tradeoff between reducing mixed stock fisheries to  protect smaller Skeena Sockeye stocks and increasing abundances of surplus enhanced fish arriving in Babine Lake to be locked out of the spawning channels. The number of enhanced Sockeye that exceed spawning capacity has decreased since 2000 but still represents a sizable proportion of the annual Skeena Sockeye return (Figure \@ref(fig:SurplusProduction)). The estimated surplus in 2020 exceeded the total number of Skeena Sockeye harvested in all fisheries. A portion of the surplus may be harvested in commercial ESSR (Excess Sockeye to spawning requirements) fisheries that occur in Babine Lake, but these fisheries do not take place every year. 

Sockeye escapement to the spawning channels, Pinkut Creek and Fulton Rivers have been relatively constant since the start of the BLDP, while total returns for both wild and enhanced Babine stocks (including catch, escapement, and surplus production), and the number of recruits produced per spawner, have decreased in recent decades, raising concerns about density dependence in freshwater and/or marine environments. Density dependence can affect productivity at multiple life history stages in both freshwater and marine environments, and at different scales. Some effects may be unique to a single stock or shared between many stocks within a region. For example, freshwater rearing capacity in Babine Lake controls the production of wild and enhanced Babine Sockeye, while density dependence in the marine environment can affect recruitment for Skeena, Nass, and other Sockeye stocks [@Peterman1982]. 

Babine smolt size has decreased continually over time, with the biggest decreases observed in the decades before the BLDP (Appendix \@ref(ChannelReview)). Smolt size is positively correlated with smolt to adult survival (SAS) [@HendersonCass], and size selective survival has been observed for Babine Sockeye [@WestLarkin]. There is a weak positive relationship between SAS and the mean length of smolts exiting Babine Lake, and a decreasing trend for smolt weight with increasing abundance, which suggests that freshwater density dependence may affect the size and survival of Babine smolts, but we do not have sufficient information to assess this at the stock level. While previous studies have concluded that lake rearing capacity is not a limiting factor for Babine Sockeye [@Shortreedetal2000PRModel], updated limnological studies have not taken place since 2013, and no reported data since 2000. 

The HBM results support a common shared year effect among Skeena Sockeye stocks, which suggests that limitations to recruitment occur on an aggregate scale. Covariation in productivity occurs at regional scales for different species and populations of Pacific salmon throughout the Northeast Pacific, [i.e., @PyperetalSpatialCov; @DorneretalCov]. Declines in Sockeye salmon populations in the Northeast Pacific region have been linked to the unprecedented abundance of Pacific salmon currently rearing in the Northeast Pacific following decades of large-scale enhancement of Pink and Chum Salmon originating from Asia and North America [@RuggeroneConnorsSxcomp].

Enhanced Babine stocks represent the largest component of Skeena Sockeye and are a key consideration for developing management targets. While wild Skeena and Nass stocks were the  focus of the biological benchmark analyses presented here, management reference points for Skeena Sockeye will need to consider the large contribution of enhanced Sockeye to the aggregate. From 1970-2020, the enhanced stocks (Pinkut and Fulton) accounted for an average of 67% of the total Skeena return (range 33-83%). Because loading targets for the spawning channels and managed sections of Pinkut Creek and Fulton River are set to achieve constant spawner densities that are maintained to maximize fry production, mathematical spawner-recruitment models such as the Ricker model, which require a range of spawner escapements (i.e., contrast in the data) may not produce useful parameter estimates. For stocks where spawning escapement is fixed, spawner recruit models and are not recommended for developing biological benchmarks or management targets for the enhanced stocks. Although the CSAS review committee generally acknowledged the challenges of conducting spawner recruit analyses for enhanced stocks with low contrast in the spawner escapement data, they recommended including spawner-recruit modelling results for the enhanced stocks in this Research Document. These results are presented in Appendix \@ref(PinkutFultonResults).

Our analyses considered the wild (non-Pinkut and Fulton) of Skeena Sockeye separately. In Sections \@ref(SpnEst) to \@ref(RunReconEst), we summarized estimation methods and run-reconstructions specific to Babine Sockeye, and assessed trends in surplus production. In  Section \@ref(SkeenaExpResults), we examined the ratio of wild and enhanced Skeena Sockeye to develop advice for expanding abundance-based reference points for the Skeena Wild aggregate to account for the enhanced contribution. 


### Salmon Population Diversity

The different Skeena and Nass stocks considered here include dozens of distinct Conservation Units which spawn in tributaries throughout both watersheds. For Skeena Sockeye, non-Babine Sockeye, with run sizes ranging from hundreds to tens of thousands of spawners, represent a comparatively small proportion of the aggregate Skeena Sockeye return but account for most of the genetic diversity of the aggregate. From a conservation perspective, maintaining the evolutionary adaptive potential within a metapopulation is important for conserving resilience to future environmental changes [@Kardos2021GeneticVariation]. Within a metapopulation, asynchronous population dynamics among component populations confer resilience and stability of associated fisheries [e.g., @Hilborn2003Biocomplexity; @Schindleretal2010Portfolio], while more synchronous population dynamics increase variability in returns and for fisheries [@Freshwateretal2015WeakenedPortfolio]. For Skeena Sockeye, the loss of biodiversity since the start of the directed commercial fishery, characterized by decreased wild Skeena Sockeye returns combined with increased proportions of enhanced BLDP-origin Sockeye has been characterized as portfolio simplification [@Priceetal2021PortfolioSimpl].     

Existing genetic variation may allow populations to adapt more quickly to rapid environmental change [@Barrett2008]. Among Skeena and Nass Sockeye stocks, there are numerous examples where stock-level variation in population characteristics is measurable and likely contributes to asynchronous population dynamics among stocks. These Sockeye populations exhibit stock-specific diversity in life history patterns, smolt size, run timing, freshwater age and age-at-return. Diversity in freshwater age structure can buffer a population from poor marine conditions at different life history stages [@Moore2014LifeHistDiv]. Diversity in adult run timing may protect some stocks with different vulnerabilities to large-scale fisheries, while diversity in life history and smolt migration timing may attenuate year-class failures in years of reduced ocean survival [@Beamish2012], or reduce the risk of mismatch between timing of ocean entry and prey availability [i.e., @Satterthwaite2014; @CarrHarrisetal2018SmoltMigration]. 

Direct links between population characteristics and conservation and/or fisheries benefits are difficult to quantify, but there are numerous recent examples of population level changes that have helped to buffer Skeena and Nass Sockeye from low returns. In the Nass, the early-timed sea-type population accounted for up to 30% of the Nass Sockeye aggregate return in 2018 and 2019, when Sockeye returns to Meziadin fell below its escapement goal. Within the Meziadin stock complex, a relatively new spawning population (Strohn Creek), which has appeared in recent decades, now accounts for a substantial proportion of the Sockeye return (M. Cleveland, Gitanyow Fisheries Authority, pers. comm., 2021). For the Skeena, extremely low Sockeye returns in 2013, 2017 and 2019, which were driven by poor returns to Pinkut and Fulton, were buffered to an extent by returns to non-Babine Skeena Sockeye systems, which accounted for up to 25% of the aggregate Sockeye return, compared to just 10% in a typical year (unpublished DFO data).

If the relative abundance, or evenness of the different component populations is taken into account, there has been a substantial decrease in Sockeye diversity among Skeena and Nass Sockeye salmon since the start of large scale directed commercial fisheries at the beginning of the 20th century [@Priceetal2019Scales]. For many Skeena and Nass Sockeye populations, Indigenous history, early settler accounts and recent reconstructed historical abundances provide evidence of much larger returns than recent time series of escapements for these stocks. For instance, Sockeye escapements to the Kitwanga River, which likely exceeded tens of thousands at the start of the 20th century [@Priceetal2019Scales], have recently seen returns as low as 230 spawners in 2018. Kitwanga Sockeye are now the focus of an intensive recovery effort [@Cleveland2019KitwangaRecovery], and recovery planshave been initiated for other Skeena populations that have been considered  conservation concerns at different times, including Lakelse Sockeye and Morice Sockeye. 

An aggregate escapement goal that assumes long-term average productivity and stable stock composition may not protect less productive populations from overexploitation. Reduced diversity and increased synchrony may introduce additional management challenges [@Freshwateretal2020Selectivity]. Maintaining genetic and within-population diversity for aggregate salmon populations may protect fish populations and associated fisheries that depend on them may increase their resilience to environmental change [@Andersonetal2015Portfolio; @Kardos2021GeneticVariation].  


The diversity of Sockeye populations has been a key consideration throughout the Skeena and Nass Sockeye escapement goal review, and was identified as an important priority to address by the bilaterally agreed-upon Terms of Reference and by the TWG and independent reviewers. Some of the aggregation approaches described in Section 2.5, including status-based methods and forward simulation modelling, are more suitable than others, such as estimating the aggregate-level MSY, for incorporating stock-level diversity into management targets. The ongoing management engagement process has focused on using forward simulation modelling, to explore tradeoffs between harvests and biological risks, and the simulation outputs can provide information about what spawner abundances are associated with the highest number of stocks achieving biological objectives. The simulations show that under recent productivity, harvests will peak at lower aggregate spawner targets than would maximize the number of healthy stocks.


### Time-varying Productivity {#ImportantTimeVar}

Sockeye salmon populations are changing rapidly with the cumulative effects of stressors including fishing pressure and climate change. Skeena and Nass Sockeye stocks have seen declining productivity, together with increasing variability and increased frequency of low returns since 2000. Skeena and Nass Sockeye are now among a growing list of major British Columbia Sockeye salmon populations (along with Rivers Inlet, Smith Inlet, and Fraser Sockeye), which once supported large scale Canadian commercial fisheries, that are now constrained by low returns and associated conservation efforts. The four lowest Nass Sockeye returns were recorded from 2017-2022. For Skeena Sockeye, the lowest escapements since the catastrophic Babine landslide in the 1950s occurred in 2013, 2017, and 2019. 

Although the temporal patterns of variation in productivity vary by stock, a general pattern of decline is evident across stocks:

* Patterns in the spawner-recruit data for individual stocks as well as the Skeena and Nass aggregates (i.e., spawners, observed recruits per spawner).
* Ricker residuals (i.e., observed productivity compared to productivity predicted by fitted models).
* decline in the productivity parameter (alpha) for many of the stocks where sufficient data are available to fit stock-specific Ricker models with time-varying productivity. 
* Consistent shared-year effect in the HBM model results across the Skeena Sockeye stocks, finding a similar pattern of decline in productivity as for single-stock Ricker fit for the Skeena aggregate data set.

Further, there is evidence that size-at-age, and fecundity for Skeena and Nass Sockeye have decreased in recent decades. Anecdotally, long time commercial fishermen targeting Skeena Sockeye report switching to smaller-mesh nets (from 5 1/2” to 4 ¾” beginning in the 1980s. Population-level changes in body size, age composition, or size-at-age have been observed in all species of salmon in different regions throughout North America [e.g., @Oke2020RecentDeclinesBodySize; @Ohlbergeretal2020CkEscQual; @SchaulGeiger2016ClimateCoho]. The patterns of decline in overall length and length-at-age for Skeena and Nass Sockeye are consistent with decreases of similar magnitudes that have been observed for Sockeye salmon populations in Southeast Alaska [e.g., @Oke2020RecentDeclinesBodySize]. 

For Skeena Sockeye sampled at the Tyee Test Fishery, length at age decreased by 2-3% for 5, 6 and 7 year old fish and remained constant for 4 year old fish between the 1980s and 2010s. The overall length of Nass Sockeye sampled at Meziadin Fishway since 2010 is substantially less than the historic average, indicating shifts in age composition combined with decreases in body length of sampled fish. The observed decreases in overall size for sampled populations of Skeena and Nass Sockeye, together with observed decreases in fecundity of approximately 13% for fish sampled at the Babine spawning channels, indicate a trend toward decreased reproductive with implications for both wild and enhanced Sockeye populations. Escapement goals that assume constant egg production over time may not account for these patterns of decline in escapement quality.

Together with low returns and apparent declines in reproductive potential, Skeena and Nass Sockeye are facing increased frequencies of extreme environmental conditions. For example, Skeena Sockeye from the 2013 brood year, which was itself the lowest Skeena Sockeye return since the years immediately following the Babine slide, experienced extreme environmental conditions throughout their life history. The spawning channels did not meet their loading targets in 2017, 2017 and 2019, and smolts which migrated to sea in 2015 encountered a marine heatwave that persisted from 2014 to 2016 that was extreme in intensity, geographic range, and the unusual depth of anomalous temperatures [@Rossetal2021Heatwaves]. 4- and 5-year old Sockeye that returned in 2017 and 2018 encountered drought conditions and extreme temperatures during the return migration and on the spawning grounds. While each of these events are system specific, and anecdotal examples of what were previously thought to be rare events, there is no question that extreme events are occurring at an increasing frequency.

Maintaining healthy and diverse Sockeye salmon populations for Skeena and Nass Sockeye will require planning for these extreme events, which may include developing escapement goals and management strategies that can adapt and rapidly respond to changing conditions, such as mitigating for extreme temperatures as has been practiced for Fraser Sockeye [@FraserSKEnroute; @FraseSkMgmtAdjFAO2021]. Maintaining population diversity will also help to buffer for aggregate populations from the likelihood of future catastrophic events, which should be considered in fisheries management, including the development of escapement goals. While fisheries managers cannot predict climate-related or other catastrophic events on a year-to-year basis, the probability of bad outcomes can be reduced by introducing buffers to mitigate risk, and by maintaining stock-level diversity within the Skeena and Nass metapopulations [@Andersonetal2015Portfolio].

A key finding of the data review was that many Skeena and Nass Sockeye stocks and both aggregates had dramatically lower recruitment productivity in recent years compared with the long-term average. Our subsequent analyses focused on exploring the performance of stocks under different productivity scenarios, and specifically compared results generated using spawner recruitment parameters developed using long-term average productivity with recent productivity (Tables \@ref(tab:ProdCompTab2) and  \@ref(tab:ProdCompBMRatioSmsy)). While decision makers need to consider that lower productivities are likely to continue in the future, we included results based on long-term average productivity to illustrate the contrast and magnitudes of difference based on different productivity assumptions. More work needs to be done to incorporate different productivity variations, including different definitions of “recent” productivity, into the current framework.  Potential future work on alternative scenarios may include environment considerations and bound likely futures based on known relationships (potentially stock by stock relationship differences may be leveraged based on existing/ongoing work).


## KEY CONCLUSIONS {#KeyConclusions}


### Alternative SR Model Fits and Productivity Scenarios

#### Alternative SR Model Forms

We explored three alternative SR model forms, which was informative because (1) the comparison between them helped with understanding the properties of each stock-specific SR data set, and (2) parameter estimates from different model forms could be used for different purposes.

For stocks with complete time series, where all three SR model forms could be fitted, the following observations are noteworthy:

* given the strong temporal trends in residuals, the AR1 Ricker fit improved the statistical properties of the fit compared to the Basic Ricker fit. Therefore, we chose parameters from the AR1 fit to generate the long-term-average productivity scenario, where available.
* productivity patterns identified through the TVP model fit with time-varying productivity differed between stocks, and in some cases the productivity patterns tracked the residuals from the Basic Ricker fit very closely (i.e., did not identify a smooth underlying pattern). These highly variable productivity patterns need to be interpreted with caution, but can still serve as a useful source of parameter estimates for alternative productivity scenarios (i.e., give a high contrast to the high and low productivity bookend scenarios). Therefore, we used the TVP parameter estimates for different time periods to generate alternative productivity scenarios, where available. TVP fits generally resulted in more uncertain estimates of the productivity parameter (i.e., wider ln.alpha posteriors), but in more precise estimates of capacity (i.e., narrower Smax posteriors).

#### Biological Benchmarks

The Nass Sockeye aggregate includes 9 different CUs which were combined into 7 stocks for our analyses, while the Skeena aggregate includes 30 extant CUs, which were combined into 24 stocks for our analyses. For both aggregates, spawner-recruitment based biological benchmarks were developed for each of the major contributing stocks. Productivity for the different Nass and Skeena stocks varies across time, with the aggregate stocks, and their largest components (Meziadin and Babine wild stocks) exhibiting near-continual declines in recruits per spawner, and the productivity parameter from single stock fits, since 2000 (Figures \@ref(fig:FitsCompProd1) and \@ref(fig:FitsCompProd2)). Plausible alternative productivity scenarios were developed to characterize high, low, long-term average, and recent productivity scenarios by sampling from the posterior distributions for the Ricker-alpha parameter from the most appropriate available models for each scenario.

Biological benchmarks were estimated for the different Skeena and Nass Sockeye stocks using the parameter distributions generated from the alternative productivity scenarios. These were used to build illustrations of equilibrium probability profiles and aggregate reference points that can be used to inform choices for aggregate escapement goals, once management objectives have been clearly defined.

Extensive testing showed that benchmark estimates for some stocks were highly sensitive to one or more of the following:

* *Data treatments* (Appendix \@ref(AltSRTest)): Filtering out brood years with R/S > 45 made a big difference in benchmark estimates for some stocks, and we consider this a necessary quality-control step. If this results in very different benchmark estimates, it indicates that 1 or 2 extreme values had a big effect, and should be investigated carefully. In contrast, infilling missing values generally had little effect on benchmark estimates, because we infilled  using average values, and the infilling did not generate any extreme points. For stocks where the infilling made a difference in the Basic Ricker estimate, this is due to the age structure of Sockeye salmon. A single missing year of spawner and run size estimates can exclude 3-5 brood years from the SR data set. For example, infilling two return year data points for Kitsumkalum added 8 brood years to the SR data set, but only changed the median Smsy estimate by 6%. Infilling five return year data points for Asitka more than doubled the available SR data from 11 brood years to 24 brood years, but also changed resulting estimates substantially (Smsy: -21%, ln.alpha: -41%).
* *Benchmark calculations* (Appendix \@ref(BMCalcTest)): Estimates of biological benchmarks were insensitive to alternative formulations (<2%), but the success rate varied for alternative implementations of the Sgen optimizer. We used the @Scheuerell2016 method to calculate Smsy, because it is the only exact solution, and the @Connorsetal2022 version of the Sgen optimizer, because it was the only non-brute-force method that did not crash for any of the tested parameter combinations.
* *Bayesian estimation*: Median Bayesian benchmark estimates from the Basic Ricker model fits were similar to the simple deterministic estimates for most stocks. Those stocks that were flagged as more than 25% different from the deterministic estimate (Figure \@ref(fig:FitsCompDetFig)) also had more uncertain Bayesian estimates (i.e., very wide posteriors), and were highly sensitive to alternative capacity priors (Figure \@ref(fig:FitsCompCapPrior)).

While estimates for smaller stocks with noisier and incomplete SR data were generally more sensitive than larger stocks with higher quality data, it was not always the same stocks that were flagged in different sensitivity tests. SR fits  for the enhanced stocks and for aggregate-level SR data, in particular, were highly sensitive to alternative SR model assumptions, even though the quality of each individual spawner and recruit estimate was high.


#### Comparing Single-stock and Aggregate Model Fits

Although calculating SR model fits for aggregate data is computationally simple, the resulting parameters are not appropriate for developing management targets. 

For the Skeena aggregate, the majority of effective spawners, and most of the adult returns, are from the BLDP enhancement facilities on Pinkut and Fulton. We explored model fits for a SkeenaWild aggregate that excluded the enhanced stocks, but this level of analysis masked the diversity of stock-specific productivity patterns identified in the stock-level analyses, particularly the substantial recent decline in productivity for several of the wild Skeena stocks with complete (or infilled complete) time series. 

For the Nass aggregate, two stocks with very different life histories and production dynamics account for most of the abundance. Historically, the Nass run was mostly from Meziadin lake-type Sockeye, but in recent years Lower Nass Sea and River Type Sockeye have increased both in absolute abundance and relative contribution. We consider it more appropriate, therefore, to model these two stocks separately, rather than in a combined SR data set.


#### Comparing Single-stock and Hierarchical SR Model Fits

At the beginning of their WinBUGS manual, @WinBUGSManual remind users in bold, red font: *"Beware: MCMC sampling can be dangerous"*. Computing power continues to increase and MCMC software continues to evolve (e.g., from WinBUGS to JAGS to STAN), but the basic challenge remains the same: If estimates are highly sensitive to model structure and prior assumptions, which version is the most “correct”? 

Extensive sensitivity testing  is standard practice in Bayesian estimation. However, in this project we had the rare opportunity of comparing two completely separate and fundamentally different implementations of the same type of SR model fit to the exact same original data set. Extensive sensitivity testing was conducted for both model implementations, but it was only through direct comparison between the two implementations that we were able to identify sources for the discrepancies. Most of the initial large differences in benchmark estimates were not due to the hierarchical structure, but resulted from alternative specifications of the capacity prior (Smax). Differences were more pronounced for stocks where the data didn't have a strong density-dependent signal (i.e., "noisy" scatterplot), which were flagged as highly sensitive in both sets of analyses. 

The ability to compare independent model implementations served as an important reminder that Bayesian estimation can be very sensitive to prior assumptions and structural differences. Resulting estimates need to be ground-truthed based on biological expertise, and the stock-specific uncertainty of the estimates needs to be considered in subsequent uses of the results. 

The implications of observed differences between estimates will differ depending on how the information is used. For example, if a subsequent decision process focuses on the sum of stock-level Smsy estimates, then large relative differences in the Smsy estimates for a few small stocks will not substantially affect the decision. If, however, a decision process focuses on status assessments that incorporate relative abundance benchmarks, such as Sgen and 80% Smsy, then large differences in benchmark estimates can translate into a very different overall status picture, which could strongly influence decisions that are made based on these assessments.

Similarly, stock-specific estimated patterns in productivity differed between single-stock and hierarchical model fits. Both sets of results can be useful, and  a direct comparison is informative. The shared-year effect in the hierarchical model looks for a common pattern across stocks, and two types of  sensitivity tests can be used to investigate the estimated patterns: 

* *top-down*: McAllister and Challenger (Appendix \@ref(app:HBMFits)) started with a hierarchical model form that included 18 modelled Skeena stocks, and then dropped one or more stocks from the analysis. In these tests, the common shared year effect was not sensitive to alternative combinations of  16-17 stocks.
* *bottom-up*: An alternative approach might be to begin by fitting hierarchical models with smaller groups of stocks and explore how the shared year effect differs for these smaller groups. For example, hierarchical model fits could be tested for (1) just the three wild Babine stocks, (2) all five Babine stocks, (3) all wild  middle Skeena lake-type Sockeye stocks with SR data, (4) all lower Skeena lake-type stocks with SR data, and (5) all upper Skeena lake-type stocks with SR data. Based on these explorations, a hierarchical model with a more nuanced spatial structure could then be developed.


### Alternative Approaches for Developing Aggregate Management Reference Points {#AggregationComparison}

This Research Document presents alternative biological benchmarks for Skeena and Nass Sockeye stocks along with a number of different approaches for developing aggregate reference points. Specific choices about how to use the information presented here, including the eventual choices for updated escapement goals for Skeena and Nass Sockeye will be made once management objectives are defined in subsequent processes, including a multistakeholder evaluation process to select an escapement goal for Skeena and Nass Sockeye which is currently underway.  

While these processes will involve different individuals and will unfold at different timelines, they all rely on the same fundamental information: (1) agreed-upon spawner-recruit data, and (2) agreed-upon approach for fitting spawner-recruit models, and (3) agreed-upon approach for calculating the resulting biological benchmarks. How the data, model fits, and biological benchmarks are considered will also differ between the processes, so we focused on the fundamental information, but also illustrate how it could be used. The approaches presented here are intended to be flexible to incorporating new information if required by the decision-making process, for example it is expected that model fits and biological benchmarks will change as data sets are updated in the future. The data sets and tools, or building blocks, presented here, are intended to provide a starting point for the decision-making process, rather than prescriptive recommendations for the choices that will eventually be made.

#### Requirements for the Escapement Goal Review

Revised escapement goals for Skeena and Nass Sockeye are required for implementing Pacific Salmon Treaty provisions. Following the evaluation of alternative approaches against selection criteria developed by a group of CSAS meeting participants, subsequent work focused on developing a refined version of the forward simulation model, along with equilibrium tradeoff profiles to evaluate tradeoffs during the management engagement process.

* *Equilibrium tradeoff plots*: Equilibrium probability profiles estimate expected yield for a given stock under different productivity scenarios. For example, the equilibrium profiles for Meziadin Sockeye  illustrate the probabilities of attaining 60 or 80% of long-term average MSY, or an equilibrium yield greater than 100,000 under long-term average or recent productivity scenarios (Figure \@ref(fig:ProfileMezidian)). These results suggest that for Meziadin Sockeye, there is a very low likelihood of attaining any of these objectives under current conditions.
* *Simulation-based reference points*: Simulation models can be used to explore tradeoffs between aggregate catch and biological risks to individual populations within each river system, across a range of aggregate escapement goals, and identify aggregate spawning goals that maximize catch, stock health, or other specified performance metrics. For the initial version of this Research Document, we built a simple simulation model to illustrate the type of information that can be incorporated in a management strategy evaluation. The simulation model was then refined following discussion at the CSAS peer review meeting and recommendations from some participants and independent reviewers to incorporate outcome uncertainty and covariation in productivity. These modifications are described in Appendix \@ref(ModelExt).

In this paper we demonstrate the potential benefits of forward simulations that generate expected trajectories under alternative assumptions. A key finding was that the responses to different harvest strategies were highly sensitive to the productivity assumption. Not surprisingly, projected abundances are lower under recent productivity scenarios than under long-term average productivity for most Skeena and Nass Sockeye stocks. The simulation also identified counter-intuitive interactions between productivity scenarios and alternative harvest strategies. For escapement goals below the current interim escapement goals, more stocks achieve the 3-generation objective under the recent productivity scenario than under the long-term average productivity scenario. Under long-term average productivity, aggregate run sizes are larger due to productive large stocks, and these run sizes translate into larger target ER, which in turn impacts less productive, smaller stocks (Figures \@ref(fig:TradeoffPlotSkeenaWildLTAvg) to \@ref(fig:TradeoffPlotNassRecent)). 

Future model extensions could address other considerations, such as area and time-specific differences in harvest impacts, and future productivity. Such expansions need to be carefully bounded to focus on mechanisms that are relevant to the objectives of the analysis.


#### Requirements for Regional Salmon Initiatives

Regional salmon initiatives can also build on the examples of alternative aggregation approaches presented in this paper:

* *Aggregate limit reference points (LRP)*: New guidance [@LRPGuidelinesSAR] identifies two of the building blocks presented in this Research Document as candidate approaches for developing LRPs under the modernized *Fisheries Act* (2019). Specifically, the logistic regression results and illustration of multi-criteria status assessments included here lay the groundwork for a formal LRP process for Skeena and Nass Sockeye.
* *Multi-criteria status assessment*:  We illustrated status considerations for a single metric, abundance relative to biological benchmarks (Sgen, 80% Smsy), but actual status assessments under the  Wild Salmon Policy are based on a combination of multiple criteria (absolute abundance, long-term and short-term trends, distribution). Completing multi-criteria status assessments for Pacific salmon conservation units is a key deliverable of Wild Salmon Policy implementation.


## PRIORITIES FOR FUTURE WORK {#PrioritiesFuture}

Through the CSAS review process in April 2022, the follow-up process on comparing aggregation approaches, discussions with the independent reviewers, and feedback from the ongoing Canadian domestic engagement process, we have identified several critical areas for future work.

Each of these tasks presents considerable work, both in terms of analyses and process, and they cannot all be addressed at the same time, given their interconnectedness and requirement for input from  many of the same people. For example:

* Development of an aggregate LRP consistent with new guidelines requires multi-criteria status assessments of individual conservation units [@LRPGuidelinesSAR]. 
* Exploring alternative strategies for managing the the non-spawning surplus of enhanced Sockeye would require developing an improved and expanded simulation model. 
* Both of these analyses would require substantial participation from the same core group of DFO staff, and would need to be completed while they continue supporting annual operational requirements (e.g., PSC technical committees).

We anticipate that the ongoing Canadian domestic engagement process and bilateral Canada/U.S. discussions through the PSC will provide the next opportunities to develop guidance on priorities and scope for the next phases of work. We summarize potential tasks below to support the scoping discussions.


### Objectives

Regardless of which approach is chosen for developing aggregate management reference points for Skeena and Nass Sockeye, the application of that approach requires clearly defined quantitative objectives. We used various examples of quantitative objectives throughout this Research Document, but the larger escapement goal review process needs to develop four distinct types of agreed-upon objectives:

* *Quantitative objectives for individual wild stocks*: We focused our analyses on simple stock-level objectives linked to the lower and upper WSP benchmarks for relative abundance (Sgen, 80%Smy). However, other biological stock-level objectives should be explored, and stock-specific harvest objectives should be identified where possible (e.g., for specific in-river fisheries) 
* *Quantitative aggregate-level objectives*: Different aggregate-level objectives are either implicitly or explicitly required for applying the alternative approaches for developing aggregate management reference points. Several require an explicit definition of what constitutes success for each component stock, and then an explicit definition of what constitutes success for the aggregate. Focusing on wild stocks only, one example we used was *"At least 80% of stocks in the aggregate should have at least 80% probability of spawner abundance exceeding the upper WSP benchmark  for the relative abundance metric (80% Smsy under long-term average productivity) after 3 generations (simulation years 11-15)"*. However, many variations of objectives like this could be considered and tested, and an agreed-upon shortlist should be developed.
* *Quantitative objectives for individual enhanced stocks*: BLDP enhancement activities are currently managed under the general objective of maximizing smolt production under historical conditions, but this general goal should be reviewed given observed changes in the environment, in wild stocks, and in population dynamics of enhanced stocks. 
* *Quantitative objectives for the total Skeena aggregate, including wild and enhanced stocks*: There is currently no explicit policy guidance regarding the desired balance between biological objectives for wild stocks, production objectives for enhanced stocks, harvest objectives for combined wild and enhanced returns, and the unharvested, non-spawning surplus of enhanced returns. A formal management strategy evaluation (MSE), combining a simulation model and a structured participatory process, offers a comprehensive framework for identifying general objectives and key mechanisms, building a custom simulation model to test alternative scenarios, and refine both the analyses and objectives over time.

### Data

The natural and social system in which we are collectively trying to manage Skeena and Nass Sockeye is rapidly changing and annual assessment programs are adapting to changes in stocks, fisheries, and available tools. As a result, the data foundation available for the Skeena and Nass Sockeye escapement goal review is continuously evolving, which affects the priorities for both analyses and process. For example, emerging DNA-based stock identification may provide better estimates of stock-specific harvest impacts. Similarly, new information is being developed about the most appropriate methods for estimating spawner escapements for two stocks (Bowser, Bear). 

The analyses presented in this Research Document used SR data up to the 2019 returns, after an in-depth review of source data [@SkeenaNassSkDataRep]. Returns in 2020, 2021, and 2022, which are not incorporated in the current analysis, were unusual for many of the stocks, and may substantially affect parameter estimates. As data are updated  and SR parameter estimates revisited, the specific values of biological benchmarks, and the results of any chosen aggregation approach will also change. However, regular updates of run reconstructions, SR fits, status assessments, and simulation reruns are a substantial task. Planning for regular updates needs to fit this task in the context of all the prioritized tasks.     

### Spawner-recruit Modelling and Productivity Scenarios

The SR analyses presented in this paper include extensive sensitivity testing (3 single-stock model forms, 4 alternative capacity priors, alternative data treatments, alternative benchmark estimation approaches, single-stock vs. hierarchical Bayesian fits). Discussions during the peer review process identified several areas for further exploration:

* *Alternative capacity priors*:	We tested four combinations with either uniform or lognormal distributions and either wide or capped bounds on the distribution. The bounds were selected based on available information for each stock, including 
lake capacity estimates based on photosynthetic rate, largest observed spawner abundance, and Bayesian posterior distributions from an initial round of SR model fits. Alternative implementations of the log-normal capacity prior and alternative bounds on the capacity prior could be tested to further explore the sensitivity of resulting SR parameter estimates for stocks with highly uncertain capacity estimates. This was identified as a priority, because large initial differences between the single-stock and hierarchical Bayesian model fits were traced to differences in how the capacity priors were set up.
* *Stochastic simulations of bias*:	Potential biases in SR parameter estimates could arise from different implementation details (e.g., data treatment, model forms, capacity priors, Bayesian MCMC implementation). While we did extensive sensitivity testing, a more formal exploration of potential biases could generate many random data sets for hypothetical stocks with known population dynamics designed to be similar to the different Skeena and Nass Sockeye stocks, then apply the alternative SR model fitting variations to test which version produces estimates most similar to the "true" values.
* *Model Form switching*: For stocks with complete SR data sets (i.e., no missing years) the current implementation of alternative productivity scenarios sampled parameters from the AR1 Ricker fit for the long-term average productivity scenario and from the most recent generation of the time-varying productivity (TVP) Ricker fit for the recent productivity scenario. Some peer review participants expressed concern over potential implications of using different model forms for the productivity scenarios. Alternative versions of the long-term average productivity scenario, based on the TVP model fit could be explored (e.g., sample from all brood years instead of just the most recent generations, or calculate the average ln.alpha across brood years for each MCMC parameter set).
* *Alternative hierarchical structure for the HBM model fit*: As discussed in Section \@ref(KeyConclusions) above, different assumptions about the hierarchical structure of Skeena Sockeye could be tested within the estimation framework developed by McAllister and Challenger (Appendix \@ref(app:HBMFits)).

### Enhanced Pinkut and Fulton

Priorities for future work relate to stock assessment, channel management, interactions with wild stocks, and alternative approaches for developing a combined enhanced-wild harvest strategy. Specific tasks include:

* *Review of channel loading targets*: We included an overview of egg, fry and smolt production data and trends in this report.  An in-depth analysis of enhanced Babine Sockeye production, which is part of a larger review of the effectiveness of salmon hatcheries in the Pacific Region, led by Pacific Salmon Foundation, is ongoing (Cam West, pers. comm., DFO Salmonid Enhancement Program (retired), 2021). However, this work only addresses technical components of channel production, with a focus on enhanced Babine Sockeye. A broader integrated review is needed to determine how channel management should respond to these observed biological changes. 
* *Investigate biological interactions between enhanced and wild populations*: Although egg, fry and smolt production data for Babine Lake suggest freshwater density dependence with reductions in smolt size associated with higher fry production, more work is needed to understand the longer term effects of BLDP enhancement on wild Babine and other Skeena Sockeye populations, including updated information about changes in freshwater rearing capacity for Babine Lake, and a detailed analysis of size selective marine survival and recruitment across a range of ocean conditions. Improvements to genetic resolution between the different Babine stocks may inform a better understanding of the potential for straying between populations.    
* *Investigate indirect interactions between enhanced and wild populations through aggregate abundance, aggregate harvest rules, and mixed-stock fisheries*: This would likely involve an expanded simulation model with a distinct component for population dynamics of the enhanced stocks and a finer resolution of simulated harvest (e.g., different wild and enhanced harvests based on fishery timing, location, and gear).
* *Review of surplus management*: Our SR analyses and resulting building blocks for aggregate management reference points focused on the wild stocks. However, harvest management of Skeena Sockeye needs to find a balance between wild stock considerations, operational considerations for the enhanced stocks, and the interaction between biological surplus from the channel stocks with aggregate harvest rates on the combined returns of wild and enhanced stocks. These challenges, which are not new to Skeena Sockeye fisheries management, apply regardless of the chosen approach for setting aggregate targets (e.g., probability profiles vs. forward simulation).


### Incorporating Biological Considerations in Aggregate Management Reference Points

Changing productivity was identified as the highest analytical priority by scoping workshop participants, technical WG members, and independent reviewers for the Skeena and Nass Sockeye escapement goal review. In this paper we present an approach for selecting SR model parameters that describe alternative productivity scenarios, and we show the implications for resulting estimates of biological benchmarks and any considerations based on these benchmarks. The importance of considering time-varying productivity is discussed in Section \@ref(ImportantTimeVar) above. Specific priorities for future work include:

* *Identifying productivity regimes*: Our analyses included an approach for generating alternative productivity scenarios (i.e., long-term average vs. recent), but we did not analyze observed time trends in productivity to look for evidence with discrete shifts in productivity (i.e., regimes). Analyses like @Rodionov2005Regimes can identify shifting productivity regimes, and assist with bounding future productivity scenarios (e.g., for forward simulation). 
* *Guidance for considering productivity changes in biological benchmarks and management reference points*: Identifying past changes in productivity is an important step, but there are many conceptual challenges with incorporating this information into a management context. For example, if recent productivity is lower than average, then Smsy estimates under a recent productivity assumption will generally also be lower. Should escapement goals be lowered in years of poor productivity, or should they stay the same to avoid ratcheting down abundance over the long-term, or should they increase to speed up rebuilding? A multi-year research initiative started in the summer of 2022 to develop formal guidance  for management considerations under changing productivity (Carrie Holt and Brendan Connors, pers. comm., DFO, 2022).
* *Incorporating demographic changes into escapement goals*: For populations that are undergoing changes in escapement quality such as reduction in body size, fecundity or sex ratio, escapement goals that assume constant egg production may underestimate the number of spawners required to achieve objectives such as maximum sustained yield, or targets for fry production [@Staton2021]. Some of these changes have been observed in Skeena and Nass Sockeye, and a more detailed analysis is needed to assess whether these changes are likely to represent significant shifts in recruitment that will need to be considered in management targets for these stocks. Alternative approaches for incorporating demographic changes into escapement may need to be considered, such as considering escapement goals based on egg production rather than spawner abundance, or incorporating demographic change explicitly into spawner recruitment modeling, like the alternative escapement goal adjustments explored  in @Connorsetal2022 for Yukon Chinook.














<!--chapter:end:04-Discussion.Rmd-->

<!-- In general, you shouldn't need to edit this file with the exception of
the following French/English translation. For a French document, set the following header to: # RÉFÉRENCES CITÉES {-} -->

\clearpage

# REFERENCES {-}


<!--chapter:end:05-references.Rmd-->


<!-- The following code should appear at the beginning of the first appendix.
After that, all subsequent sections will be turned into appendices. -->

`r if(knitr:::is_latex_output()) '\\Appendices'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'`



# TECHNICAL PROCESS PARTICIPANTS  {#app:TWG}


The TWG consists of members from Fisheries and Oceans Canada, North Coast Area First Nations, Pacific Salmon Foundation, and consulting organizations (Table \@ref(tab:TWGTable)). Two independent reviewers were appointed by Canada and Alaska (Table \@ref(tab:ReviewersTable)).



```{r TWGTable, echo = FALSE, results = "asis"}


particpants.src <- read.csv("data/Reference Tables/TWG_Members.csv",stringsAsFactors = FALSE,
														fileEncoding = "UTF-8-BOM") %>% arrange(Type, Name)
  
twg.df <- particpants.src %>%
							dplyr::filter(Type == "TWG") %>%
							select(-Type)

  
csasdown::csas_table(twg.df,
  caption = "Members of the Technical Working Group (TWG).",
  format = "latex",
  #landscape = FALSE,
  font_size = 10) %>%
  kableExtra::column_spec(1, width = "12em") %>%
  kableExtra::column_spec(2, width = "35em") %>%
  kableExtra::row_spec(1:(dim(twg.df)[1]-1), hline_after = TRUE) 



```




```{r ReviewersTable, echo = FALSE, results = "asis"}



reviewers.df <- particpants.src %>%
							dplyr::filter(Type == "Reviewer") %>%
							select(-Type)

  
csasdown::csas_table(reviewers.df ,
  caption = "Independent reviewers for the escapement goal review.",
  format = "latex",
  #landscape = FALSE,
  font_size = 10) %>%
  kableExtra::column_spec(1, width = "12em") %>%
  kableExtra::column_spec(2, width = "35em") %>%
  kableExtra::row_spec(1:(dim(reviewers.df)[1]-1), hline_after = TRUE) 



```



\clearpage
# CHARACTERISTICS of ALTERNATIVE APPROACHES FOR DEVELOPING AGGREGATE MANAGEMENT REFERENCE POINTS {#AggregationAppendix}

This appendix includes one table for each of the alternative aggregation methods. All tables have the same structure: for each criterion, there is a single rating in all capitals (YES/NO/MAYBE), followed by a brief rationale. Table  \@ref(tab:TableAltApproaches) describes the aggregation methods. Table \@ref(tab:TableCriteria) describes the criteria. Table \@ref(tab:TableSummary) summarizes the results across aggregation methods.



\clearpage
(ref:TableCriteriaAggSmsy) Rationale for criteria ratings – Aggregate Smsy estimate. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaAggSmsy, echo = FALSE, results = "asis"}



criteria.table.src <- read_csv("data/AggregationApproachTables/AggregationTable_AppendixDetails.csv")

table.in <- criteria.table.src %>% dplyr::filter(Method == "Agg Smsy") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaAggSmsy)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```




\clearpage
(ref:TableCriteriaSumSmsy) Rationale for criteria ratings – Sum of stock-level Smsy estimates. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaSumSmsy, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Sum of Smsy") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaSumSmsy)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage
(ref:TableCriteriaUmsyComp) Rationale for criteria ratings – Comparison of stock-level Umsy estimates. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaUmsyComp, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Umsy Comp") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaUmsyComp)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```




\clearpage
(ref:TableCriteriaSpnStkEquProf) Rationale for criteria ratings – Stock-level equilibrium profiles based on fixed escapement targets. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaSpnStkEquProf, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Spn Equ Prof") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaSpnStkEquProf)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```



\clearpage
(ref:TableCriteriaERAggEquProf) Rationale for criteria ratings – Aggregate-level equilibrium profiles based on fixed exploitation rate targets. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaERAggEquProf, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "ER Equ Prof") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaERAggEquProf)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```





\clearpage
(ref:TableCriteriaStockStatus) Rationale for criteria ratings – Stock-level status considerations. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaStockStatus, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Stock Status") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaStockStatus)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```







\clearpage
(ref:TableCriteriaLogReg) Rationale for criteria ratings – Logistic Regression. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaLogReg, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Log Reg") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaLogReg)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "9em") %>%
     kableExtra::column_spec(2, width = "37em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```











\clearpage
(ref:TableCriteriaSim) Rationale for criteria ratings – Forward Simulation. Summary rating for each criterion is based on the current implementation of the example in this Research Document. YES means that the current example meets the criterion. MAYBE means that current eample could be modified or expanded to meet the criterion, depending on time and resources. NO means that the criterion cannot be met with this aggregation approach. For the time requirement, SHORT means that it can be applied immediately to the SR parameter estimates. MEDIUM means that at least 6 months will be required for either process (e.g., choice of quantitative objectives) or method developments (e.g., pending publication of guidelines, followed by review of implementation). LONG means that a multi-year process is likely needed for full implementation.

```{r TableCriteriaSim, echo = FALSE, results = "asis"}


table.in <- criteria.table.src %>% dplyr::filter(Method == "Sim") %>% select(-Method)



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","l"),
                  caption = "(ref:TableCriteriaSim)")  %>%
	kableExtra::row_spec(1:dim(table.in)[1]-1, hline_after = TRUE) %>%
     kableExtra::column_spec(1, width = "7em") %>%
     kableExtra::column_spec(2, width = "40em")
   #kableExtra::row_spec(c(1:3,5), extra_latex_after = "\\cmidrule(l){2-3}") 


```







<!--chapter:end:06_01_TWG.Rmd-->

\clearpage
# CUSTOM R PACKAGES AND FUNCTIONS {#SingleStockSRFits}


## *RapidRicker* Package

### Purpose

As part of this project, we started developing the R package *RapidRicker*, which runs spawner-recruit data quality checks, tests the sensitivity of standard biological benchmarks using simple Ricker fits, and implements Bayesian SR fits using the JAGS sampling engine [@Plummer03jags] via the *jags()* function from the *R2jags* package [@R2jags]. 

The motivation for building this package was the large number of stocks covered by the Skeena and Nass Sockeye escapement goal review. Routine aspects of data review, such as checking for potential outliers or concerns regarding contrast, presented a non-trivial challenge in an analysis covering dozens of stocks within 2 aggregates, with data continuously being updated as the data reviews progressed. With the large number of stocks, we also faced the challenge of being consistent across stocks with data treatment choices (e.g., criteria for identifying outliers).

Most of the analyses in this report were implemented using the *RapidRicker* package. A basic worked example and the JAGS code follow. Package functions are available on [Github](https://github.com/SOLV-Code/RapidRicker).

### Setting Up


```{r, eval=F, echo=T}

# Install
install.packages("devtools") # Install the devtools package
library(devtools) # Load the devtools package.
install_github("SOLV-Code/RapidRicker", dependencies = TRUE,
               build_vignettes = FALSE)

# Load
library(RapidRicker)	
library(tidyverse)	

# check the built in data set
?SR_Sample # opens help file
head(SR_Sample) # shows the first few rows

# check the function help files
?checkSRData
?calcDetModelFit
?calcDetRickerBM
?calcMCMCModelFit
?calcMCMCRickerBM 
```


### Run the Data Check

```{r, eval=F, echo=T}

# look at the default criteria for the data check 
flags_default

# apply the data check to data for 1 stock
data.chk <- checkSRData(SR_Sample[SR_Sample$Stock == "Stock1",])
names(data.chk)
print(data.chk$Summary)
print(head(data.chk$Data))

```

### Test the Simple Deterministic Ricker Fits

```{r, eval=F, echo=T}

det.fit <- calcDetModelFit(sr_obj = SR_Sample[SR_Sample$Stock == "Stock1",],
                              sr.scale = 10^6, min.obs=15)
det.fit

det.bm  <- calcDetRickerBM(fit_obj = det.fit,sr.scale = 10^6, 
                    Smsy.method = "Scheuerell2016",
					     Sgen.method = "Connorsetal2022")
det.bm

```


### Test the Bayesian Fits


```{r, eval=F, echo=T}

sr.use <- SR_Sample[SR_Sample$Stock == "Stock1",] %>% select(Year, Spn, Rec,logRpS)
head(sr.use)
sr.scale.use <- 10^6

#default priors and inits
priors.up <- generatePriors(sr_obj = sr.use , sr.scale=10^6, model_type = "Basic",
                            capacity.prior.type = "uniform")
inits.up <- generateInits(priors.up)

test.basic.up <- calcMCMCModelFit(
  sr_obj = sr.use, sr.scale = sr.scale.use  ,
  model.type = "Basic",
  model.file = "BUILT_IN_MODEL_Ricker_UniformCapPrior.txt",
  min.obs = 15,
  mcmc.settings = list(n.chains = 2, n.burnin = 20000, 
  										 n.thin = 60, n.samples = 80000),
  mcmc.inits = inits.up,
  mcmc.priors = priors.up,
  mcmc.output = "post",
  mcmc.out.path = "MCMC_Out",
  mcmc.out.label = "MCMC",
  mcmc.seed = "default",
  tracing = FALSE
)
names(test.basic.up)
head(test.basic.up$Summary)

```



\clearpage
## JAGS Code for single-stock SR model fits


### JAGS Code for Basic Ricker Model

```{r, eval=F, echo=T}
model{
	# adapted from code originally developed by Catherine Michielsens, Sue Grant, 
	# and Bronwyn MacDonald. Modifications based on comments and code samples 
	# from Ann-Marie Huang, Brendan Connors, Charmaine Carr-Harris, and 
	# Wendell Challenger.

    for (i in 1:N) {                #loop over N sample points
      R_Obs[i] ~ dlnorm(logR[i],tau_R)    #likelihood 
      logR[i] <- RS[i] +log(S[i])         # calc log(R) fitted values
       RS[i] <- ln.alpha - beta * S[i]     # ricker model
	  log.resid[i] <-  log(R_Obs[i]) - logR[i] 
   }

    ln.alpha ~ dnorm(p.alpha,tau_alpha)  #prior for ln.alpha 
    beta <- 1/ S.max				     # prior for beta

   # capacity prior: uniform OR lognormal. Use only one!!!!!
   S.max ~ dunif(1/10^6, max.scalar * smax.in )  # data is in millions
   
   S.max ~ dlnorm(log(smax.in), tau_smax) T(0,smax.limit)
	smax.limit <- max.scalar * smax.in # typical default  3 * (Max Obs)
    
    
	# non-updating samples (so can plot priors)
	S.max.prior ~ dunif(1/10^6, max.scalar * smax.in)
	ln.alpha.prior ~ dnorm(p.alpha,tau_alpha)
	
	#prior for precision parameter
    tau_R ~ dgamma(shape.tau_R,lambda_tau_R)  
    sigma <- 1/sqrt(tau_R) 			
    
	# bias correction for lognormal skewness
	ln.alpha.c <- ln.alpha + (sigma * sigma / 2) 

}
```

\clearpage

### JAGS Code for AR1 Ricker Model


```{r, eval=F, echo=T}
model{

# This is a JAGS version of the Ricker model fit with lag-1 autoregression 
# correction (AR1). Adapted from code originally developed by Catherine 
# Michielsens, Sue Grant, and Bronwyn MacDonald. Modifications based on comments
# and code samples from Ann-Marie Huang, Brendan Connors, Charmaine Carr-Harris, 
# and Wendell Challenger. The code is expanded for AR1 based on Eq21 and 22 of  
# Fleischman and Evenson (2010; ADFG FMS10-04). 

# do first year    
R_Obs[1] ~ dlnorm(logR[1],tau_R)    
logR[1] <- log(S[1]) + RS[1]    
RS[1] <- ln.alpha - beta * S[1] + phi * log.resid.0    

# do second year    
R_Obs[2] ~ dlnorm(logR[2],tau_R)    
logR[2] <- log(S[2]) + RS[2]     
RS[2] <- ln.alpha - beta * S[2] + phi * log.resid[1]    
log.resid[1] <-  log(R_Obs[1]) - logR[1]    

#loop over rext of N sample points (starting with the third)    
for (i in 2:N) { 
log.resid[i] <-  log(R_Obs[i]) - logR[i] 
}

for (i in 3:N) {       
R_Obs[i] ~ dlnorm(logR[i],tau_R)  # likelihood 
logR[i] <- log(S[i]) + RS[i]      
RS[i] <- ln.alpha - beta * S[i] + phi * log.resid[i-1] 
} 

ln.alpha ~ dnorm(p.alpha,tau_alpha)            #prior for ln.alpha     
beta <-1/S.max    # prior for beta     

# capacity prior: uniform OR lognormal. Use only one!!!!!
S.max ~ dunif(1/10^6, max.scalar * smax.in )  # data is in millions

S.max ~ dlnorm(smax.in, tau_smax) T(0,smax.limit)
smax.limit <- max.scalar * smax.in # typical default  3 * (Max Obs)
	
# non-updating samples (so can plot priors)
S.max.prior ~ dunif(1/10^6, max.scalar * smax.in)
ln.alpha.prior ~ dnorm(p.alpha,tau_alpha)

tau_R ~ dgamma(shape.tau_R,lambda_tau_R)    #prior for precision parameter     
sigma <- 1/sqrt(tau_R)   # based on Fleishman and Evenson (2010; ADFG FMS10-04)

phi ~ dnorm(0.5,0.0001) #T(0.0001,0.9999)
log.resid.0 ~ dnorm(0,tau.red) #T(-3,3)  
tau.red <- tau.white * (1-phi*phi)     
tau.white ~ dgamma(shape.tauw,lambda_tauw)    

# bias correction for lognormal skewness
ln.alpha.c <- ln.alpha + ((sigma * sigma) / (2 * (1-phi*phi)) ) 
}
```


### JAGS Code for Recursive Bayes Ricker Model with Time-varying Productivity

```{r, eval=F, echo=T}
model{
	# adapted from code by Ann-Marie Huang, which was originally contributed by 
	# Catherine Michielsens. Modifications based on comments and code samples 
	# from Ann-Marie Huang, Brendan Connors, Charmaine Carr-Harris, and Wendell 
	# Challenger.

    for (i in 1:N) {       #loop over N sample points
      R_Obs[i] ~ dlnorm(logR[i],tau_R)       #likelihood 
      logR[i] <- RS[i] +log(S[i])            # calc log(R) 
      RS[i] <- ln.alpha[i] - beta * S[i]     # ricker model
	  year[i]<-i
	  log.resid[i] <-  log(R_Obs[i]) - logR[i]  
	}

    for (i in 2:N){
          ln.alpha[i] <- ln.alpha[i-1] + w[i]
          w[i]~ dnorm(0,tauw)
    }

    #prior for alpha (actually ln.alpha!)
    
    ln.alpha[1] ~ dnorm(p.alpha,tau_alpha)    
    
    # prior for beta
    beta <-1/ S.max					   
	
	# capacity prior: uniform OR lognormal. Use only one!!!!!
   S.max ~ dunif(1/10^6, max.scalar * smax.in )  # data is in millions
	
	# non-updating samples (so can plot priors)
	S.max.prior ~ dunif(1/10^6, max.scalar * smax.in)
	ln.alpha.prior ~ dnorm(p.alpha,tau_alpha)
	
	S.max ~ dlnorm(log(smax.in), tau_smax) T(0,smax.limit)
	smax.limit <- max.scalar * smax.in # typical default  3 * (Max Obs)
	
    tau_R ~ dgamma(shape.tau_R,lambda_tau_R)    #prior for precision parameter
    sigma <- 1/sqrt(tau_R) 			# based on Fleishman and Evenson

	tauw~ dgamma(shape.tauw,lambda_tauw)
    varw<- 1/tauw
	sigw<- 1/sqrt(tauw)

   # bias correction for lognormal skewness
    for (i in 1:N) {  
			ln.alpha.c[i] <- ln.alpha[i] + (sigma * sigma / 2) 
	}
	
}

```



## Benchmark calculation functions {#BMFuns}

### R Code for Smsy Calculation  {#BMFunsSmsy}

*Rapid Ricker* includes four alternative options for calculating Smsy: (1) approximation from @Hilborn1985Proxies, (2) approximation from @PetermanPyperGrout2000ParEst, (3)
explicit solution from @Scheuerell2016, using code from the [samSim Package](https://github.com/Pacific-salmon-assess/samSim), and (4) a brute force approximation.

The main function handles inputs and specifications, and includes three of the four alternative calculation approaches:

```{r, eval=F, echo=T}

#' calcRickerSmsy
#'
#' This function calculates Smsy for a Ricker a,b parameters. Note: This function 
#' DOES NOT apply bias correction on alpha. Whether the output is bias-corrected 
#' estimates or not depends on the par set provided by the user. This keeps the 
#' parameter estimation and benchmark calculation steps clearly separated.
#' 
#' @param X  a data frame with columns ln.alpha, beta
#' @param method  one of "Hilborn1985","Petermanetal2000","Scheuerell2016", or
#'                        "BruteForce"
#' @param sr.scale scalar applied to SR data in the model fitting step, 
#'                 need it here to scale up the Sgen values
#' @param out.type either "BMOnly" or "Full"
#' @keywords Smsy
#' @export

calcRickerSmsy <- function(X, method = "Scheuerell2016",sr.scale =1, 
													 out.type = "Full"){
  
if(!(method %in% c("Hilborn1985","Petermanetal2000","Scheuerell2016",
									 "BruteForce") )){
  warning("Method must be one of Hilborn1985,Petermanetal2000,
  				Scheuerell2016, BruteForce")
  stop()}

X.orig <- X

# check for negative ln.a or b pars
X$ln.alpha[X$ln.alpha < 0] <- NA
X$beta[X$beta < 0] <- NA

do.idx <- !is.na(X$ln.alpha) & !is.na(X$beta)

smsy.est <- rep(NA, dim(X)[1] )

if(sum(do.idx)>0){

if(method == "Hilborn1985") {
  smsy.est[do.idx] <-  X$ln.alpha[do.idx]/X$beta[do.idx] *
  	                       (0.5-0.07*X$ln.alpha[do.idx]) * sr.scale  }

if(method == "Petermanetal2000") {   
  peterman.approx <- (0.5 - 0.65 * X$ln.alpha[do.idx]^1.27 / 
                          (8.7 + X$ln.alpha[do.idx]^1.27))
  smsy.est[do.idx] <- X$ln.alpha[do.idx] * peterman.approx[do.idx] / 
                            X$beta[do.idx]  * sr.scale } 

if(method == "Scheuerell2016") { 
# adapted from samSim package (https://github.com/Pacific-salmon-assess/samSim)
  smsy.est[do.idx] <- (1 - gsl::lambert_W0(exp(1 - X$ln.alpha[do.idx]))) / 
  	                        X$beta[do.idx] * sr.scale  } 
  
if(method == "BruteForce") { 
    smsy.est[do.idx] <-   mapply(smsy.proxy, ln.a = X$ln.alpha[do.idx] ,
                                b = X$beta[do.idx], sr.scale = sr.scale )}   
} # end if any do.idx 

umsy.est <- X$beta * smsy.est/sr.scale

if(out.type == "Full"){return(bind_cols(X.orig,SmsyCalc = method, 
                      	Smsy = smsy.est, Umsy = umsy.est)) }
if(out.type == "BMOnly"){return(bind_cols(Smsy = smsy.est, Umsy = umsy.est))  }

} # end calcRickerSmsy 
```

\clearpage
The brute-force approximation is implemented as a sub-routine:

```{r, eval=F, echo=T}

smsy.proxy <- function(ln.a,b,sr.scale){

if(!is.na(ln.a) & !is.na(b)){
spn.check <- seq((1/sr.scale), 1/b ,length.out = 3000)  
rec.check <-  ricker.rec(S = spn.check,ricker.lna = ln.a, ricker.b = b)
test.df <- data.frame(Spn = spn.check, Rec = rec.check) %>% 
		mutate(Yield = Rec-Spn) %>% arrange(-Rec)
s.msy <- spn.check[which.max(rec.check - spn.check) ]  * sr.scale
}

if(is.na(ln.a) | is.na(b)){s.msy <- NA}

return(s.msy)
}

```


### R Code for Sgen Calculation   {#BMFunsSgen}

*Rapid Ricker* includes four alternative options for calculating Sgen: (1) solver function extracted from @HoltOgden2013, (2) solver function extracted from the [samSim R package](https://github.com/Pacific-salmon-assess/samSim), (3) solver function used in @Connorsetal2022 and generously shared by the lead author, and (4) a brute force approximation.

The main function handles inputs and specifications, and includes three of the four alternative calculation approaches:

```{r, eval=F, echo=T}

#' calcRickerSgen
#'
#' This function calculates Sgen for a set of Ricker ln.a,b,sigma parameters, 
#' and optionally Smsy. NOTE: If method is "HoltOgden2013", then Smsy is always
#'  calculated based on Hilborn (1985) approximation, and if Smsy is provided, 
#'  it will give a warning that it was ignored. Note: This function DOES NOT 
#'  apply bias correction on alpha. Whether the output is bias-corrected 
#'  estimates or not depends on the par set provided by the user. This keeps 
#'  the parameter estimation and benchark calculation steps clearly separated.
#'
#' @param X  a data frame with columns ln.alpha, beta, sigma, and optionally Smsy
#' @param method  one of "HoltOgden2013", "samSim", "Connorsetal2022","BruteForce"
#' @param sr.scale scalar applied to SR data in the model fitting step, 
#'                    need it here to scale up the Sgen values
#' @param out.type either "BMOnly" or "Full"
#' @keywords Sgen
#' @export

calcRickerSgen <- function(X, method = "Connorsetal2022",sr.scale = 1, 
                               out.type = "Full",tracing = FALSE){

if(!(method %in% c("HoltOgden2013", "samSim", "Connorsetal2022",
									 "BruteForce") )){
  warning("Method must be one of HoltOgden2013, SamSim, Connorsetal2022, 
  				BruteForce")
  stop()}

X.orig <- X

# check for negative ln.a or b pars
X$ln.alpha[X$ln.alpha < 0] <- NA
X$beta[X$beta < 0] <- NA

do.idx <- !is.na(X$ln.alpha) & !is.na(X$beta) 
sgen.est <- rep(NA, dim(X)[1] )

if(sum(do.idx)>0){

#---------------------------------------------
if(method == "HoltOgden2013") {

  if(!is.null(X$Smsy[do.idx]) & sum(is.na(X$Smsy[do.idx])) == 0){
  	warning("Smsy provided as input, but not used for this method! ")}
  if(is.null(X$sigma)){X$sigma <- 1}
   sgen.est[do.idx] <- unlist(mapply(Sgen.solver.HO, 
                                a = exp(X$ln.alpha[do.idx]), 
                                b = X$beta[do.idx], 
                                sig = X$sigma[do.idx]))  * sr.scale
} # end if HoltOgden2013

#--------------------------------------------
if(method == "samSim") {

if(is.null(X$Smsy[do.idx]) | sum(is.na(X$Smsy[do.idx])) > 0){
	warning("Need to provide Smsy column in input data frame for this method! ")
	stop()}

     if(is.null(X$sigma)){X$sigma <- 1}


  samsim.out <-  mapply(sGenSolver.samSim.wrapper, ln.a = X$ln.alpha[do.idx], 
                                b = X$beta[do.idx], 
                                sigma = X$sigma[do.idx],
                                SMSY = X$Smsy[do.idx])
   sgen.est[do.idx] <- samsim.out  * sr.scale
} # end if samSim

#---------------------------------------------
if(method == "Connorsetal2022") {

  if(is.null(X$Smsy[do.idx]) | sum(is.na(X$Smsy[do.idx])) > 0){
  	warning("Need to provide Smsy column in input data frame for this method! ")
  	stop()}
  # https://stackoverflow.com/questions/38961221/uniroot-solution-in-r
  bc.out<-   mapply(get_Sgen.bc, a = exp(X$ln.alpha[do.idx]),b = X$beta[do.idx],
  									int_lower = -1, int_upper =  1/X$b[do.idx]*2, 
  									SMSY = X$Smsy[do.idx]/sr.scale)
    sgen.est[do.idx] <- bc.out * sr.scale
}  # end if "Connorsetal2022"

if(method == "BruteForce") {

  if(is.null(X$Smsy[do.idx]) | sum(is.na(X$Smsy[do.idx])) > 0){
  	warning("Need to provide Smsy column in input data frame for this method! ")
  	stop()}

  sgen.est[do.idx] <-   mapply(sgen.proxy, ln.a = X$ln.alpha[do.idx] ,
									b = X$beta[do.idx], 
									Smsy = X$Smsy[do.idx], 
									sr.scale = sr.scale )

  }
} # end if any do.idx

if(out.type == "Full"){
	      return(bind_cols(X.orig,SgenCalc = method,Sgen = sgen.est) %>% 
	      			 	mutate(Ratio = round(Smsy/Sgen,2) )) }
if(out.type == "BMOnly"){return(sgen.est)  }

} # end calcRickerSgen
```


Solver subroutine for the @HoltOgden2013 implementation

```{r, eval=F, echo=T}

Sgen.model.HO <-function(S,a,b,sig,trace = FALSE){
  PR<-a*S*exp(-b*S)
  SMSY<-(log(a)/b)*(0.5-0.07*log(a))
  epsilon.wna=log(SMSY)-log(PR)	#residuals
  epsilon=as.numeric(na.omit(epsilon.wna))
  nloglike=sum(dnorm(epsilon,0,sig, log=T))
  if(is.na(sum(dnorm(epsilon,0,sig, log=T)))==TRUE) print(c(a,b,sig))
  return(list(PR=PR, epsilon=epsilon, nloglike=nloglike))
  #actually returns postive loglikelihood (CH note)
}

Sgen.fn.HO <- function(S,a,b,sig){ -1.0*Sgen.model.HO(S,a,b,sig)$nloglike}	
#gives the min Ricker LL

Sgen.solver.HO <- function(a,b,sig) {
  SMSY<-(log(a)/b)*(0.5-0.07*log(a))

  SRfit=optimize(f=Sgen.fn.HO,interval=c(0, SMSY), a=a, b=b, sig=sig)	 
  # nb: not optim() !!
  return(list(SRfit=SRfit$minimum))  # returns the minimum S
}
```

Solver subroutines for the samSim implementation

```{r, eval=F, echo=T}
sGenSolver.samSim.wrapper <- function(ln.a, b, sigma,SMSY){
  sgen.val <- sGenSolver.samSim( theta = c(ln.a, b, sigma), sMSY = SMSY)
  sgen.out <- as.numeric(sgen.val)
  return(sgen.out)
}

sGenOptimum.samSim <- function(S, theta, sMSY) {
  a = theta[1]
  b = theta[2]
  sig = exp(theta[3])
  prt <- S * exp(a - b * S)
  epsilon <- log(sMSY) - log(prt)
  nLogLike <- sum(dnorm(epsilon, 0, sig, log = T))

  return(list(prt = prt, epsilon = epsilon, nLogLike = nLogLike, S = S))
}

sGenSolver.samSim <- function(theta, sMSY) {
  #gives the min Ricker log-likelihood
  fnSGen <- function(S, theta, sMSY) -1.0 * 
                         sGenOptimum.samSim(S, theta, sMSY)$nLogLike
  fit <- optimize(f = fnSGen, interval = c(0, ((theta[1] / theta[2]) * 
                                                (0.5 - 0.07 * theta[1]))),
                  theta = theta, sMSY = sMSY)
  return(list(fit = fit$minimum))
}
```


Solver subroutines for the @Connorsetal2022 implementation
 
```{r, eval=F, echo=T}
get_Sgen.bc <- function(a, b, int_lower, int_upper, SMSY) {
  fun_Sgen.bc <- function(Sgen, a, b, SMSY) {Sgen * a * exp( - b* Sgen) - SMSY}
  Sgen <- uniroot(fun_Sgen.bc, interval=c(int_lower, int_upper), 
  								a=a, b=b, SMSY=SMSY)$root
  }
```


The brute-force approximation is implemented as a sub-routine:

```{r, eval=F, echo=T}
ricker.rec  <- function(S,ricker.lna,ricker.b) {
	                            exp( (ricker.lna - ricker.b * S) + log(S) )}

sgen.proxy <- function(ln.a,b,Smsy, sr.scale){

if(!is.na(ln.a) & !is.na(b)){

spn.check <- seq((1/sr.scale),1.5*Smsy/sr.scale,length.out = 3000)
rec.check <-  ricker.rec(S = spn.check,ricker.lna = ln.a, ricker.b = b)
s.gen <- min(spn.check[rec.check > Smsy/sr.scale],na.rm=TRUE) *sr.scale
return(s.gen)

}}

```


## ER-based equlibrium profile functions {#EquProfFuns}

This function calculates equilibrium spawner abundance and equilibrium catch under the following assumptions: (1) There is no recruitment or age-at-return process variability (all v_{y,j} are 0); (2)  All populations are fished at equal exploitation rates; (3) The same exploitation rate is used year after year; (4) Productivity of component stocks is stable over time; (5) there are no other significant sources of mortality (e.g., no en-route mortality, no pre-spawn mortality). Function developed from code shared by Brendan Connors (DFO), implementing the approach from @SchnuteKronlund1996Equprof.

The main functions handles inputs, settings for the calculations, and outputs:

```{r, eval=F, echo=T}
#' calcAggEqProf
#'
#' This function calculates equilibrium profiles using eq_ricker_us() for 
#' each MCMC sample in the input file, then generates stock-level and 
#' aggregate-level summaries across MCMC samples.
#' #' @param data.use a data frame with columns SampleID, Aggregate, StkID, 
#'                       Stock , Umsy,Smsy, and Sgen
#' @export

calcAggEqProf <- function(data.use){

for(u.do in seq(0, 1, 0.01)){

print(paste("doing U =",u.do))

  #u.do <- 0.7

  out.raw <- bind_cols(
    data.use %>% select(SampleID,Aggregate,StkID,Stock),
    eq_ricker_us(U_msy = data.use$Umsy, S_msy = data.use$Smsy, 
                       S_gen = data.use$Sgen , U.check = u.do)
  )

  tmp.out.bystk <- out.raw %>% group_by(Aggregate, StkID, Stock) %>%
    summarize(U = median(U),
              NumSamples =  n(),
              NumNA = sum(is.na(S)),
              ProbOverfished = sum(overfished,na.rm=TRUE)/n(),
              ProbExtirpated = sum(extirpated,na.rm=TRUE)/n(),
              ProbBelowSgen = sum(belowSgen,na.rm=TRUE)/n(),
              EqSpn_p10 = quantile(S,probs=0.1,na.rm=TRUE),
              EqSpn_p25 = quantile(S,probs=0.25,na.rm=TRUE),
              EqSpn_Med = median(S,na.rm=TRUE),
              EqSpn_p75 = quantile(S,probs=0.75,na.rm=TRUE),
              EqSpn_p90 = quantile(S,probs=0.9,na.rm=TRUE),
              EqCt_p10 = quantile(C,probs=0.1,na.rm=TRUE),
              EqCt_p25 = quantile(C,probs=0.25,na.rm=TRUE),
              EqCt_Med = median(C,na.rm=TRUE),
              EqCt_p75 = quantile(C,probs=0.75,na.rm=TRUE),
              EqCt_p90 = quantile(C,probs=0.9,na.rm=TRUE),
              .groups = "keep"
    )

  tmp.agg.sums <- out.raw %>% group_by(Aggregate,SampleID) %>%
    summarize(U = median(U),AggSpn = sum(S,na.rm=TRUE), 
              AggCt = sum(C,na.rm=TRUE),
              NumStks = n(),
              NumStksOverfished = sum(overfished,na.rm=TRUE),
              NumStksExtirpated = sum(extirpated,na.rm=TRUE),
              NumStksBelowSgen = sum(belowSgen,na.rm=TRUE),
              .groups = "keep"
    )

  tmp.out.byagg <- tmp.agg.sums %>% group_by(Aggregate) %>%
    summarize(U = median(U),
              NumSamples =  n(),
              NumNA = sum(is.na(AggSpn)),
              NumStksOverfished_p10 = quantile(NumStksOverfished,
                       probs=0.1,na.rm=TRUE),
              NumStksOverfished_p25 = quantile(NumStksOverfished,
                       probs=0.25,na.rm=TRUE),
              NumStksOverfished_Med = median(NumStksOverfished,na.rm=TRUE),
              NumStksOverfished_p75 = quantile(NumStksOverfished,
                       probs=0.75,na.rm=TRUE),
              NumStksOverfished_p90 = quantile(NumStksOverfished,
                       probs=0.9,na.rm=TRUE),
              NumStksExtirpated_p10 = quantile(NumStksExtirpated,
                       probs=0.1,na.rm=TRUE),
              NumStksExtirpated_p25 = quantile(NumStksExtirpated,
                        probs=0.25,na.rm=TRUE),
              NumStksExtirpated_Med = median(NumStksExtirpated,na.rm=TRUE),
              NumStksExtirpated_p75 = quantile(NumStksExtirpated,
                       probs=0.75,na.rm=TRUE),
              NumStksExtirpated_p90 = quantile(NumStksExtirpated,
                       probs=0.9,na.rm=TRUE),
              NumStksBelowSgen_p10 = quantile(NumStksBelowSgen,
                       probs=0.1,na.rm=TRUE),
              NumStksBelowSgen_p25 = quantile(NumStksBelowSgen,
                       probs=0.25,na.rm=TRUE),
              NumStksBelowSgen_Med = median(NumStksBelowSgen,na.rm=TRUE),
              NumStksBelowSgen_p75 = quantile(NumStksBelowSgen,
                       probs=0.75,na.rm=TRUE),
              NumStksBelowSgen_p90 = quantile(NumStksBelowSgen,
                       probs=0.9,na.rm=TRUE),
              EqSpn_p10 = quantile(AggSpn,probs=0.1,na.rm=TRUE),
              EqSpn_p25 = quantile(AggSpn,probs=0.25,na.rm=TRUE),
              EqSpn_Med = median(AggSpn,na.rm=TRUE),
              EqSpn_p75 = quantile(AggSpn,probs=0.75,na.rm=TRUE),
              EqSpn_p90 = quantile(AggSpn,probs=0.9,na.rm=TRUE),
              EqCt_p10 = quantile(AggCt,probs=0.1,na.rm=TRUE),
              EqCt_p25 = quantile(AggCt,probs=0.25,na.rm=TRUE),
              EqCt_Med = median(AggCt,na.rm=TRUE),
              EqCt_p75 = quantile(AggCt,probs=0.75,na.rm=TRUE),
              EqCt_p90 = quantile(AggCt,probs=0.9,na.rm=TRUE),
              .groups = "keep"
    )

    if(exists("out.summary.stk")){ out.summary.stk <- 
                       bind_rows(out.summary.stk,tmp.out.bystk) }
  if(!exists("out.summary.stk")){ out.summary.stk <- tmp.out.bystk }

  if(exists("out.summary.agg")){ out.summary.agg <- 
                       bind_rows(out.summary.agg,tmp.out.byagg) }
  if(!exists("out.summary.agg")){ out.summary.agg <- tmp.out.byagg }

  out.summary.stk <- out.summary.stk %>% arrange(Aggregate, StkID)
  out.summary.agg <- out.summary.agg %>% arrange(Aggregate)

} # end looping through U values

# alternate aggregation calc
out.summary.agg.stk.pm <- out.summary.stk %>% group_by(Aggregate,U) %>%
  summarize(NumStksOverfishedv2p20 = sum(ProbOverfished >= 0.2),
            NumStksExtirpatedv2p20 = sum(ProbExtirpated >= 0.2),
            NumStksBelowSgenv2p20 = sum(ProbBelowSgen >= 0.2),
            NumStksOverfishedv2p40 = sum(ProbOverfished >= 0.4),
            NumStksExtirpatedv2p40 = sum(ProbExtirpated >= 0.4),
            NumStksBelowSgenv2p40 = sum(ProbBelowSgen >= 0.4),
            .groups = "keep"
  )

out.summary.agg <- out.summary.agg %>% left_join(out.summary.agg.stk.pm, 
                       by = c("Aggregate","U"))

return(list(summary.stk = out.summary.stk, summary.agg = out.summary.agg))

}
```

The core calculations are done in  subroutine:

```{r, eval=F, echo=T}

#' eq_ricker_us
#'

#' @param U_msy  point estimate of exploitation rate at MSY, typically 
#'          one MCMC sample. This version uses S_msy and U_msy as inputs. 
#'          eq_ricker_ab() does the comparative calculation using ln.alpha and 
#'          beta inputs. 
#' @param S_msy point estimate of exploitation rate at MSY, typically one 
#'                 MCMC sample
#' @param S_gen point estimate of Sgen, the spawner abundance that allows 
#'                  rebuilding to Smsy in 1 generation in absence of fishing
#' @param U.check vector with ER increments to evaluate, default is 0.5
#' @export
#' @examples

eq_ricker_us <- function(U_msy, S_msy, S_gen, U.check = 0.5) {
  Seq <- ((U_msy - log((1 - U_msy)/(1 - U.check)))/U_msy) * S_msy
  Seq[Seq < 0] <- 0

  Ceq <- (U.check * Seq)/(1 - U.check)
  Ceq[is.na(Ceq)] <- 0
  Ceq[Ceq < 0] <- 0

  overfished <- ifelse(U.check > U_msy, 1, 0)
  extirpated <- ifelse(Seq == 0, 1, 0)
  belowSgen <- ifelse(Seq < S_gen, 1, 0)

  return(data.frame(U = U.check, S = Seq, C = Ceq, 
                       overfished = overfished, extirpated= extirpated,
                       belowSgen = belowSgen))
}
```





<!--chapter:end:06_03-appendix_SingleStockFits.Rmd-->


\clearpage
# HIERARCHICAL BAYESIAN MODEL (HBM) OF SOCKEYE SALMON STOCK-RECRUIT DATA IN THE SKEENA RIVER SYSTEMS {#app:HBMFits}

<!-- 
**FINAL REVISIONS NEEDED: (1) FIX EQUATION THAT HAD TO BE TAKEN OUT, (2) CHECK THROUGH REVIEWS FOR EDITS** 
-->



```{r,echo=FALSE, message=FALSE}
library(tidyverse)
library(csasdown)

HBM.results <- readRDS("data/HBM/HBM_appendix_tables.rds")

pasteList <- function(x, suffix=NULL) {
  n <- length(x)
  if (!is.null(suffix)) x <- paste0(x, suffix)
  if(n == 1) return(x)
  a <- paste(head(x, n=(n-1)), collapse = ", ")
  paste(c(a, tail(x, 1)), collapse = ", and ")
}
# str(HBM.results)

# TASKS
# - Standardize terms: lognormal or log-normal,  capitalize Normal?
# - methods - include JAGS
# - [ ] prettyNum
# -[ ] intro to sensitivity
# - [ ] Citations
# - [ ] shrinkage 
# - [ ] Umsy definition
```

<!-- 
Questions

Normal draws were defined in terms of precision, does the remainder of the document

See equation \@ref(eq:HBMLinRick)
-->

**This appendix contributed by Murdoch McAllister and Wendell Challenger**

*Note: This appendix is a stand-alone contribution to the Research Document. It has been included
as a cross-check on key results (e.g., productivity scenarios), and to establish a link between
the single-stock SR analyses present in the main report and previous work by Korman and English (2013) which used a hierarchical Bayesian approach. This appendix applies the approach of Korman and English (2013) to the updated SR data and includes extensive sensitivity testing to allow more direct comparisons with the single-stock model fits.*

A hierarchical Bayesian model (HBM) that was developed for the estimation of Ricker stock-recruit model parameters for Sockeye salmon stocks in the Skeena river system. There was insufficient time to complete an HBM for the Nass River basin.  This structuring was used because it was assumed the exchangeability in the stock productivity parameter was restricted to stocks within each basin.  Differences in the frequency distribution of life history attributes of stocks such as age composition of spawners, the relative frequency of lake rearing and non-lake rearing life history types and differences in run timings and marine migration routes exist between Sockeye salmon stocks in these two watersheds.  



## Mathematical and Statistical Formulation of the HBM {#app:HBMFits-1}

The Skeena HBM models use a linearized form of the Ricker stock-recruit model:


\begin{equation}
\log{\left(\frac{R_{s,y}}{S_{s,y}}\right)=a_s-}\beta_s\cdot\ S_{s,y}+\epsilon_{s,y}
(\#eq:HBMLinRick)
\end{equation}

where $R_{s,y}$ is the observed abundance of recruits produced in stock $s$ in brood year $y$ by the corresponding abundance of spawners brood year $S_{s,y}$. The parameter $a_s$ is the natural logarithm of the maximum rate of population increase, $\beta_s$ is the stock-specific coefficient for the density-dependent effect of spawner abundance on stock productivity and $\epsilon_{s,y}$ represents stock by  year error term that is assumed to be Normally distributed. 


The HBM framework presented is based on that formulated by @KormanEnglish2013 who estimated stock-recruit parameters for numerous salmon stocks in the Skeena River system.  The HBM presumes, as @KormanEnglish2013 did, that the Ricker $a$ parameter is exchangeable between stocks within a river system with a hyper prior mean and hyper prior standard deviation:

\begin{equation}
  a_s \sim  \ln \textrm{Normal}\left(\mu_a,\ \tau_a\right) 
\end{equation}

where $\mu_a$ is the natural logarithm of the hyper prior median of the Ricker $a$ parameter, and $\tau_a$ is the hyperprior precision of the Ricker $a$ parameter.  The following hyperpriors were assigned for $\mu_a$ and $\tau_a$:
  
\begin{equation}
  \mu_a \sim \textrm{Normal}\left({0.5,\ 10^{-6}}\right)
  \textrm{, and }
  \tau_a \sim  \textrm{Gamma} \left(0.5,\ 0.5 \right).
\end{equation}

@KormanEnglish2013 choose these priors as vague priors and the same hyperpriors were retained in the current formulation.




A informative log-normal prior was also assumed for density-dependency parameter  $\beta_s$ which follows the formulation used in @KormanEnglish2013:  

\begin{equation}
  \beta_s \sim \ln \textrm{Normal}\left(\mu_{\beta,s},\ \tau_{\beta,s}\right)
\end{equation}

where $\mu_{\beta,s}$  was obtained based on results from previous limnological lake productivity analyses that assessed the spawner abundance (i.e., $S_{\textrm{max},s})$ which could generate on average the maximum number of recruits for each stock. The values $\mu_{\beta,s}$  and $\tau_{\beta,s}$ were determined as:

  
\begin{equation}
  \mu_{\beta,s}= \log \left(  \frac{1}{S_{\textrm{max},s}} \right)
  \textrm{,  and  } 
  \tau_{\beta,s}=\left(\frac{1}{\sigma_{\textrm{max},s}}\right)^2.
\end{equation}
  
where $\sigma_{\textrm{max},s}$  is the prior standard deviation in the natural logarithm of $S_{\textrm{max},s}$.  Note that values for $\sigma_{\textrm{max},s}$  are nearly identical to the coefficient of variation (CV) for smaller values of $\sigma_{\textrm{max},s}$ (i.e., about 0.5 and lower) and as such we use CV and $\sigma_{\textrm{max},s}$ interchangeably when describing precision of the prior. That said, the ratio of CV to $\sigma_{\textrm{max},s}$ increases to noticeably larger than 1 (i.e., CV is larger than $\sigma_{\textrm{max},s}$) at larger values of $\sigma_{\textrm{max},s}$ meaning the true CV is larger than stated and the prior will be more diffuse. The values  of $S_{\textrm{max},s}$ and $\sigma_{\textrm{max},s}$ for each stock in the Skeena watershed are provided in Table \@ref(tab:PrSmax).

<!-- TABLE C.1. Smax Priors -->
```{r PrSmax, echo=FALSE, results='asis'}
cap <- "Prior median values for stock-specific $S_\\textrm{max}$ and the prior standard deviation in the natural logarithm of $S_\\textrm{max}$ (i.e., $\\sigma_{\\textrm{max},s}$) based on results from previous lake productivity analyses.  The two instances where three stocks are listed in the same line, the available mean $S_\\textrm{max}$ values were added since the individual rearing lakes for these stocks were geographically very close together and the stock-recruit data for these stocks were thus combined into a single time series of stock-recruit data for stock-recruit parameter estimation."
# data.frame(Yes = 1:10, No = LETTERS[11:20]) %>%
HBM.results$Smax %>%
  filter(Basin == "Skeena") %>%
  mutate(prSmax = round(prSmax)) %>%
  rename(
    `$S_{\\textrm{max},s}$` = prSmax,
    `CV/$\\sigma_{\\textrm{max},s}$` = prCV
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l","l", "l", "r", "r"),
    caption = cap
  )
```
The values for $\sigma_{\textrm{max},s}$ were obtained for most of the Skeena Sockeye salmon stocks from @KormanEnglish2013. It should be noted that a value for $\sigma_{\textrm{max},s}$ of 2.00 makes the corresponding prior for $S_{\textrm{max},s}$ vague and diffuse. For example,  relative to the median, the lower and upper 95% credible intervals would be about fifty times lower and fifty times higher than the median.  In cases where $\sigma_{\textrm{max},s}$ was set at 2.00, at least some members of the Working Group had raised concerns about the empirical basis for the values for $S_{\textrm{max},s}$ that could be obtained for those stocks.  The Babine stocks represented a stock aggregate that was divided five ways, the Asitka was based on an approximation and Morice was a two lake system where only the larger lake was measured. Due to these issues diffuse priors were used for these stocks.

At the time of analysis no lake productivity estimate was available for Asitka.  As a result a crude approximation of the prior median $S_{\textrm{max},s}$ for Asitka was obtained from the product of the lake surface area for Asitka and the average $S_{\textrm{max},s}$ per unit lake area for Skeena watershed lakes for which $S_{\textrm{max},s}$ estimates for Sockeye salmon were available.  Due the large uncertainty associated with $S_{\textrm{max},s}$ for Asitka, a value of 2.00 was assumed for $\sigma_{\textrm{max},s}$ for Askita which makes this into a vague prior.   

For the five Babine Lake stocks there was admittedly highly uncertain over the appropriate stock-specific lake capacity due to uncertainty over how the common resource is shared between the five stocks as juveniles for at least five of the recognized Babine stocks rear in Babine Lake.  The prior median for $S_{\textrm{max},s}$ for each of the five Babine Lake stocks were obtained by dividing the total Babine lake productivity estimate by  five, providing an equal allocation of the shared resource to each Babine stock.  Because the true proportion of lake productivity used by each stock remains unknown a high level of uncertainty (i.e., a $\sigma_{\textrm{max},s}$  value 2.00) was assigned for each for each Babine Lake stock.

As in @KormanEnglish2013 a Normal likelihood function was applied in the HBM and the same prior was applied to the standard deviation in the deviations between predicted and observed productivity:

\begin{equation}
  \log{\left(\frac{R_{s,y}}{S_{s,y}}\right) \sim   \textrm{Normal}\left(\mu_{s,y}^{RS},\  \tau_s \right)} 
\end{equation}

where $\log(R_{s,y}/S_{s,y})$ is the observed natural logarithm of the ratio of recruits produced by spawners (i.e., productivity) in brood year $y$ for stock $s$, $\mu_{s,y}^{RS}$ is the stock-specific productivity predicted by the Ricker stock-recruit model (see equation \@ref(eq:HBMLinRick)), conditioned on parameters $a_s$ and $\beta_s$, and $\tau_s$ is the stock-specific precision used in the likelihood function and is given by:

\begin{equation}
  \tau_s=\frac{1}{\sigma_s^2}
\end{equation}
  
where $\sigma_s$ is the stock-specific standard deviation in the deviates between observed and predicted productivity.  The prior for $\sigma_s$ is the same as that applied by @KormanEnglish2013:

\begin{equation}
  \sigma_s \sim \textrm{Uniform}(0.05,\ 10).
\end{equation}

The original @KormanEnglish2013 HBM did not consider time varying changes in productivity; because this was of interest in the current investigation, the @KormanEnglish2013 HBM was extended to include a common shared year effect on stock productivity (i.e., $\log(R_{s,y}/S_{s,y})$). It is hypothesized that due to the spatial proximity of the Sockeye salmon stocks within a basin (i.e., Skeena basin) and overlap between stocks in migration pathways in freshwater and the marine environment that the annual productivity of different stocks will deviate from Ricker model predictions in the same direction and with similar magnitude in each year.   Therefore, in addition to a stock-specific deviate between the Ricker model prediction of productivity and the observed productivity, a common shared deviate or year effect was also included in the HBM.  The linearized form of the Ricker model that includes the common shared year effect $T_y$ in year $y$ is thus given by:

\begin{equation}
  \mu_{s,y}^{RS}=a_s-\beta_s\cdot S_{s,y}+T_y
\end{equation}
  

The prior for $T_y$ is given by,

\begin{equation}
  T_y \sim \textrm{Normal}\left(0,\ \tau_T\right)
\end{equation}
  
where $\tau_T$  is the prior precision in the common shared year effect that is computed as $\tau_T=1/\sigma_T^2$ with $\sigma_T$ representing the standard deviation in common shared year effects.  The hyperprior for $\sigma_T$  was determined as:

\begin{equation}
  \sigma_T \sim \ln \textrm{Normal} \left(\log{\left(0.8\right),\ \frac{1}{{0.7}^2}}\right).
\end{equation}
  
The prior for $\sigma_T$ was chosen to be mildly informative with the prior central tendency and prior precision set at values to allow the data to inform a large range of values and accommodate potentially large interannual variation in common shared year effects. 

The estimated common shared year effects are expected to represent average deviations from Ricker model stock-productivity predictions across stocks in a given year.  The common shared year effect could potentially result from better than average or worse than average survival rates from natural mortality within a given year experienced by the stocks in either freshwater or saltwater.  However, it could also result from run reconstruction errors, for example, underestimation or overestimation of the total catch. 

## Model Fitting

The MCMC algorithms applied for posterior integration of the HBM included WinBUGS 1.4.3 and JAGS and practically identical parameter estimates were obtained between the two software packages.    Under the initial WinBUGS implementations the burn-in was achieved within about 10,000 iterations and well-pronounced posterior density functions were obtained for parameter estimates.   However, under both software packages, for some of the stocks, the MCMC chains could stray very rarely to extreme low values for the Ricker $\beta$ parameter or extreme high values for the Ricker $a$ parameter.  Extreme low values for the Ricker $\beta$ parameter map out to extreme high values, e.g., ten times the value of the posterior mode, for derived parameters such as the average unfished spawner abundance, $S_0$, or spawning stock abundance associated with the maximum sustainable yield, $S_{msy}$.  Such extreme outlier values were considered to be well outside of the support of the data and may be artifacts of the operation of the MCMC algorithms applied. 

To prevent the inclusion of extreme outlier values, a minimum boundary was applied to the prior for the Ricker $\beta$ parameter.  The prior minimum value for the Ricker $\beta$ parameter was established as follows.  An upper bound on the value for $S_\textrm{max}$ was obtained for each stock as five times the prior central tendency specified for $S_\textrm{max}$ :  




\begin{equation}
  S_{\textrm{max},\textrm{max},s} = 5 \times S_{\textrm{max},s} \textrm{ and } \beta_{\textrm{min},s}=\frac{1}{S_{\textrm{max},\textrm{max},s}}. 
\end{equation}


The adjusted prior for $\beta_s$ that was applied was thus:

\begin{equation}
  \textrm{max}\left(\beta_s \sim \ \ln \textrm{Normal}\left(\mu_{\beta,s},\ \ \tau_{\beta,s}\right),\ \beta_{\textrm{min},s}\right)
\end{equation}

The application of this modified prior for $\beta_s$ thus prevented anomalously low values for $\beta_s$ and anomalously large MCMC values for abundance reference points such as  $S_\textrm{max}$, $S_\textrm{msy}$, and $S_0$. 


The posterior predictive distribution for the Ricker $a$ parameter for an unsampled population, $a_p$, is given by:

\begin{equation}
  a_p \sim\  \ln \textrm{Normal}\left(\mu_a,\ \tau_a\right)
  (\#eq:unsamp)
\end{equation}
  
This distribution reflects the effective prior density function for the Ricker $a$ parameter that was applied to estimate this parameter for each stock. 

To provide approximations of how productivity could be varying systematically over years, representations of time varying productivity were obtained by adding the running $n = 4$ or 5 year averages of the common shared year effects to the $a$ parameter for each stock:

\begin{equation}
  a_{s,y,n}=a_s+\frac{1}{n}\cdot\sum_{i=y-n+1}^{y}T_i
  (\#eq:timevarprod)
\end{equation}


Final estimates of the posterior distributions were derive in JAGS by running six 
independent chains with differen starting points for 100,000 MCMC iterations 
after a burn-in of 20,000. Posterior samples were thinned keeping every 10th 
sample in order to reduce auto-correlation in both fundamental and derived 
parameters. Convergence was assess through a combination of diagnostic plots (e.g., traceplots, posterior distributions and Gelman-Rubin-Brooks plots) and criterion such as Rhat, Gelman and Rubin's 
potential scale reduction factor [@GelmanRubin1992], including the multivariate version [@BrooksGelman1998], and Geweke's diagnostic [@Geweke1992]. In all cases plots showed goods sampling
from the posterior with little to no autocorrelation and all diagnostic 
criterion were within ranges that are generally associated with 
convergence (i.e., Rhat < 1.05, Gelman-Rubin within [0.99, 1.01], and Geweke within [-2,2]).




## Sensitivity Analyses {#app:HBMFits-2}

Some additional model runs were implemented to evaluate some different features of the HBM.  See Table \@ref(tab:SensitivityRuns) for brief descriptions of these additional model runs. We term the model run using above described specifications for the HBM the ''base case''.  Note that the results reported below were obtained from earlier WinBUGS 1.4.3 code versions of the HBM which included a simpler approximation for computing $S_\textrm{MSY}$ [i.e., from Table 7.2 of @HilbornWalters1992] than was used in the main body of the report.  The results from these additional runs are summarized further below.   


  
<!-- TABLE C.2. Sensitivity Analyses -->
```{r SensitivityRuns, echo=FALSE, results='asis'}
cap <- "Description of sensitivity runs to evalute the sensitivity of estimation results to model structure and some key inputs."

data.frame(
  `Run` = c(seq(1,6), "7-24", "25", "26"), 
  Description = c(
    "HBM base case but including Korman and English’s (2013) coding in computing precision in the likelihood function from $\\sigma$",
    "Same as HBM base case but with no prior lower bounds on $\\beta$.",
    "Same as HBM base case but leaving out common shared year effects.",
    "Non-hierarchical model run with no common shared year effect but including the same  $S_\\textrm{max}$ prior information as in the base case HBM.",
    "Same as HBM base case but normal priors on  $S_\\textrm{max}$ instead of the base case lognormal prior on Ricker $\\beta$.",
    "Same as HBM base case but with vague Ricker $\\beta$ priors, but including the prior lower bound on Ricker $\\beta$.",
    "Leave out stock-recruit data in the HBM, one stock at a time.",
    "Leave out stock-recruit data for the Babine Enhanced stocks in the HBM (i.e., Pinkut and Fulton).",
    "Same as HBM base case but leave out Babine Enhanced stocks (i.e., Pinkut and Fulton) and apply vague  $S_\\textrm{max}$ priors to Bear, Kitwanga, and Sustut."
  ),
  check.names = FALSE
) %>%
  csas_table(
    format = "latex", 
    escape = FALSE, 
    font_size = 10, 
    # align = c("p{1.2cm}","p{12cm}"),
    align = c("l", "l"),
    caption = cap, #paste("(tab:SensitivityRuns)", cap)
  ) %>%
  kableExtra::row_spec(seq(1,8), hline_after = TRUE) %>%
  kableExtra::column_spec(2, width = "12cm")
```


## Results

### Shrinkage in the Ricker $a$ Parameter in Going From a Non-HBM to an HBM  {#HBMShrinkage}

```{r, echo=FALSE}
shrink <- (HBM.results$Shrinkage$Shrinkage * 100) %>% range() %>% round
kitwanga <- round((HBM.results$Shrinkage %>% filter(Stock == "Kitwanga") %>% select(Shrinkage) %>% unlist)*100 )
pinkut <- round((HBM.results$Shrinkage %>% filter(Stock == "Pinkut") %>% select(Shrinkage) %>% unlist)*100 )
fulton <- round((HBM.results$Shrinkage %>% filter(Stock == "Fulton") %>% select(Shrinkage) %>% unlist)*100 )
other <- HBM.results$Shrinkage %>% filter(Stock %in% c("Kitwanga", "Pinkut", "Fulton" == FALSE)) %>% select(Shrinkage) %>% unlist %>% range
other <-round(other*100,1)
sig.sd <- (HBM.results$Shrinkage$`Diff.SD` * -100) %>% range() %>% round(digits = 1)  # % reduction
```

In both the non-HBM and HBM models a common shared year effect was estimated to make the shrinkage analysis valid.  Under the non-HBM the prior for the Ricker $a$ parameter for each stock was similarly lognormal, had $a$ prior median of $\log(0.5)$ and precision of 1.  Compared to the non-HBM a moderate amount of shrinkage can be seen seen for the HBM estimates (Figure \@ref(fig:HBMShrinkage)). For example the Asitka stock which had the highest posterior median value for the Ricker $a$ parameter under both models showed a `r -shrink[1]`% decrease in the Ricker $a$ parameter (Table \@ref(tab:HBMShrinkageVal)).  This was the largest percentage decrease among the Skeena Sockeye salmon stocks included in the HBM.  In contrast the Kitwanga stock which had the second lowest posterior median for the Ricker $a$ parameter under the non-HBM showed the largest increase of `r kitwanga`% when going from the non-HBM to the HBM.  In contrast the Pinkut and Fulton stocks which had posterior medians for Ricker $a$ under the non-HBM close to the middle range of the Posterior median estimates showed very little shrinkage, i.e., `r pinkut`% and `r fulton`%, respectively (Figure \@ref(fig:HBMShrinkage), Table \@ref(tab:HBMShrinkageVal)).  For all of the 18 Skeena Sockeye salmon stocks the posterior standard deviation (SD) for Ricker $a$ parameter estimates were all smaller, i.e., `r sig.sd[1]`% to `r sig.sd[2]`% smaller, under the HBM than under the non-HBM (Table \@ref(tab:HBMShrinkageVal)).


<!-- Figure C.1. Shrinkage Plot -->
```{r, echo=FALSE}
cap <- "Shrinkage plot for posterior median values for the Ricker $a$ parameter obtained under non-HBM and HBM models of stock-recruit data for 18 Skeena River Sockeye salmon stocks."
```
```{r HBMShrinkage, echo=FALSE, fig.width=7, fig.cap=cap}
HBM.results$Shrinkage %>%
  select(Stock, ends_with("median")) %>%
  gather(key=Model, value = Median, ends_with("median")) %>%
  mutate(
    Model = str_replace(Model, "[\\.[:space:]]median", "") %>% factor(levels = c("nonHBM", "HBM"))
  ) %>%
  ggplot(., aes(x=Model, y = Median)) +
  geom_point(aes(color = Stock)) +
  geom_line(aes(group = Stock, color = Stock)) +
  theme_classic(14) +
  theme(legend.title = element_blank()) +
  labs(
    y = "Posterior Median"
  )
```

<!-- TABLE C.3. Ricker a shrinkage -->
```{r HBMShrinkageVal, echo=FALSE, results='asis'}
cap <- "Posterior median estimates and standard deviations (SD) for the Ricker $a$ parameter for the 18 Skeena River Sockeye salmon stocks under non-HBM and HBM models  The percentage change shows the percentage change in the parameter estimate in going from the non-HBM to the HBM implementation.  "

HBM.results[['Shrinkage']] %>%
  mutate(
    nonHBM.median = round(nonHBM.median, 3),
    HBM.median= round(HBM.median, 3),
    nonHBM.SD = round(nonHBM.SD, 3),
    HBM.SD = round(HBM.SD, 3),
    Shrinkage = paste0(round(Shrinkage * 100,1), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100,1), "\\%")
  ) %>%
  rename(
    `nonHBM Median` = nonHBM.median,
    `HBM Median` = HBM.median,
    `nonHBM SD` = nonHBM.SD,
    `HBM SD` = HBM.SD,
    `SD \\% Diff` = Diff.SD,
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap #paste("(ref:tabSenRun1)", cap)
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

### Common Shared Year Effect

The common shared year effects represent average deviations from Ricker model stock-productivity predictions across stocks in a given year (Figure \@ref(fig:HBMYrEff)). Estimates varied from year to year with many years being close to the Ricker model predictions, while other years showing notable positive and negative deviations The 1994 common shared year effect represented the largest deviation from Ricker stock-productivity predictions, with a strong reduction in productivity. The 4-year and 5-year rolling means provide smoothed representations of estimated common shared year effects within four and five year blocks and make it easier to visualize potential common shared trends in stock productivity than plots of annual estimates.  The common shared year effects derive from the average of stock productivity deviates across the stocks where stock-recruit data are available in a given year and acquire credibility through commonality in annual productivity deviates across several of the stocks.  In contrast, Kalman filter representations of time varying productivity rely on apparent time series of deviates from Ricker-model predictions of long-term average productivity based on data from a single stock.  The Kalman filter approach does not allow any potential statistical representation of common patterns in productivity deviates between stocks in larger watershed. Also in contrast to the common-shared year effect approach the Kalman filter approach cannot be applied to stock-recruit data time series where there are one or more missing years of data within a time series.

<!-- FIGURE C.2. Common Shared Year Effect -->
```{r, echo=FALSE}
cap <- "Estimated common shared year effect shared across the 18 Skeena stocks with a 4-year rolling mean (A) and a 5-year rolling mean (B) overlaid. Points indicate mean with error bars indicating the 95\\% credible intervals, while line and shading indicates the mean and 95\\% credible intervals for the rolling mean."
```
```{r HBMYrEff, fig.width=7.5, fig.cap=cap}
include_graphics("data/HBM/Appendix-HBM-common_year_eff.pdf")
```

While these yearly deviations, whether from common shared year effects or Kalman filter, could represent improved or diminished survival in the fresh or saltwater system, they could also represent an under or overestimation in total catch, and there is currently no way to distinguish between authentic shared survival rate effects and run reconstruction error effects. As such, negative or positive year effects cannot be directly attributed to either an environmental or run reconstruction effect as the two will be confounded. 

As such, time varying productivity should be viewed as a method to improve model fitting, but does not provide objective criteria that can be used to set or judge management responses.   For example, where there were runs of strongly positive or negative common shared year effects in recent years, this could reflect either runs of either high or low stock productivity shared between stocks.  Alternatively, it could reflect a run of years in which fishery catches for all or most stocks were systematically under-reported or over-reported.  If the latter hypothesis were correct, then any adjustments to harvest control rules, e.g., adjustments to escapement targets, to attempt to respond to apparent productivity changes, could have unintended consequences in achieving fishery and salmon population conservation objectives.

To evaluate the sensitivity of reference points to estimated changes in productivity, high and low productivity periods were identified and the posterior predictive distributions for $S_{msy}$ and \Umsy were computed based on the common shared year effects in these blocks of years based on estimates of time varying productivity by stock (see equation \@ref(eq:timevarprod)).  The years showing the largest mostly positive common shared year effects included 1980-1992 (Fig. \@ref(fig:HBMYrEff)).  The years showing the lowest mostly negative common shared year effects included 1999-2014 (Fig. \@ref(fig:HBMYrEff)).  $S_{msy}$ and \Umsy estimates were significantly lower in the latter block of years with the posterior means for \Smsy and also \Umsy ranging between 21% and 50% lower than those estimates in the high productivity period from 1980-1992 (Tables \@ref(tab:SmsyProd) and \@ref(tab:UmsyProd)).  The 95% credible intervals for the percentage differences for both \Smsy and \Umsy included only negative values (Tables \@ref(tab:SmsyProd) and \@ref(tab:UmsyProd)).  These results suggest that systematic reductions in stock productivity could result in estimates of both \Umsy and \Smsy being lowered.  Should \Smsy be used in forming escapement targets and reference points for fishery management and conservation, lowered productivity could result in lower escapement targets.  Because the \Umsy estimates are highly correlated with the \Smsy estimates, the \Umsy estimates could also be expected to decrease with decreases in \Smsy estimates and it would be appropriate also to reduce harvest rates to accommodate lowered productivity.  However, if only changes in estimates of \Smsy were considered in responses to apparent changes in stock productivity and reductions in harvest rates were not implemented, this could also lead to unintended consequences in meeting stock conservation and fishery objectives.

<!-- TABLE C.4. Smsy in high/low years -->
```{r SmsyProd, echo=FALSE}
cap <- "Comparison of stock-specific $S_\\textrm{MSY}$ in high (i.e., 1980-1992) and low (i.e., 1999-2014) periods of productivity and the percentage differnce between periods."

HBM.results[['Smsy Productivity']] %>%
  mutate(
    High = round(High),
    Low = round(Low),
    Diff      = ifelse(is.na(Diff), NA, paste0(round(Diff*100), "\\%")),
    Diff_2.5  = ifelse(is.na(Diff), NA, paste0(round(Diff_2.5*100), "\\%")),
    Diff_97.5 = ifelse(is.na(Diff), NA, paste0(round(Diff_97.5*100), "\\%"))
  ) %>%
  # select(Low, High, Diff, Diff_2.5, Diff_97.5) %>%
  rename(
    `High` = High,
    `Low` = Low,
    `\\% Diff` = Diff,
    `Lower CI` = Diff_2.5,
    `Upper CI` =Diff_97.5
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 5)),
    caption = cap #paste("(ref:tabSenRun1)", cap)
  ) %>%
  kableExtra::column_spec(2:5, width = "1.5cm")
```

\clearpage
<!-- TABLE C.5. Umsy in high/low years -->
```{r UmsyProd, echo=FALSE}
cap <- "Comparison of stock-specific $U_\\textrm{MSY}$ in high (i.e., 1980-1992) and low (i.e., 1999-2014) periods of productivity and the percentage differnce between periods."

HBM.results[['Umsy Productivity']] %>%
  mutate(
    High = round(High),
    Low = round(Low),
    Diff      = ifelse(is.na(Diff), NA, paste0(round(Diff*100), "\\%")),
    Diff_2.5  = ifelse(is.na(Diff), NA, paste0(round(Diff_2.5*100), "\\%")),
    Diff_97.5 = ifelse(is.na(Diff), NA, paste0(round(Diff_97.5*100), "\\%"))
  ) %>%
  # select(Low, High, Diff, Diff_2.5, Diff_97.5) %>%
  rename(
    `High` = High,
    `Low` = Low,
    `\\% Diff` = Diff,
    `Lower CI` = Diff_2.5,
    `Upper CI` =Diff_97.5
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 5)),
    caption = cap #paste("(ref:tabSenRun1)", cap)
  ) %>%
  kableExtra::column_spec(2:5, width = "1.5cm")
```

\newpage
### Sensitivity Run 1:  Effect of Korman and English’s (2013) Coding Error for their HBM {#app:HBMFits-2-1}
```{r, echo=FALSE}
sig.mean <- (HBM.results$`Run 1`$Diff.mean * 100) %>% range() %>% round
sig.sd <- (HBM.results$`Run 1`$Diff.SD * 100) %>% range() %>% round
```


There was a coding error in the model used by @KormanEnglish2013; their code incorrectly transformed the parameter $\sigma_i$ to $\tau_i$ using the incorrect code:  \texttt{tau[i]<-pow(sd[i],-0.5)} which instead should have been \texttt{tau[i]<-pow(sd[i],-2)}. This coding error changed very little the HBM posterior results for the Ricker $a$ and $\beta$ parameters for the 18 stocks (Table \@ref(tab:SenRun1)).  However, the estimates of the $\sigma_i$ parameter were between `r sig.mean[1]`% and `r sig.mean[2]`%  of those obtained under the HBM base case.   The posterior SDs for $\sigma_i$ ranged between `r sig.sd[1]`% and `r sig.sd[2]`%  larger than those obtained under the base case.  Either the lower or upper boundary point on the prior for $\sigma_i$ was hit for some of the stocks.    If the coding error had persisted, any simulations of stock-recruit data using the estimated values for $\sigma_i$ may have led to results with excessive variability.  It is thus essential for this coding error to be corrected in any further implementations of Korman and English’s (2013) HBM.

<!-- TABLE C.6 Sensitivity Run 1 -->
```{r SenRun1, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $\\sigma$ from the base case (coding error removed) and model run that included Korman and English’s (2013) coding error."

HBM.results[['Run 1']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    Code.Error.mean = round(Code.Error.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    Code.Error.SD = round(Code.Error.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100), "\\%")
  ) %>%
  rename(
    `Base Case mean $\\sigma$` = Base.Case.mean,
    `Code Error mean $\\sigma$` = Code.Error.mean,
    `\\% Diff` = Diff.mean,
    `Base Case SD($\\sigma$)` = Base.Case.SD,
    `Code Error SD($\\sigma$)` = Code.Error.SD,
    `\\% Diff ` = Diff.SD,
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap #paste("(ref:tabSenRun1)", cap)
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```



\clearpage
### Sensitivity Run 2:  Same as HBM Base Case But With No Prior Lower Bound on the Ricker $\beta$ Parameter



When there was no prior lower bound included the Ricker $\beta$ parameter by stock, the estimates of $S_\textrm{max}$ and MSY-based reference points included some extremely high MCMC chain values that were way out in the tails of the Markov chains and very far removed from the range of values with support from the data; for these stocks, posterior means, medians, and probability interval values were highly sensitive to the inclusion of these extreme outlier values in the chains (see Table \@ref(tab:SenRun2) for some example results for $S_\textrm{MSY}$).  It is common in implementations of MCMC implementations such as WinBUGS to set bounds on key variables in the model to prevent extreme outlier values in the chains from affecting the posterior calculations [e.g., @MeyerMillar1999; @MichielsensMcAllister2004].  It is thus recommended that prior lower bounds on the Ricker $\beta$ parameter be implemented for each stock to eliminate this source of bias.

The $U_\textrm{MSY}$ estimates from the HBM were relatively insensitive to lifting the prior maximum bound on $S_\textrm{max}$ (Table \@ref(tab:SenRun2Umsy)).   While posterior means differed by less than a few percent for all stocks, posterior standard deviation values were up to 13\% larger when the prior maximum bound on \Smax was lifted.  


<!-- TABLE C.7 Sensitivity Run 2 -->
```{r SenRun2, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$  from the HBM base case and model run where no prior upper bounds were placed on \\Smax."

HBM.results[['Run 2 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    No.Bound.mean = round(No.Bound.mean),
    Base.Case.SD = round(Base.Case.SD),
    No.Bound.SD = round(No.Bound.SD),
    Diff.mean = paste0(round(Diff.mean*100,1), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 1), "\\%")
  ) %>%
  rename(
    `Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
    `No Bound mean $S_\\textrm{MSY}$` = No.Bound.mean,
    `\\% Diff` = Diff.mean,
    `Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
    `No Bound SD($S_\\textrm{MSY}$)` = No.Bound.SD,
    `\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap, #paste("(ref:tabSenRun2)", cap)
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- TABLE C.8 Sensitivity Run 2 -->
```{r SenRun2Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run where no prior upper bounds were placed on \\Smax."

HBM.results[['Run 2 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean,3),
    No.Bound.mean = round(No.Bound.mean,3),
    Base.Case.SD = round(Base.Case.SD,3),
    No.Bound.SD = round(No.Bound.SD,3),
    Diff.mean = paste0(round(Diff.mean*100,1), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 1), "\\%")
  ) %>%
  rename(
    `Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
    `No Bound mean $U_\\textrm{MSY}$` = No.Bound.mean,
    `\\% Diff` = Diff.mean,
    `Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
    `No Bound SD($U_\\textrm{MSY}$)` = No.Bound.SD,
    `\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap, #paste("(ref:tabSenRun2)", cap)
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Run 3: Same as HBM Base Case But Leaving Out Common Shared Year Effects

```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 3 Smsy']]$Diff.mean * 100)  %>% range() %>% round
sig.sd <- (HBM.results[['Run 3 Smsy']]$Diff.SD * 100) %>% range()  %>% round
```

Significant, strong common shared year effects were estimated for numerous years in the 1960-2014 brood year time series.  Separating out this effect provided more precise estimates of Ricker stock recruit parameters for several of the stocks and more precise estimates of management quantities of interest.  This also allowed for common shared systematic change in stock productivity to be estimated.  The posterior means for the Smsy reference point by stock for example were between about `r sig.mean[1]`% and `r sig.mean[2]`% different between runs when common shared year effects were excluded versus accounted for (Table \@ref(tab:SenRun3)).  When no common shared year effect was included in the HBM, estimates of Ricker stock-recruit parameters and associated management parameters for several of the stocks were on average less precisely estimated with posterior SDs being up to about `r sig.sd[2]`% larger for $S_\textrm{msy}$ for several of the stocks.

The $U_\textrm{MSY}$ estimates from the HBM were relatively insensitive to removing common shared year effects from the HBM (Table \@ref(tab:SenRun3Umsy)).  

<!-- TABLE C.9 Sensitivity Run 3 -->
```{r SenRun3, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the HBM base case and model run where no prior upper bounds were placed on \\Smax."

HBM.results[['Run 3 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    No.TE.mean = round(No.TE.mean),
    Base.Case.SD = round(Base.Case.SD),
    No.TE.SD = round(No.TE.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
  	`No \\newline $T_y$ \\newline mean $S_\\textrm{MSY}$` = No.TE.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
  	`No \\newline $T_y$ \\newline SD($S_\\textrm{MSY}$)` = No.TE.SD,
  	`\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap #paste("(ref:tabSenRun3)", cap)
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- Table C.10 Sensitivity Run 3 -->
```{r SenRun3Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run where no prior upper bounds were placed on \\Smax."

HBM.results[['Run 3 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    No.TE.mean = round(No.TE.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    No.TE.SD = round(No.TE.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
  	`No \\newline $T_y$ \\newline mean $U_\\textrm{MSY}$` = No.TE.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
  	`No \\newline $T_y$ \\newline SD($U_\\textrm{MSY}$)` = No.TE.SD,
  	`\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap #paste("(ref:tabSenRun3)", cap)
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Run 4: Non-Hierarchical Model Run With No Common Shared Year Effect But Including the Same $S_\textrm{max}$ Prior Information As in the Base Case HBM

```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 4 Smsy']]$Diff.mean * 100) %>% range()  %>% round
sig.sd <- (HBM.results[['Run 4 Smsy']]$Diff.SD * 100) %>% range()  %>% round

smsy.diff <- filter(HBM.results[['Run 4 Smsy']], Stock == "Sum Smsy across stocks") %>% select(Diff.mean, Diff.SD) %>% unlist * 100 %>% round

umsy.diff <- HBM.results[['Run 4 Umsy']] %>% select(Diff.SD) %>% unlist %>% max
umsy.diff <- round(umsy.diff*100)
```


When a nonhierarchical Bayesian model(nonHBM) with no common shared year effects was run, estimates of Ricker stock-recruit parameters and associated management parameters for several of the stocks were on average less precisely estimated with posterior SD in Smsy for example on average being about `r sig.sd[1]`% to `r sig.sd[2]`% different between the nonHBM and HBM runs (Table \@ref(tab:SenRun4)).  Percentage differences between the nonHBM and HBM for posterior mean estimates for Smsy parameters ranged by stock between `r sig.mean[1]`% and `r sig.mean[2]`%.  These results indicate that on average Ricker and msy-based parameter estimates are more precisely estimated with the HBM and results for some quantities for some stocks can differ considerably.  However, the sum of the $S_\textrm{MSY}$ estimates across stocks was only about `r round(smsy.diff['Diff.mean'])`% larger under the non-hierarchical model but the posterior SD in the $S_\textrm{MSY}$ values summed over the Skeena River stocks was `r round(smsy.diff['Diff.SD'])`% larger than that for the HBM (Table \@ref(tab:SenRun4)).

The posterior means for  $U_\textrm{MSY}$ were relatively insensitive to fitting a nonHBM, but the posterior SDs for $U_\textrm{MSY}$ mostly increased by up to `r umsy.diff`% larger than those under the HBM  (Table \@ref(tab:SenRun4Umsy)).  

<!-- TABLE C.11 - Sensitivity Run 4 -->
```{r SenRun4, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the base case HBM and nonHBM run."

HBM.results[['Run 4 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    nonHBM.mean = round(nonHBM.mean),
    Base.Case.SD = round(Base.Case.SD),
    nonHBM.SD = round(nonHBM.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
  	`nonHBM \\newline mean $S_\\textrm{MSY}$` = nonHBM.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
  	`nonHBM \\newline SD($S_\\textrm{MSY}$)` = nonHBM.SD,
  	`\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- TABLE C.12 - Sensitivity Run 4 -->
```{r SenRun4Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the base case HBM and nonHBM run."

HBM.results[['Run 4 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    nonHBM.mean = round(nonHBM.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    nonHBM.SD = round(nonHBM.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
  	`nonHBM \\newline mean $U_\\textrm{MSY}$` = nonHBM.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
  	`nonHBM \\newline SD($U_\\textrm{MSY}$)` = nonHBM.SD,
  	`\\% Diff ` = Diff.SD
  ) %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Run 5: Normal Priors on Smax in the HBM Instead of the Lognormal Prior on Ricker $\beta$

```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 5 Smsy']]$Diff.mean * 100) %>% range()  %>% round
sig.sd <- (HBM.results[['Run 5 Smsy']]$Diff.SD * 100) %>% range()  %>% round
```

When a normal prior for $S_\textrm{max}$ was applied instead of a lognormal prior on the Ricker $\beta$ parameter in the HBM, but otherwise using the same input information from the lake productivity analyses, the posterior estimates for several of the quantities became less precise and posterior estimates differed markedly for some of the quantities.  Posterior SDs for $S_\textrm{MSY}$ were for example much larger on average, e.g., between about `r sig.sd[1]`% and `r sig.sd[2]`% larger (Table \@ref(tab:SenRun5)).  Posterior mean estimates for $S_\textrm{MSY}$ differed between the two model runs by `r sig.mean[1]`% to `r sig.mean[2]`%.  Though it appears the same information is used in a normal prior for $S_\textrm{max}$, this prior on average loses information about the Ricker $\beta$ parameter compared to a prior for the Ricker $\beta$ parameter that uses the same $S_\textrm{max}$ information. 

The $U_\textrm{MSY}$ estimates from the HBM were relatively insensitive to replacing the lognormal prior for the Ricker $\beta$ parameter with a normal prior for  $S_\textrm{max}$ (Table \@ref(tab:SenRun5Umsy)). 

<!-- TABLE C.13 - Sensitivity Run 5 -->
```{r SenRun5, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the HBM base case and model run with a Normal priors on $S_\\textrm{max}$."

HBM.results[['Run 5 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    Normal.Prior.mean = round(Normal.Prior.mean),
    Base.Case.SD = round(Base.Case.SD),
    Normal.Prior.SD = round(Normal.Prior.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
  	`Normal Prior mean $S_\\textrm{MSY}$` = Normal.Prior.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
  	`Normal Prior SD($S_\\textrm{MSY}$)` = Normal.Prior.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
<!-- TABLE C.14 - Sensitivity Run 5 -->
```{r SenRun5Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run with a Normal priors on $S_\\textrm{max}$."

HBM.results[['Run 5 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    Normal.Prior.mean = round(Normal.Prior.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    Normal.Prior.SD = round(Normal.Prior.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
  	`Normal Prior mean $U_\\textrm{MSY}$` = Normal.Prior.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
  	`Normal Prior SD($U_\\textrm{MSY}$)` = Normal.Prior.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Run 6: Application of Vague Ricker $\beta$ Priors in the HBM, but Including the Prior Lower Bound on $\beta$
```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 6 Smsy']]$Diff.mean * 100) %>% range() %>% round
sig.sd <- (HBM.results[['Run 6 Smsy']]$Diff.SD * 100) %>% range() %>% round

umsy.diff <- HBM.results[['Run 6 Umsy']] %>% select(Diff.mean) %>% unlist %>% max
umsy.diff <- round(umsy.diff*100)
```

When a vague prior was applied for the Ricker $\beta$ prior for all 18 Skeena Sockeye salmon stocks in the HBM, posterior estimates for the abundance-based management quantities were less precise on average with posterior SDs, e.g., for \Smsy ranging between `r sig.sd[1]`% to `r sig.sd[2]`% larger under the vague priors (Table \@ref(tab:SenRun6)).  Posterior means for $S_\textrm{MSY}$ estimates in the HBM that used vague priors for the Ricker $\beta$ parameter were between about `r sig.mean[1]`% and `r sig.mean[2]`% different from those obtained under the mixture of informed and vague priors in the base case HBM.  The use of informative priors for the Ricker $\beta$ parameter based on prior information on $S_\textrm{max}$ via the lake productivity analyses thus combined with the data to provide more precise stock-recruit parameter estimates that in some cases differed from the less precise estimates given by the stock-recruit data.  


The $U_\textrm{MSY}$ estimates from the HBM were relatively insensitive to prior for all Ricker $\beta$ parameter vague priors though for a few of the stocks the posterior means were up to `r umsy.diff`% larger (Table \@ref(tab:SenRun6Umsy)). 

<!-- TABLE C.15 - Sensitivity Run 6 -->
```{r SenRun6, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the HBM base case and model run with a Normal priors on $S_\\textrm{max}$."

HBM.results[['Run 6 Smsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    Vague.Prior.mean = round(Vague.Prior.mean),
    Base.Case.SD = round(Base.Case.SD),
    Vague.Prior.SD = round(Vague.Prior.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $S_\\textrm{MSY}$` = Base.Case.mean,
  	`Vague $\\beta$ Prior mean $S_\\textrm{MSY}$` = Vague.Prior.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($S_\\textrm{MSY}$)` = Base.Case.SD,
  	`Vague $\\beta$ Prior SD($S_\\textrm{MSY}$)` = Vague.Prior.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- TABLE C.16 - Sensitivity Run 6 -->
```{r SenRun6Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run with a Normal priors on $S_\\textrm{max}$."

HBM.results[['Run 6 Umsy']] %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    Vague.Prior.mean = round(Vague.Prior.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    Vague.Prior.SD = round(Vague.Prior.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean $U_\\textrm{MSY}$` = Base.Case.mean,
  	`Vague $\\beta$ Prior mean $U_\\textrm{MSY}$` = Vague.Prior.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD($U_\\textrm{MSY}$)` = Base.Case.SD,
  	`Vague $\\beta$ Prior SD($U_\\textrm{MSY}$)` = Vague.Prior.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

\clearpage
### Sensitivity Runs 7-25.  Leave Out Stock-Recruit Data in the HBM, One Stock at a Time

To evaluate the relative influence of each stock-recruit data set from each stock, the HBM was run dropping out the stock-recruit data for one stock at a time.  In run 25 the stock-recruit data for both of the Babine Enhanced stocks (i.e., Fulton and Pinkut) were left out of the HBM.  The posterior predictive distribution for the Ricker $a$ parameter for an “unsampled” stock was computed for each of these runs (see equation \@ref(eq:unsamp)).  The posterior predictive distributions were plotted under each of the HBM runs 7-25 and under the base case HBM shows that the posterior predictive distributions were very similar between all HBM runs 7-25 and the base case HBM run (Figure \@ref(fig:jackProd)).  The posterior mean estimates of the time series of common shared year effects also did not change their sign or markedly change in magnitude when the stock-recruit data from one of the stocks or both of the Babine enhanced stocks were removed from the HBM (Figure \@ref(fig:jackTE)).  These results indicate that no one stock, nor the enhanced Babine Sockeye salmon stocks, had a substantive influence on the HBM results.  Furthermore results indicate that regardless of whether Pinkut and Fulton channel data were included or excluded, very similar among stocks posterior distribution of the Ricker $a$ parameter (Figure \@ref(fig:jackProd)) and shared year-effects on productivity (Figure \@ref(fig:jackTE)) were produced.  

The temporal variation in shared year effect estimates represent the combined effects of potential changes in both freshwater and marine processes (Figure \@ref(fig:jackTE)).  The combined effects of spawning channels on recruits/spawner are clearly similar to those of nearby natural spawning streams (Figure \@ref(fig:jackTE)).  However, by providing more spawning gravel, the channels have increased the total abundance of adult recruits and spawners in the Skeena system (Randall Peterman, pers. comm.).

<!-- Figure C.3 - Sensitivity Run 7-25 -->
```{r, echo=FALSE}
cap <- "Posterior predictive distributions for the Ricker $a$ parameter when the Sockeye salmon stock-recruit data from one stock at a time was dropped from the HBM for the Skeena watershed (runs 7-24) and the stock-recruit data were dropped for both of the Babine Enhanced Sockeye salmon stocks, i.e., both Pinkut and Fulton (run 25). Base case posterior predictive distribution is coloured dark grey, with colours used for runs 7-25."
```
```{r jackProd,   fig.cap=cap}
include_graphics("data/HBM/Appendix-HBM-Productivity-jackknife.pdf")
```

<!-- Figure C.4 - Sensitivity Run 7-25 -->
```{r, echo=FALSE}
cap <- "Posterior mean estimates for the common shared year effect when the Sockeye salmon stock-recruit data from one stock at a time was dropped from the HBM for the Skeena watershed (runs 7-24) and the stock-recruit data were dropped for both of the Babine Enhanced Sockeye salmon stocks, i.e., both Pinkut and Fulton (run 25). Base case posterior predictive distribution is coloured dark grey, with colours used for runs 7-25."
```
```{r jackTE,   fig.cap=cap}
include_graphics("data/HBM/Appendix-HBM-yeareffect-jackknife.pdf")
```



\clearpage
### Sensitivity Run 26: Removal of Enhanced Stocks and Vague Ricker $\beta$ Priors for Select Stocks
```{r, echo=FALSE}
sig.mean <- (HBM.results[['Run 26 Smsy']]$Diff.mean * 100) %>% range() %>% round
sig.sd <- (HBM.results[['Run 26 Smsy']]$Diff.SD * 100) %>% range() %>% round


smsy.diff <- HBM.results[['Run 26 Smsy']] %>% 
  # select(Stock, Diff.mean, Diff.SD) %>% 
  arrange (Diff.mean) %>%
  mutate(Base.Case.CV = round(Base.Case.SD /Base.Case.mean*100)) %>%
   mutate(Run.26.CV = round(Run.26.SD /Run.26.mean*100)) %>%
  mutate(Perc.Diff =  round(Diff.mean*100), Perc.SD =  round(Diff.SD*100)) %>%
  arrange(Stock)
smsy.diff2 <- smsy.diff %>% filter(Stock %in% c("Bear", "Kitwanga", "Sustut"))

umsy.diff <- HBM.results[['Run 26 Umsy']] %>% select(Stock, Diff.mean) %>% 
  arrange (Diff.mean) %>% 
  mutate(Perc =  round(Diff.mean*100)) %>%
  tail(n=2) %>%
  arrange(Stock)
# umsy.diff <- round(umsy.diff*100)
```

For the final sensitivity run (hereafter, 'run 26') the enhanced stocks were removed and vague priors were applied to the Ricker $\beta$ prior for Bear, Kitwanga and Sustut, which were stocks highlighted as stocks of interest when the HBM results were compared to results form single stock models.  Estimates of the posterior mean for the abundance-based management quantity \Smsy were generally similar to the base case (i.e., less than 10% change) for stocks where the \Smax prior was not changed relative to the base case. The exception was Babine Late Wild, which had a `r filter(smsy.diff, Stock %in% c("Babine Late Wild"))$Perc.Diff`% reduction in the \Smsy estimate, however Babine Late Wild also had the highest posterior SD estimate of all stocks with a CV of `r filter(smsy.diff, Stock %in% c("Babine Late Wild"))$Base.Case.CV`% and `r filter(smsy.diff, Stock %in% c("Babine Late Wild"))$Run.26.CV`% for  base case and run 26 respectively. For Bear, Kitwanga, and Sustut applying a vague prior resulted in declines in the posterior mean of \Smsy  from `r max(smsy.diff2$Perc.Diff)`% to `r min(smsy.diff2$Perc.Diff)`% relative to the base case. Precision in \Smsy estimates varied from stock to stock ranging from a `r -sig.sd[1]`% reduction in the posterior SD to an increase of  `r sig.sd[2]`% relative to the base case (Table \@ref(tab:SenRun26)). For Bear, and Sustut the posterior SD was reduced (`r pasteList(-filter(smsy.diff, Stock %in% c("Bear", "Sustut"))$Perc.SD, "\\%")` respectively), while for Kitwanga the Posterior SD was increased by `r filter(smsy.diff, Stock %in% c("Kitwanga"))$Perc.SD`%. For stocks where changes were not applied to the \Smax  prior changes to the posterior SD ranged from  `r filter(smsy.diff, Stock %in% c("Bear", "Sustut", "Kitwanga") == FALSE)$Perc.SD %>% min`% to `r filter(smsy.diff, Stock %in% c("Bear", "Sustut", "Kitwanga") == FALSE)$Perc.SD %>% max`%.


The \Umsy posterior estimates  were relatively insensitive to these changes implemented in run 26 (i.e., under 10% change in the posterior mean), the exceptions were `r pasteList(umsy.diff$Stock[1:2])` which had posterior means that were   `r pasteList(umsy.diff$Perc[1:2], "\\%")` larger (Table \@ref(tab:SenRun26Umsy)). 

<!-- TABLE C.17 - Sensitivity Run 26 -->
```{r SenRun26, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $S_\\textrm{MSY}$ from the HBM base case and model run where enhanced stocks were removed and vague $S_\\textrm{max}$ priors  were used for Bear, Kitwanga, and Sustut."

HBM.results[['Run 26 Smsy']] %>%
  filter(Stock %in% c("Pinkut","Fulton") == FALSE) %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean),
    Run.26.mean = round(Run.26.mean),
    Base.Case.SD = round(Base.Case.SD),
    Run.26.SD = round(Run.26.SD),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean \\Smsy` = Base.Case.mean,
  	`Run 26 mean \\Smsy` = Run.26.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD(\\Smsy)` = Base.Case.SD,
  	`Run 26 SD(\\Smsy)` = Run.26.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::row_spec(18, hline_after = TRUE) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```

<!-- TABLE C.18 - Sensitivity Run 26 -->
```{r SenRun26Umsy, echo=FALSE, results='asis'}
cap <- "Posterior means and posterior standard deviations for $U_\\textrm{MSY}$ from the HBM base case and model run where enhanced stocks were removed and vague  $S_\\textrm{max}$ priors were used for Bear, Kitwanga, and Sustut."

HBM.results[['Run 26 Umsy']] %>%
  filter(Stock %in% c("Pinkut","Fulton") == FALSE) %>%
  mutate(
    Base.Case.mean = round(Base.Case.mean, 3),
    Run.26.mean = round(Run.26.mean, 3),
    Base.Case.SD = round(Base.Case.SD, 3),
    Run.26.SD = round(Run.26.SD, 3),
    Diff.mean = paste0(round(Diff.mean*100,0), "\\%"),
    Diff.SD = paste0(round(Diff.SD * 100, 0), "\\%")
  ) %>%
  rename(
  	`Base Case mean \\Umsy` = Base.Case.mean,
  	`Run 26 mean \\Umsy` = Run.26.mean,
  	`\\% Diff` = Diff.mean,
  	`Base Case SD(\\Umsy)` = Base.Case.SD,
  	`Run 26 SD(\\Umsy)` = Run.26.SD,
  	`\\% Diff ` = Diff.SD
  )  %>%
  csas_table(
    format = "latex",
    escape = FALSE, 
    font_size = 10,
    align = c("l", rep("r", 6)),
    caption = cap 
  ) %>%
  kableExtra::column_spec(2:7, width = "1.5cm")
```


## Summary

HBM analysis provides an effective approach to formulating priors for key parameters in Bayesian ecological models and has often been applied to estimate productivity parameters for Pacific and Atlantic salmon and other fish species populations [@PrevostOthers2003; @MichielsensMcAllister2004; @SuPetermanHaeseker2004; @Clark2004; @ForestOthers2010; @PulkkinenOthers2011].  The diagnostics shown in this appendix which are routine to HBM analysis [e.g., @GelmanOthers2004; @MichielsensMcAllister2004; @ForestOthers2010] support the basic HBM assumption of exchangeability of the datasets from all 18 stocks with regards to the Ricker  $a$ parameter.  We did not find evidence to support the notion that the two enhanced stocks have consistently different productivity than the non-enhanced stocks.  This is for example based on results (1) from both HBM and non-HBM runs shown in Table \@ref(tab:HBMShrinkageVal) where the posterior means for the Ricker $a$ parameter for the two enhanced stocks were neither consistently higher nor lower than estimates from the 16 non-enhanced stocks, and (2) from the jackknife analysis which showed that the posterior predictive distribution for Ricker $a$ was highly insensitive to leaving out the two enhanced stocks, either one at a time or both at the same time (Figure \@ref(fig:jackProd)).  

Comparisons from the non-HBM and HBM also show that there was a moderate but not extreme amount of shrinkage in Ricker $a$ parameter estimates, where the maximum percentage change in Ricker $a$ was no more than 20% and less than 10% in 13 of the 18 stocks when going from the non-HBM to the HBM.  Shrinkage is to be expected in HBM analysis and the amount of shrinkage found here was not excessive and not so for the two enhanced stocks.  The posterior SDs of the Ricker $a$ parameter for the two enhanced stocks from the non-HBM and HBM (Table \@ref(tab:HBMShrinkageVal)) were also neither consistently larger nor smaller than the posterior SDs for this parameter for the 16 wild stocks.  The estimates of derived parameters such as \Smsy for the two enhanced stocks were also not excessively imprecise with posterior CVs for \Smsy being 0.56 for Pinkut and 0.37 for Fulton (Table \@ref(tab:SenRun2)).  These posterior CVs are well within the range of posterior CVs for fish stock parameters commonly estimated using Bayesian methods. <!--These results thus also refute the notion that due to lack of contrast in the spawner data from the practice of channel loading, the parameter estimates obtained from fitting a Ricker model to their stock recruit data will be excessively imprecise.  -->

Estimation of common shared year effects showed that the estimated effects were statistically significant with 95% PIs not overlapping with zero for 18 of the 55 years.  The estimated effects were highly pronounced in some years, and could change abruptly and considerably in relatively few years.  For example, the estimated effects went from the absolute minimum to the absolute maximum extreme in three years, i.e., -1.37 in 1994 to +1.02 in 1997.  And the most pronounced year effect of all could be linked to epizootic disease event, e.g., in 1994, indicating that at least some of the estimated effects could be attributed to ecological events and the notion that the estimated effects were all due to stock assessment errors is unlikely.  

The above results on common shared year effects suggest that caution is needed should it be of interest to implement random walk models for the Ricker $a$ parameter.  Random walk models typically assume that there exists positive correlation between adjacent years, e.g., for the Ricker $a$ parameter, and tend to provide smoothed trajectories of parameter estimates.  When estimates of common shared year effects from models that exclude random walk jump abruptly been years and shift from minimum to maximum value in only a few years, bias in annual estimates of the Ricker $a$ parameter could thus be expected from random walk models for this parameter.  The common shared year effect also suggested, despite frequent extreme variation between years, extended periods of on average potentially higher productivity in the 1980s to early 1990s and lower than average productivity from 2003-2014.  Estimates of quantities of interest such as \Smsy were also moderately sensitive to the inclusion and exclusion of common shared year effects in the HBM (Table \@ref(tab:SenRun3))).  

Careful attention to how the prior for \Smax was applied in MCMC was also an important consideration in the analysis.  In MCMC integration, values for estimated parameters in Markov Chains even after burn in can wander considerably and occasionally jump to extreme values well outside of the area of support of the data when no constraints are imposed.  The presence of extreme values in approximations of posteriors from MCMC can potentially cause pronounced bias in the estimated parameters.  We found that this was the case in the application of the non-HBM and HBM and extreme parameter values and the incidence in MCMC chains of quantities of interest such as Smsy could be eliminated by implementation of upper and lower bounds for key parameters such as \Smax or Ricker $\beta$.  It is thus appropriate to apply upper and lower bounds for key parameters in MCMC code to prevent these rare but extreme values from occurring and biasing posterior results.  

<!--chapter:end:06_04-appendix_HBM.Rmd-->


# SENSITIVITY TESTS: DATA TREATMENT AND BENCHMARK CALCULATIONS

## Spawner-Recruit Data Filtering and Infilling {#AltSRTest}

### Purpose

We filtered out implausible spawner-recruit observations and infilled gaps to allow fitting model forms that require complete time series (Section \@ref(AvailableSRData)). We tested the effect of alternative data treatments on benchmark estimates from the basic Ricker model.

Infilling a few return years can drastically increase the number of brood years available for spawner-recruit analyses.  For example, if a single spawner estimate is missing from the time series, then recruits cannot be calculated for 3-6 earlier brood years, depending on the age composition for the stock. If there are several gaps, many brood years may have incomplete cohort information and can't be used in the analyses. 

### Methods

We applied two alternative data filter options and then either infilled or didn't infill 1-yr gaps in spawner estimates. Infilled spawner values were calculated as the average of previous and subsequent estimates, and then the corresponding run size was calculated using the year-specific exploitation rate estimate from the run reconstruction models. The infilled spawner and run size estimates were then used in the recruit calculation based on on available age composition data.

This generated six alternative versions of the spawner-recruit time series: 

* *Main*: original data set generated by the data review documented in @SkeenaNassSkDataRep
* *Filter1k*: exclude brood years where R/S > 1,000 
* *Filter45*: exclude brood years where R/S > 45
* *Main_Infill*: original data with infills where possible
* *Filter1k_Infill*: Filter1k data with infills where possible
* *Filter45_Infill*: Filter45 data with infills where possible

This sensitivity test applied the Basic Ricker model (Section \@ref(ModelForms)) with capped uniform capacity priors (Section \@ref(Priors)) to all stocks where any filtering or infilling occurred. The Basic Ricker model is the only one that can be applied to all stocks, because it does not require a continuous time series.

### Results


There were very few cases where a filtered year could be infilled afterwards (Table \@ref(tab:AltSRTestTab1)). The number of infilled return years and resulting additional brood year estimates varied between stocks. In some cases, a few infills allowed for many additional brood year estimates. For example, infilling spawner and run size estimates for Bear made it possible to complete recruit estimates for another 13 brood years (from 36 to 49 data points). 

Benchmark and parameter estimates were quite stable across data variations for some stocks (e.g., Bear, Johnston, Sustut, Kitsumkalum, Mcdonell), but very sensitive for others (e.g., Kwinageese, Swan/Stephens). 



### Conclusions

We chose to use the *Filter45_Infill* version of the data for the analyses presented in this Research Document, because it excluded several extreme outliers and it completed the time series for several stocks, allowing AR1 and TVP models to be applied.



(ref:AltSRTestTab1) Summary of Filtering and Infilling Test. For each data version, table lists the number of spawner-recruit data points (BrYr), the number of filtered years that were infilled and included (Filter), the number of years the were infilled (Infill), and the resulting % change in median posterior estimates of Ricker parameters and standard benchmarks. All SR model fits used the Bayesian Basic Ricker (BR) with capped uniform prior (cu), with the same MCMC settings (as described in Section \@ref(SRFitting)).

```{r AltSRTestTab1, echo = FALSE, results = "asis"}


table.in <- alt.sr.test1 %>% mutate_all(as.character) %>% select(-Model)

# https://stackoverflow.com/questions/36084147/index-of-the-first-occurence-of-each-value-in-a-vector
lines.idx <- which(!duplicated(table.in$Stock))[-1]  # drop first element


table.in[is.na(table.in)] <- ""
table.in$Stock[duplicated(table.in$Stock)] <- ""

col.names.use = c("Stock","Version","BrYr","Filter","Infill","beta","ln.alpha","Seq","Smsy","Sgen")



   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
	 mutate_all(function(x){gsub("_", "\\\\_", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l",rep("r",8)),
                  caption = "(ref:AltSRTestTab1)", col.names = col.names.use ) %>%
   add_header_above(c(" " = 1, "Data" = 4, "Change in median estimates (%)" = 5)) %>%
     kableExtra::row_spec(lines.idx -1, hline_after = TRUE) #%>%
     #kableExtra::column_spec(3, width = "10em") %>%
     #  kableExtra::row_spec(c(1:5,7:14,16:22), extra_latex_after = "\\cmidrule(l){2-8}") 

```


\clearpage
## Infilling with a Generic State-Space Model  {#StateSpaceTest}

### Purpose

The infilling approach and sensitivity test summarized in Section \@ref(AltSRTest) were debated at the peer review meeting in April 2022, and further sensitivity testing of the infill approach was requested. 

Rather than setting up a bootstrap test to evaluate sensitivity, we decided to test a Bayesian approach that has been commonly used for Alaskan and northern transboundary salmon escapement goal analyses: a  Bayesian state-space model that integrates the run reconstruction and spawner-recruit parameter estimation steps into a single model fit [e.g., @BernardJones2010AlsekCk; @HamazakietalKusko2012; @Fleischmanetal2013CJFASStateSpace; @FleischmanMcKinleyKenai2013; @MillerPestalTakuSk; @Connorsetal2022]. 

In this type of model we don't need to infill missing brood years up front to fit AR1 or TVP model forms, but instead the model searches for SR parameters *and* annual estimates of run reconstruction components (e.g., spawners, harvest, age composition) that *together* give the best fit. Any missing brood years are filled in as part of the Bayesian run reconstruction.

Previous applications have been highly case-specific in terms of the run reconstruction components and their prior distributions. For example:

*  @Fleischmanetal2013CJFASStateSpace modelled the Karluk River Chinook run based on a weir count in the lower river, three different fisheries below the weir (subsistence, recreational, commercial), and a recreational fishery above the weir. Observation errors were specified based on the estimate type: weir counts and commercial harvest estimates based on fish sales slips were considered precise, but recreational harvest  estimates based on mail-in surveys were considered more uncertain.
* @FleischmanMcKinleyKenai2013 modelled the late run of Kenai River Chinook using eight components covering various time periods and locations: multi-beam sonar, in-river test fishery, split-beam sonar,  lower river sport fishery, commercial set-net fishery, sonar echo-length, radio-telemetry capture-recapture estimates, and genetic capture-recapture estimates.
* @MillerPestalTakuSk modelled the Taku Sockeye run reconstruction based on three components: in-river mark-recapture estimates at the border, below-border harvests, and above border harvests.

This level of detail is prohibitive for our project covering 20 stocks in two aggregates. However, a generic version of an integrated run reconstruction and spawner-recruit model could be applied efficiently across multiple stocks while providing some flexibility for stock-specific considerations. Such a generic state-space model is being developed by Toshihide Hamazaki (ADF&G), who generously shared an [interactive online prototype](https://hamachan.shinyapps.io/Spawner_Recruit_Bayes/) implemented in Shiny-R. We refer to this tool as the *Hamazaki App* throughout the paper.

The Hamazaki App allows users to fit alternative SR models, explore standard probability profiles based on the SR parameters (e.g., probability of achieving at least 75% of MSY at different fixed escapement targets), and even generate simple forward simulations with different types of harvest strategy. The state-space option in the Hamazaki App implements the methods described in @HamazakietalKusko2012, but simplifies the run reconstruction to three components:  Harvest estimates, either escapement or run, and run age composition.  For each annual abundance observation, users can specify a level of uncertainty, expressed as a CV, and for run age composition a weight to be used, expressed as an effective sample size (*efn*). With a structure like this, users can capture changes in assessment approach over time (e.g., earlier data based on aerial surveys can be assigned a larger CV than more recent estimates from a capture-recapture program). Individual run age composition observations that are considered very poor can be down weighted by assigning higher CV (e.g., if the weir was washed out partway through the season and the estimate was expanded to account for it) or lower efn (e.g., if a year has fewer completed age readings).


### Methods

We used the Hamazaki App to test 10 alternative versions of SR model fit, covering three Ricker model forms (Basic, AR1, and TVP; Section \@ref(ModelForms)), two estimate types (regular, state-space), and two data sets (with or without infilling). Only the Basic Ricker model could be applied to data without infilling with the regular estimation approach, but in the state-space approach all three model forms could be applied.

We tested these alternatives on two stocks: Kwinageese, which has a shorter time series and four missing brood years, and Lakelse, which has a longer time series and two missing brood years (Figure \@ref(fig:SRDataOverview)). For both stocks, we assigned moderate uncertainty to the spawner and run data (CV = 0.2) and large effective sample size (efn = 100). The "no infill" version of the data for the state-space estimates used the infilled numbers in order to populate all the fields in the data file, but assigned much larger uncertainty (CV = 0.6) and a very low effective sample size (efn = 0), so that the state-space model puts very little weight on the infilled values in the estimation step. The "infill" version of the data for the state-space estimates used the infilled numbers and assigned a large effective sample size (efn = 100), so that the model treats the infilled values just like  observed values.

Note that results for the time-varying productivity model (TVP) are not directly comparable to our results. The Hamazaki App reports average parameter and benchmark estimates across all brood years as the default, and those estimates are reported here. However, in our analyses we subsampled from various time windows (Section \@ref(ModelSelection)) to generate alternative productivity scenarios (e.g., last 2 generations). The Hamazaki App also identifies shifts in productivity regimes and generates benchmark estimates for each regime, but we did not fully explore this feature, and do not report the results here.


### Results

For both stocks, Bayesian parameter estimates for all 10 alternative fits converged and generated median posterior estimates of biological benchmarks (Figure \@ref(fig:StateSpaceComp), Tables \@ref(tab:StateSpaceTab1) and \@ref(tab:StateSpaceTab2)). However, the sensitivity of estimates differed between stocks and varied between benchmarks: (1) Benchmark estimates were less sensitive than abundance estimates for individual brood years, (2) Smax and Seq estimates were more sensitive than Smsy estimates; (3) Lakelse estimates were more sensitive than Kwinageese estimates, even though Kwinageese has fewer brood years of SR data and has more missing years in the time series.

For all state-space model fits, the posterior distribution of spawner estimates was more uncertain (i.e., wider) with the "no infill" version of the data (with larger CV on the input values) and the median estimate differed depending on the SR model form (Figure \@ref(fig:StateSpaceComp)). The difference in posterior median abundance estimates was larger for Lakelse than for Kwinageese.

Median posterior benchmark estimates for Kwinageese are so similar across the 10 alternative fits that they are identical for practical purposes (Table \@ref(tab:StateSpaceTab1)). A more in-depth comparison may show differences in the shape of the posteriors (i.e., wider or narrower, more or less skewed), but this would require more thorough testing of the model settings (i.e., the CV and efn values) and the MCMC specifications (i.e., sample size, burn-in, thinning), which falls outside the scope of this example.

Median posterior benchmark estimates for Lakelse differ more between model forms and estimate types than between data versions with or without infilling (Table \@ref(tab:StateSpaceTab2)). State-space estimates are lower than the regular Bayesian estimates for all model forms and data versions.



### Conclusions

For the two stocks tested in this example, the effect of infilling depends more on stock-specific details (e.g., what the scatter of SR data points looks like, and where the infill values fall) and model fitting approach than the specific details of the infilling step itself. This result supports our current infilling approach for this round of work, and sets the stage for future work to more fully explore the strengths and limitations of applying generic state-space models across all 20 modelled Skeena and Nass Sockeye stocks. 



\clearpage

(ref:StateSpaceComp) State-space posterior estimates of spawner abundance for missing brood years. Each panel shows six alternative estimates of spawner abundance for a stock and brood year, comparing three SR model forms (Basic, AR1, TVP) and two alternative data sets (*Infill* = assign infilled values the same uncertainty and weight as observed values, *No Infill* = assign larger uncertainty and lower weight to infilled values). Top panels show two of four missing brood years for Kwinageese, bottom panels show both missing brood years for Lakelse.

```{r StateSpaceComp,  fig.cap="(ref:StateSpaceComp)" }
include_graphics("data/StateSpaceTest/StateSpace_EstimateComparison.png")
```



\clearpage
(ref:StateSpaceTab1) Kwinageese: Posterior median estimates of biological benchmarks for alternative model forms, estimate types, and data versions.

```{r StateSpaceTab1, echo = FALSE, results = "asis"}


table.in <- read.csv("data/StateSpaceTest/HamazakiAppOutputs_ReportTableSource.csv",stringsAsFactors = FALSE) %>% 
	dplyr::filter(Stock == "Kwinag") %>% select(-Stock)
							

table.in$ModelForm[duplicated(table.in$ModelForm)] <- ""

col.names.use = c("Model Form","Est Type","Data Version","Smsy","Smax","Seq")

table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
	 mutate_all(function(x){gsub("_", "\\\\_", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l",rep("r",3)),
                  caption = "(ref:StateSpaceTab1)", col.names = col.names.use ) %>%
     kableExtra::row_spec(c(4,7), hline_after = TRUE) %>%
    kableExtra::row_spec(c(2,5,8), extra_latex_after = "\\cmidrule(l){2-6}") 

```



(ref:StateSpaceTab2) Lakelse: Posterior median estimates of biological benchmarks for alternative model forms, estimate types, and data versions.  

```{r StateSpaceTab2, echo = FALSE, results = "asis"}


table.in <- read.csv("data/StateSpaceTest/HamazakiAppOutputs_ReportTableSource.csv",stringsAsFactors = FALSE) %>% 
	dplyr::filter(Stock == "Lakelse") %>% select(-Stock)
							

table.in$ModelForm[duplicated(table.in$ModelForm)] <- ""

col.names.use = c("Model Form","Est Type","Data Version","Smsy","Smax","Seq")

table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
	 mutate_all(function(x){gsub("_", "\\\\_", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","l",rep("r",3)),
                  caption = "(ref:StateSpaceTab2)", col.names = col.names.use ) %>%
     kableExtra::row_spec(c(4,7), hline_after = TRUE) %>%
    kableExtra::row_spec(c(2,5,8), extra_latex_after = "\\cmidrule(l){2-6}") 

```






<!--chapter:end:06_06-appendix_AltSRTest.Rmd-->

\clearpage
## Test of Alternative Benchmark Calculation Approaches {#BMCalcTest}


### Purpose



This appendix summarizes results for the following tests: (1) alternative benchmark calculation approaches for a single set of $ln.a$ and $b$ parameters (e.g. Hilborn 1985 vs. Scheuerell 2016 Smsy calculations), (2) alternative benchmark calculation approaches across a grid of *ln.a* and *b* parameter values, (3) speed test for the alternative implementations.

Appendix \@ref(BiasCorrtest) summarizes tests related to the bias correction on $ln.a$.


### Alternative Smsy Calculations

We implemented four alternative Smsy calculation approaches (Table \@ref(tab:SmsyCalcs)) as part of the *RapidRicker* package [@RapidRicker], including the approximations from @Hilborn1985Proxies and @PetermanPyperGrout2000ParEst, the explicit solution from @Scheuerell2016, and a brute force calculation (i.e., for each parameter set $[ln.a,b]$ calculate recruits for 3,000 increments of spawner abundance, then select the increment with the largest difference between recruits and spawners)  . The R code for all four versions is included in Appendix \@ref(BMFunsSmsy).


(ref:SmsyCalcs) *Alternative Smsy Calculation Approaches*.

```{r SmsyCalcs, echo = FALSE, results = "asis"}


table.in <- smsy.eq


table.in$Calculation = linebreak(table.in$Calculation)
   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l"),
                  caption = "(ref:SmsyCalcs)") #%>%
     #kableExtra::column_spec(3, width = "10em") 


```

### Alternative Sgen Calculations


We implemented four alternative Sgen calculation approaches as part of the *RapidRicker* package [@RapidRicker], including three versions of  optimization code [@HoltOgden2013; @samSim; @Connorsetal2022 ] and a brute force calculation (i.e., for each parameter set $[ln.a,b]$ calculate recruits for 3,000 increments of spawner abundance, then select the increment with the smallest spawner abundance for which $Rec \geq Smsy$). The R code for all four versions is included in Appendix \@ref(BMFunsSgen).

Note that the @samSim version has been incorporated in the the [samSim package](https://github.com/Pacific-salmon-assess/samSim) and we label that option *samSim* in the *RapidRicker* functions. 



\clearpage
### Tests


* *Test 1: Sample Parameter Set*: Applied the alternative calculation approaches to a sample parameter set with $ln.a = 1.3$ and $b = 5e-4$. Sgen calculations are relative to Smsy values, so this gives 16 total variations: 4 alternative Smsy calculations, and then 4 alternative Sgen calculations for each Smsy value.
* *Test 2: Grid of ln.a and b parameters*: Applied the alternative calculation approaches to a set with all possible combinations of 100 $ln.a$ values from $ln(1.1)$ to $ln(10)$ and 100 $b$ values from 100 to 1 Mill capacity ($b = 1/Smax$, $b$ values from $1/100$ to $1/10^6$), resulting in 16 estimates for each of 10,000 alternative sets of $[ln.a, b]$.  
* *Test 3: Computing Speed*: Applied the 4 Smsy calculation methods and 4 Sgen calculation methods to 10,000 parameter sets. 


### Results


All the alternative calculation methods (4 for Smsy, 4 for Sgen) generated benchmarks values that are essentially identical for a sample parameter set (Table \@ref(tab:BMCalcTest1)).

For 10,000 alternative combinations of ln.a and b, Smsy values varied by a maximum of `r paste0(test2.vals[1],"%")` across 4 alternative calculation methods. Sgen values varied by a maximum of `r paste0(test2.vals[2],"%")` across 16 alternative calculation methods (4 alternative Smsy calculations by 4 alternative Sgen calculations).

Computing speed differed between calculation implementations, with brute force calculations much slower than the approximate Smsy calculations (Hilborn 1985, Peterman et al. 2000), the exact solution for Smsy (Scheuerell 2016), and the three alternative Sgen solver implementations (Table \@ref(tab:BMCalcTest3)). 


### Conclusions

Based on these results, we decided to use in this report:

* the @Scheuerell2016 method for Smsy, because it is the only exact solution
* the @Connorsetal2022 version of the Sgen optimizer, because it is the only non-brute-force method that did not crash for any of the $[ln.a, b, sd]$ combinations in the bias correction tests (Appendix \@ref(BiasCorrtest)).



\clearpage


(ref:BMCalcTest1) Benchmark Calculation Test 1. Estimates of biological benchmarks for $ln.a = 1.3$ and $b = 5e-4$ using 4 alternative Smsy calculations and 4 alternative Sgen calculations. Note that the @HoltOgden2013 version of the Sgen optimizer has a built-in Smsy calculation using the @Hilborn1985Proxies approximation, and therefore generates the same result for the four alternative Smsy inputs.

```{r BMCalcTest1, echo = FALSE, results = "asis"}


table.in <- test1.table #%>% select(-ln.a,-b)

   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","r","r","r","l","r","r"),
                  caption = "(ref:BMCalcTest1)") %>%
     kableExtra::row_spec(c(4,8,12), hline_after = TRUE) #%>%
     #kableExtra::column_spec(3, width = "10em") %>%
     #  kableExtra::row_spec(c(1:5,7:14,16:22), extra_latex_after = "\\cmidrule(l){2-8}") 

```



(ref:BMCalcTest3) Benchmark Calculation Test 3. Computing time for alternative benchmark calculation approaches over 10,000 sample values.

```{r BMCalcTest3, echo = FALSE, results = "asis"}


table.in <- test3.table %>%
	mutate(Time_s = round(Time_s,2)) %>% 
	dplyr::rename(Benchmark = BM, "Time(s)" = Time_s) %>%
	select(-n)

   
table.in %>% 
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
#   mutate_all(function(x){gsub("\\\\#","\#", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10, align = c("l","l","r"),
                  caption = "(ref:BMCalcTest3)") #%>%
     #kableExtra::row_spec(c(6,15,23), hline_after = TRUE) %>%
     #kableExtra::column_spec(3, width = "10em") %>%
     #  kableExtra::row_spec(c(1:5,7:14,16:22), extra_latex_after = "\\cmidrule(l){2-8}") 

```








<!--chapter:end:06_06-appendix_BMCalcTests.Rmd-->

\clearpage
## EFFECT OF BIAS CORRECTION ON BENCHMARK ESTIMATES {#BiasCorrtest}

### Purpose

Preliminary benchmark estimates were flagged as potentially erroneous during the TWG process. Sgen values in particular seemed too low for several of the stocks. Once we verified the benchmark calculation code (Appendix \@ref(BMCalcTest)), we explored the effect of the log-normal bias correction for the productivity parameter alpha (Table \@ref(tab:BiasCorrCalcs)) on estimates of Smsy and Sgen. Note that this section refers to both the alpha parameter and the its natural log, ln.alpha, depending on the context.


### Approach


* Generated combinations of [alpha,sigma] that spanned the range of preliminary estimates for Skeena and Nass Sockeye stocks:
   * alpha parameters from 1.4 to 20 (ln.alpha from 0.336 to 3)
   * sigma parameters from 0.2 to 1.6
   * beta parameter does not affect the relative values, only the absolute scale, so fixed at  0.0005.
* Used @Scheuerell2016 method for Smsy, because it is the only exact solution. 
* Used @Connorsetal2022 method for Sgen, because it was the only non-brute-force method that did not crash for any of the [ln.alpha, b, s] combinations tested.
* Calculated Smsy and Sgen using either ln.alpha or ln.alpha' = ln.alpha + (sigma^2)/2
* Calculated the % differences due to bias correction for Smsy, Sgen, and the Ratio of Smsy/Sgen 
* Repeated the calculation with the simple deterministic parameter estimates (ln.alpha, beta, sigma) for those Skeena and Nass Sockeye stocks included in our analyses (i.e., wild stocks with at least 5 brood years of spawner-recruit data).



### Results

Larger sigmas resulted in small Smsy increases for stocks with higher intrinsic productivity (alpha >5, ln.alpha > 1.6), but resulted in substantial Smsy increases for lower productivity (alpha < 3, ln.alpha < 1.1). For example, Smsy roughly doubles (Perc diff = 100%) due to the bias correction for alpha = 1.5 and sigma = 1 (ln.alpha = 0.405, ln.alpha' = 0.905). Skeena and Nass Sockeye stocks fall on different gradients, with % difference due to bias correction ranging from ~5% to ~60% (Figure \@ref(fig:BiasCorrSmsyEffect)). Bias correction increased or decreased Sgen values, depending on the combination of ln.alpha and sigma (Figure \@ref(fig:BiasCorrSgenEffect)). Sgen decreases for all but one of the Skeena and Nass Sockeye stocks. For many stocks, Sgen decreased by more than 20%. The bias correction increased the distance between Smsy and Sgen as sigma increased (Figure \@ref(fig:BiasCorrRatioEffect)). For 3 stocks, the ratio of Smsy/Sgen more than doubled due to the bias correction.

Table \@ref(tab:TableBiasCorr) lists results by stock.


### Conclusions

Given these observed effects, we chose to report medians and percentiles without bias correction throughout this Research Document, but included the bias-corrected version in Appendix \@ref(BiasCorrectedBM). Section \@ref(BMMethods) describes how the bias correction is linked to how management objectives are defined.



\clearpage
(ref:BiasCorrSmsyEffect) Effect of Bias Correction on Smsy. Each line shows how, for a specific value of the alpha parameter, the difference between original and bias-corrected estimate changes as the sigma parameter increases. Uncertainty in the model fit increases from left to right, as sigma increases, resulting in a larger difference between estimates (i.e., lines curve upward). The effect of bias correction is larger at lower productivity (i.e., lower alpha parameter). Points show where each stock falls on the gradients of uncertainty and productivity, using a simple deterministic Ricker fit to all available data. The red horizontal line separates the results into the range where bias corrected estimates are larger than the original estimates (top) or lower than original estimates (bottom).

```{r BiasCorrSmsyEffect,  fig.cap="(ref:BiasCorrSmsyEffect)" }
include_graphics("data/BiasCorr/BiasCorr_Smsy_Effect.png")
```


\clearpage
(ref:BiasCorrSgenEffect) Effect of Bias Correction on Sgen. Layout as per Figure \@ref(fig:BiasCorrSmsyEffect).

```{r BiasCorrSgenEffect,  fig.cap="(ref:BiasCorrSgenEffect)" }
include_graphics("data/BiasCorr/BiasCorr_Sgen_Effect.png")
```


\clearpage
(ref:BiasCorrRatioEffect) Effect of Bias Correction on Ratio of Smsy/Sgen. Layout as per Figure \@ref(fig:BiasCorrSmsyEffect).

```{r BiasCorrRatioEffect,  fig.cap="(ref:BiasCorrRatioEffect)" }
include_graphics("data/BiasCorr/BiasCorr_Ratio_Effect.png")
```


\clearpage
(ref:TableBiasCorr) Effect of bias correction on estimates of Smsy, Sgen, and the ratio of Smsy/Sgen. Stocks sorted from lowest to highest productivity (ln.alpha). All results for a simple deterministic Ricker fit to all available data.

```{r TableBiasCorr, echo = FALSE, results = "asis"}
table.in <- read.csv("data/BiasCorr/BiasCorr_RickerDetFits.csv",stringsAsFactors = FALSE) %>%
          select(Stock,		ln.alpha,	sigma, Smsy.PercDiff,	Sgen.PercDiff,	Ratio.PercDiff) %>%
          mutate(ln.alpha = round(ln.alpha,2),	sigma = round(sigma,2), Smsy.PercDiff =round(Smsy.PercDiff),	Sgen.PercDiff=round(Sgen.PercDiff),	Ratio.PercDiff=round(Ratio.PercDiff)) %>% arrange(ln.alpha)

names(table.in) <- gsub(".PercDiff","",names(table.in) )

table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",5)),
                  caption = "(ref:TableBiasCorr)" ) %>%
   # kableExtra::row_spec(c(4,5,9,10), hline_after = TRUE)# %>%
	add_header_above(c(" " = 3, "% Difference" = 3)) 

```



<!--chapter:end:06_06-appendix_BiasCorrTests.Rmd-->

# SIMULATION MODEL EXTENSIONS  {#ModelExt}

Outcome uncertainty and covariation in productivity were identified as key revisions during the peer review meeting in April 2022, and then developed with feedback from the independent reviewers for the overall escapement goal review process (Sec. \@ref(EGProcess)). The specific implementations summarized in the appendices have not been formally peer-reviewed  through the CSAS process, but helped show the potential magnitude of effects on simulation results for the worked examples.  
 
## OUTCOME UNCERTAINTY {#OutcomeUncApp}

### Introduction


Within a simulation model, all the variables can be known exactly, and harvest strategies can be implemented perfectly. In practice, however, perfect control of the outcome is not possible. Target harvest and ER for the aggregate can differ from what the target should be, if run size were known perfectly. Actual harvest and ER will also differ from target ER due to factors such as physical and biological variables that affect the vulnerability of fish to fishing gear (e.g., river conditions, depth of fish in the water column, migration routes, migration timing), enroute mortality, and non-compliance with fishing regulations. Finally, ER for component stocks differ from the aggregate ER, depending on timing and area of fisheries relative to migration routes and timing.

Outcome uncertainty was not included in the simulation model described in initial version of this Research Document which was presented for peer review in April 2022, but was subsequently approximated in the current model implementation based on historical ER patterns. More complex mechanisms could be implemented in the future, which would bring this model closer to a full management strategy evaluation (MSE).

### Historical Patterns in Aggregate Exploitation Rate and Harvest

To investigate historical patterns, we generated time series of aggregate run size (catch plus number of spawners), spawner abundance, harvest, and ER by summing the estimates from the run reconstructions for component stocks.

The total amount harvested and the percent of the run that were harvested (ER) have declined for both aggregates since the mid-1990s (Figure \@ref(fig:OutUncPlotAggErHarv)). The time series can be split into three distinct periods:

* *Pre-1995*: For both aggregates, harvest amounts were highly variable, but ER was fairly stable in the range of 50%-70%.
* *1995-2009*: For the Skeena Wild aggregate, the mid-1990s are a clear breakpoint, with lower harvests and ER following (1) a large-scale fleet reduction in 1996 (the Mifflin Plan), (2) the introduction of gear restrictions to reduce interceptions of non-target species such as Coho and Steelhead (i.e., limiting fishing activity to daylight hours, mandatory weed lines and shorter length and set times for gillnets), and (3) the implementation of the 1999 PST Chapter 2 Annex, which introduced the Week 30/31 provisions for the District 104 purse seine fishery to reduce U.S. interceptions of Skeena Sockeye in July. For the Nass aggregate, ER stayed similar to the earlier time period until 2007, and harvest amounts were at or above the 1980s harvests.
* *2010+*:  For both aggregates, ER and harvest amount were much reduced compared to earlier years. Reasons for this change include (1) the implementation around 2009/2010 of the current Canadian domestic harvest rule for Skeena Sockeye following the work of the Independent Science Review Panel [@Waltersetal2008ISRP], (2) shifting of Canadian fishing effort later to avoid early-timed stocks, and (3) low returns for many years during the recent time period. 

By comparing annual ER and harvest to run size, we can approximate the overall harvest approach across all fisheries (Figure \@ref(fig:OutUncPlotAggFit), Figure \@ref(tab:OutUncTabAggFit)). This is the overall outcome at the end of the fishing season, which reflects environmental conditions, all the fishery-specific pre-season planning, in-season decision-making based on uncertain and rapidly changing information, and actual behaviour of fish and harvesters. Clear abundance-based patterns emerge for both aggregates:

* *SkeenaWild*: Aggregate ER tended to be lower for years with run size near or below the current assumed interim escapement goal of 500,000 (Figure \@ref(fig:OutUncPlotAggFit), Panel A). However, even at run sizes below the assumed current interim goal, the aggregate ER was highly variable, and as high as 60% for some early years with low run size. The aggregate harvest amount has declined with run size, and annual harvests cluster tightly around a fitted regression line for each time period (Figure \@ref(fig:OutUncPlotAggFit), Panel B). The slope for the most recent time period is shallower (i.e., amount of additional harvest for each incremental increase in run size is less in recent years than it was in earlier years).
* *Nass*: Aggregate ER tended to be lower for years with run size near the current assumed interim escapement goal of 200,000 (Figure \@ref(fig:OutUncPlotAggFit), Panel C), and aggregate run size in the reconstructions from 1982-2009 has never fallen below the interim goal. The aggregate harvest amount has declined with run size, and annual harvests cluster tightly around a fitted regression line for each time period (Figure \@ref(fig:OutUncPlotAggFit), Panel D). The slope for the recent time periods is shallower than in earlier years.

### Estimating Aggregate-level Outcome Uncertainty from Historical Patterns

Using the approach by @Collieetal2012RiskFW we can use the fitted regression lines in Panels B and D of Figure \@ref(fig:OutUncPlotAggFit) to estimate two properties of the historical harvest outcomes (Table \@ref(tab:OutUncTabAggFit)):

1.	*No Fishing Point*: Extrapolate the harvest amounts to the lower run sizes lower than any observed and identify the implied run size below which there would have been no harvest (i.e., point of no fishing or the lower management reference point, which is the x intercept of the fitted line). Note that these empirically derived estimates of the implied no-fishing point reflect the net outcome of all sources of variation in catch for a given run size, and hence are not the same as the limit reference points that managers may have had in mind at the time. 
2.	*Outcome Uncertainty*: Use the scatter of points around the fitted line to estimate the overall outcome uncertainty (i.e., assuming that the fitted line represents the actual strategy, how far off was the outcome in each year?). Statistically, this is estimated as the coefficient of variation (CV) based on the Root Mean Square Error (RMSE) scaled by Mean Harvest. A lower CV means that actual outcomes are closer to the estimated strategy (i.e., lower outcome uncertainty).

*Skeena Wild Aggregate*

Mean run size and harvest have declined over time, from a run size of over 1 million  and 650,000 harvested in the years before 1995, to 470,000 run size and 150,000 harvested for 2010-2019 (Figure \@ref(tab:OutUncTabAggFit)). The implied no fishing point is basically the same for all three time periods, at a run size of about 150,000. Outcome uncertainty was lower in earlier years (CV = 11%), then almost doubled in recent years (CV= 18%), but still much lower than the 30-50% CV for four Alaskan Chum stocks analyzed by @Collieetal2012RiskFW.

Based on the implied no-fishing point of 150,000 fish for Skeena Wild, we can infer a lower reference point that was used for the total Skeena aggregate in the past. Total Skeena Sockeye escapement (wild plus enhanced) has averaged about 3 times larger than the wild-stock escapement alone, ranging from 2 to 5 times larger. This roughly translates into an average historical lower reference point (i.e., no fishing point) of about 450,000 total Skeena run size, with a range from 300,000 to 750,000 total Skeena run size.

*Nass Aggregate*

Mean run size and harvest have declined in recent years, from more than 600,000 run size and more than 400,000 harvest in the two earlier time periods, to 350,000 run size and  170,000 harvest for 2010-2019 (Figure \@ref(tab:OutUncTabAggFit)). The implied no fishing point has roughly doubled over time, from about 59,000 before 1995 to about 116,000 since 2010. Outcome uncertainty was similar to Skeena Wild in earlier years (CV = 11%), then dropped (CV= 7-8%), again much lower than the 30-50% CV for four Alaskan Chum stocks analyzed by @Collieetal2012RiskFW.


*Magnitude of observed aggregate-level outcome uncertainty*

The outcome uncertainty described by the CVs for the linear fits in parts B and D of Figure \@ref(fig:OutUncPlotAggFit) appears small, but when translated into variation in ER across years for a given run size (Figure \@ref(fig:OutUncPlotAggFit), panels A and C), the result is a very large range in % ER. For instance, for the Skeena, a run size of roughly 0.5 million resulted in anywhere from a 25% to 50% ER in the 1995-2009 period and over 60% pre-1995. For low-productivity stocks, the high end of this ER range is potentially detrimental.


\clearpage
(ref:OutUncPlotAggErHarv) Time series of aggregate exploitation rate (ER) and harvest for two aggregates. Aggregate spawners, harvest, and run size were calculated as the sum of stock-specific run reconstructions. The time series are split into three time periods that roughly line up with major changes in the management approach. 

```{r OutUncPlotAggErHarv,  out.width= 440,  fig.cap="(ref:OutUncPlotAggErHarv)"}
include_graphics("data/OutcomeUncertainty/ERandHarvest_4Panels.png")
```







\clearpage
(ref:OutUncPlotAggFit) Annual aggregate exploitation rate and harvest as a function of run size. These plots summarize the overall outcome of annual stock-specific and fishery-specific management actions and physical/biological conditions, and can be used to approximate the underlying harvest strategy that was in place. In Panels A and C, various shapes of harvest control rule could be fitted to the observed data (e.g., a curvilinear function like Eqtn. 1 in @HoltPetermanOutcomeUnc, a hockey stick, or a step function with incremental increases in ER), but this would require either specifying or estimating various shape parameters for the functions (e.g., slopes, inflection points, breakpoints). In panels B and D, however, strong linear relationships between total harvest and total run size emerge (coefficient of determination $r^2$, adjusted for the number of observations and number of parameters is larger than 0.9 for all time periods for both aggregates; Table \@ref(tab:OutUncTabAggFit)). 

```{r OutUncPlotAggFit,  out.width= 440, fig.cap="(ref:OutUncPlotAggFit)"}
include_graphics("data/OutcomeUncertainty/OutcomeUncertainty_4Panels.png")
```





\clearpage
(ref:OutUncTabAggFit) Summary of estimated historical harvest strategy. For each aggregate and time period, table shows the mean run, mean harvest, estimated no fishing  point (i.e., x-intercept for linear regression fit in Figure \@ref(fig:OutUncPlotAggFit) ), estimated exploitation rate (i.e., slope of the fitted line), and associated adjusted $r^2$ and CV.


```{r OutUncTabAggFit, echo = FALSE, results = "asis"}


table.in <- read_csv("data/OutcomeUncertainty/Harvest_CV_Calcs_Report.csv") 

table.in$Aggregate[duplicated(table.in$Aggregate)] <- ""

table.in$Mean.Run <- prettyNum(round(table.in$Mean.Run), big.mark=",")
table.in$Mean.Harvest <- prettyNum(round(table.in$Mean.Harvest), big.mark=",")	
table.in$NoFishingPoint <- prettyNum(round(table.in$NoFishingPoint), big.mark=",")

table.in$TimeWindow <- recode(table.in$TimeWindow, "pre95" = "Up to 1994",
															"from95to2009" = "1995 to 2009",
															"since2010" = "Since 2010",
															"allyears" = "All years")


table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","r","r","r","r","r","r"),
                  caption = "(ref:OutUncTabAggFit)" ,
   					 col.names = c('Agg', 'Time\nWindow', 'Run', 'Harvest', 'No\nFishing','ER','Adj $r^2$','CV')
   					 )  %>%
	kableExtra::row_spec(2:dim(table.in)[1]-1, hline_after = TRUE) %>%
	kableExtra::column_spec(8, bold = TRUE,background = "lightgray") 


```


\clearpage
### Historical Differences between Aggregate-level and Stock-level ER

To investigate observed differences in annual ER between stocks, we generated various diagnostic plots of stock-level ER and aggregate-level ER, where aggregate-level ER was calculated from the sum of stock-level run reconstructions. Specifically, we examined the ratio of stock and aggregate-level ERs and their differences over time and relative to aggregate ER. Ratios of stock-level ER and aggregate ER have changed substantially for many stocks since the mid-1990s, consistent with the observed changes in aggregate ER and harvest highlighted above.

Figure \@ref(fig:StkERDiffMorice) shows one example for Morice Sockeye in the SkeenaWild aggregate. Figure \@ref(fig:StkERDiffComp)  summarizes the mean and spread of ratios across stocks, for two different time periods. Tables \@ref(tab:OutUncTabStkScalars95) and \@ref(tab:OutUncTabStkScalarsAll) list the corresponding values. Some notable observations:

* Nass stocks tend to return earlier than the bulk of the Skeena run.
* Nass stocks tend to have very similar ER, with a mean ratio near 1 and a narrower spread than observed for the Skeena stocks.
* The latest-timed Skeena stocks (i.e., Babine LW) generally have higher ER (due to Week 31 provision and later-timed Canadian fisheries), while the earlier-timed stocks generally have lower ER. This difference is more pronounced when looking at more recent data only (starting 1995) than for all years of data.
* *Lakelse* and *Mcdonell*: These are the earliest SkeenaWild stocks, and they have the lowest ER.
* *Babine* stocks: Mean ER is similar for the three component wild stocks, but the link between run timing and estimated ER is still clear. Babine Early Wild has the lowest mean ER, which almost matches the aggregate ER. Babine Mid Wild migrate later and have a slightly higher mean ER than the aggregate.  Babine Late Wild have the latest migration among the wild Skeena stocks, and have the highest mean ER (except for Sustut, see below).
* *Sustut*: Estimated exploitation rates for Sustut are a clear outlier among the SkeenaWild stocks. While escapement data for the Sustut stock comes from a weir count and is considered to be reliable, there is a terminal FSC fishery just downstream of the weir facility with an average reported harvest of 682 (min = 135, max = 1,954) since 1994, when the current fishery started (road access to the site was only established in the early 1990s). This terminal harvest, which is additional to the harvests in mixed stock fisheries in the mainstem Skeena and marine fisheries that affect all other Skeena stocks, may explain the higher and more variable ERs observed for this stock.


\clearpage
(ref:StkERDiffMorice) Example of ER diagnostics – Morice Sockeye (Middle Skeena Lake Type). Plot shows ratios and differences, both over time and relative to aggregate ER. For many stocks, these patterns show a break point in the mid-1990s, so data are split into earlier years up through 1994, and more recent years starting in 1995. Before 1995, Morice ER and aggregate SkeenaWild ER are very similar (ratio around 1, differences around 0), but have increasingly diverged in recent years. ER values in the panels on the right are in %. For example, if aggregate ER was 45% and Morice ER was 32%, then the ratio was 0.71 and the difference was -13.

```{r StkERDiffMorice,  out.width= 415, fig.cap="(ref:StkERDiffMorice)"}
include_graphics("data/OutcomeUncertainty/ER_DiagnosticPlots_Morice_3_AdjustmentDiagnostics.png")
```



\clearpage
(ref:StkERDiffComp) Stock-specific scalars for exploitation rate (ER) estimated for two alternative time periods. Estimates are based on the observed ratio of stock-specific ER and aggregate ER. Points and whiskers show the mean ± 2 SD. Stocks are grouped by aggregate, and sorted based on spawning location within each aggregate, from the mouth of the river to upstream locations. Stocks are also assigned to one of five timing groups, from 1 = earliest to 5 = latest. Peak timing and run duration of stocks relative to each other vary by year and differ by area (e.g., Alaskan fisheries, Canadian marine fisheries, in-river fisheries). Timing assignments are rough groupings based on long-term average peak migration through lower river assessment projects (Tyee test fishery for the Skeena, and Nass fish wheels). Tables \@ref(tab:OutUncTabStkScalars95) and \@ref(tab:OutUncTabStkScalarsAll) list the corresponding values.

```{r StkERDiffComp,  out.width= 400, fig.cap="(ref:StkERDiffComp)"}
include_graphics("data/OutcomeUncertainty/ER_Scalars_byStock.png")
```




\clearpage
(ref:OutUncTabStkScalars95) Distribution parameters for stock-level ER scalars based on observed differences to aggregate ER using data since 1995. *n* is the number of years with stock-specific ER estimates from the run reconstruction. Mean values larger than 1.1 or smaller than 0.9 are highlighted and marked with an asterisk (i.e., stocks where the mean ER differs by more than 10% from the aggregate ER). Note that these scalars are relative to the aggregate ER, so that a scalar of 1.1 (a 10% difference) means an Agg ER of 30% becomes a stock-level ER of 33%, not a stock-level ER of 40%. Stocks are grouped by life history and adaptive zone (LHAZ).

```{r OutUncTabStkScalars95, echo = FALSE, results = "asis"}


table.in <- read_csv("data/OutcomeUncertainty/Generated_ER_Scalars_ByStock.csv") %>%
							dplyr::filter(Version == "Starting 1995") %>%
							select(MU, LHAZ,StkNmS,n,mean, sd,p10,p25,p50,p75,p90) %>%
							mutate(across(c(mean, sd,p10,p25,p50,p75,p90),~format(round(.x, 2), nsmall = 2)))
							

table.in$MU[duplicated(table.in$MU)] <- ""
table.in$LHAZ[duplicated(table.in$LHAZ)] <- ""

cols.mean <- rep("white",dim(table.in)[1])
cols.mean[table.in$mean < 0.9] <- "cyan"
cols.mean[table.in$mean > 1.1] <- "orange"


table.in$mean[table.in$mean < 0.9] <- paste0("*",table.in$mean[table.in$mean < 0.9])
table.in$mean[table.in$mean > 1.1] <- paste0("*",table.in$mean[table.in$mean > 1.1])



table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","r","r","r","r","r","r"),
                  caption = "(ref:OutUncTabStkScalars95)" ,
   					 col.names = c('Agg', 'LHAZ','Stock', 'n', 'mean', 'sd','p10','p25','p50','p75','p90')
   					 )  %>%
  kableExtra::row_spec(c(4), hline_after = TRUE) %>%
  kableExtra::row_spec(c(1,9,15), extra_latex_after = "\\cmidrule(l){2-11}") %>%
  kableExtra::row_spec(c(2:3,5:8,10:14, 16:19), extra_latex_after = "\\cmidrule(l){3-11}") %>%
  kableExtra::column_spec(5, background =  cols.mean)




```


\clearpage
(ref:OutUncTabStkScalarsAll) Distribution parameters for stock-level ER scalars based on observed differences to aggregate ER using all available data. *n* is the number of years with stock-specific ER estimates from the run reconstruction. Mean values larger than 1.1 or smaller than 0.9 are highlighted (i.e., stocks where the mean ER differs by more than 10% from the aggregate ER). Note that these scalars are relative to the aggregate ER, so that a scalar of 1.1 (a 10% difference) means an Agg ER of 30% becomes a stock-level ER of 33%, not a stock-level ER of 40%.

```{r OutUncTabStkScalarsAll, echo = FALSE, results = "asis"}


table.in <- read_csv("data/OutcomeUncertainty/Generated_ER_Scalars_ByStock.csv") %>%
							dplyr::filter(Version == "All Years") %>%
							select(MU, LHAZ, StkNmS,n,mean, sd,p10,p25,p50,p75,p90) %>%
							mutate(across(c(mean, sd,p10,p25,p50,p75,p90),~format(round(.x, 2), nsmall = 2)))
							

table.in$MU[duplicated(table.in$MU)] <- ""
table.in$LHAZ[duplicated(table.in$LHAZ)] <- ""

cols.mean <- rep("white",dim(table.in)[1])
cols.mean[table.in$mean < 0.9] <- "cyan"
cols.mean[table.in$mean > 1.1] <- "orange"

table.in  %>%     
   mutate_all(function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_all(function(x){gsub("%", "\\\\%", x)}) %>%
   mutate_all(function(x){gsub("\\\\n","\n", x)}) %>%
   csas_table(format = "latex", escape = FALSE, font_size = 10,align = c("l","l","r","r","r","r","r","r"),
                  caption = "(ref:OutUncTabStkScalarsAll)" ,
   					 col.names = c('Agg', 'LHAZ', 'Stock', 'n', 'mean', 'sd','p10','p25','p50','p75','p90')
   					 )  %>%
  kableExtra::row_spec(c(4), hline_after = TRUE) %>%
  kableExtra::row_spec(c(1,9,15), extra_latex_after = "\\cmidrule(l){2-11}") %>%
  kableExtra::row_spec(c(2:3,5:8,10:14, 16:19), extra_latex_after = "\\cmidrule(l){3-11}") %>%
  kableExtra::column_spec(5, background =  cols.mean)




```


\clearpage

### Model Implementation of Aggregate and Stock-level ER Scalars

Given the observed patterns summarized above, we decided to simulate outcome uncertainty in the current model as 2 multiplicative scalars, rather than additive variation. Specifically, for Stock *i* in aggregate *j*, for year *k* in simulated trajectory *l*:


\begin{equation} 
	Stk.ER_{i,j,k,l} = Target.ER_{j,k,l} * Agg.Scalar_{j,k,l} * Stk.Scalar_{i,j,k,l}
\end{equation} 


For example:

* If the ER Target for the Skeena Wild aggregate is 10% and the randomly sampled aggregate scalar for Skeena Wild is  0.94, then the actual ER for the Skeena Wild aggregate is 9.4%.
* If the randomly sampled scalar for Alastair is 0.51, then the actual ER for Alastair is 4.8% (10 * 0.94 * 0.51). 

This approach for the aggregate scalar is analogous to the approach by @HoltPetermanOutcomeUnc, who estimated aggregate-level multiplicative scalars for each component of an abundance-based harvest rule that had three inputs (maximum ER, Run size below which the ER is 0, and a shape parameter).

The second step of also applying a stock-specific scalar captures two important properties. Simulated outcomes in terms ER will differ between stocks, but they will be correlated with each other, and with the aggregate (i.e., there is random variation around each ER value, but for a simulated year with larger target ER for the aggregate, all the component stocks will also tend to have larger ER).

The parameterization of these distributions of scalars is critical. To be useful, the modeling approach needs to approximately reflect the mean magnitude of the scalar, as well as variation around that mean. Even if the specifics are wrong, but the overall properties are right, the model will give useful guidance. 
 We created the following alternative scenarios for sensitivity testing:
 
*	*Aggregate Scalars*:  Three variations that cover the observed range (Table \@ref(tab:OutUncTabAggFit)). *None* =  no difference between aggregate target ER and aggregate ER outcome; *Narrow* = normal distribution with CV= 5%; *Wide* = normal distribution with CV= 15%. 
* *Stock-level Scalars*: Three variations. None = no difference between aggregate ER and stock-level ER; All year and Starting 1995 = use the sample distributions (Figure \@ref(fig:StkERDiffComp), Tables \@ref(tab:OutUncTabStkScalars95) and \@ref(tab:OutUncTabStkScalarsAll)).

Together this gives 3 x 3 = 9 alternative scenarios of outcome uncertainty to be tested against alternative productivity assumptions, alternative harvest strategies, and alternative assumptions about covariation in productivity.

Three fundamental questions need to be considered:

1.	*How does the model specify the target ER for the aggregate?*  The aggregate target ER in the simulation will depend on the user-specified type and specific values for the harvest rule. The current priority is to test alternative levels of a fixed escapement strategy. We are also testing alternative levels of a fixed ER strategy to show the contrast in expected performance, and provide support for the recommendation to explore various types of abundance-based rules in the future. 
2.	*How can we capture additional properties of the aggregate scalars?* The Skeena data (Figure \@ref(fig:OutUncPlotAggFit), Panel A) show not only variation around some target nonlinear ER function, but also a bias upward in the harvest rate at low run size compared to the optimal nonlinear function that is associated with an interim escapement goal of 300,000. That bias is important to capture in order to fully reflect the conservation consequences of outcome uncertainty. However, it cannot be easily implemented and tested in the current model structure. We consider this extra level of complexity a high priority for future work, but beyond the scope of the current worked example of the simulation model.
3.	*How can we capture additional properties of the stock-specific scalars?* Outcome uncertainty is likely correlated between stocks (e.g., ER for all the early migrating stocks in a simulated year will tend to differ from the aggregate ER in the same direction, because they pass through the same gauntlet of fisheries at the same time). This could be implemented in the current model structure, similar to the covariation in productivity, which is the second major model extension in response to the science review. However, it would take considerable effort to replicate the productivity covariation analyses with the ER differences to generate the parameters for this. We consider this extra level of complexity a high priority for future work, but beyond the scope of the current worked example of the simulation model.




<!--chapter:end:06_10_ModelExt1_OutcomeUncertainty.Rmd-->

\clearpage
## MODELLING COVARIATION IN PRODUCTIVITY {#CovarProdApp}



### Concepts

The current simulation model simulates 20 stocks in two aggregates: Nass aggregate (4 stocks), SkeenaWild aggregate (16 modelled stocks). Simulated recruits are based on spawner numbers for the brood year, the fitted relationship between spawners and productivity (i.e., recruits/spawner), with randomly sampled noise to reflect natural variability and uncertainty. The initial model results presented in the CSAS science review process assumed a stock-specific amount of variability around the underlying spawner-recruit relationship, but the randomly sampled variability for an individual stock was independent of the variability in other stocks (e.g., in a simulated year, Babine Late Wild could have worse-than-expected recruitment and Babine Early wild could have better-than-expected recruitment). However, the spawner recruit data suggest that covariation in recruitment productivity occurs for some nearby stocks (e.g., positive covariation would mean that in a year with a good productivity for one stock, other stocks would also tend to have good productivity).  Covariation in salmon productivity has been documented at different scales, from stocks in an aggregate to coastwide patterns by species [e.g., @CkCov2017].

Depending on the type of harvest strategy, the level of covariation can strongly influence the aggregate and individual trajectories of run size, harvest, and spawner abundance. Participants in the science review therefore identified covariation in productivity as a high-priority extension of the simulation model. 



### Estimating Historical Covariation in Productivity 

To estimate historical covariation in productivity, we estimated the log residuals from the basic Ricker model fit (i.e., the one without a time-varying productivity parameter), and then estimated the correlation between each pair of productivity time series for modelled stocks. We  then averaged the correlations for groups of stocks (Figure \@ref(fig:AltCorrMat)). Stocks were grouped based on life history and freshwater adaptive zone (LHAZ).  There are two LHAZ with modelled stocks on the Nass (Lower Nass Sea & River Type, Upper Nass Lake Type) and three LHAZ with modelled stocks on the Skeena (Lower, Middle, and Upper Skeena Lake Type). 

Notable observations included:

* *longer time period*:  Positive correlations were observed within and between stocks in the Skeena LHAZ, but the correlation is weaker between Middle Skeena stocks and others. Specifically, correlations are larger than 0.4 within all three Skeena LHAZ, and between lower and upper Skeena stocks. Correlations between middle Skeena stocks and the other Skeena stocks are lower, around 0.175. We observed negative correlations between Lower Nass SRT and all other Skeena and Nass stocks (strongest negative for correlation with Upper Skeena).
* *shorter time period*: We observed stronger correlations than for the longer time period within 2 of the 3 Skeena LHAZ stocks, similar correlations between lower and upper Skeena stocks, and no correlation between middle Skeena LHAZ and other Skeena stocks. There is a stronger negative correlation between Nass Lake Type and Skeena Lake Type stocks.
* Very weak correlation was observed within Upper Nass Lake Type for either time period, so set to 0 in both time periods.
* No correlation was calculated within Lower Nass SRT, because it consists of only a single stock.

\clearpage
(ref:AltCorrMat)  Observed correlation in productivity within and between groups of Skeena and Nass Sockeye stocks. Estimates are based on residuals, ln(recruits/spawner), from Ricker fits for the long-term average productivity scenario (i.e., no time-varying productivity parameter). Missing brood years for some stocks were either left as NA or infilled based on mean residual for other stocks with the same life history and in the same adaptive zone (LHAZ). Note that diagonal cells with bold font are the correlations among stocks within the LHAZ, not the correlation of the LHAZ with itself, which would be 1. Estimates only cover 20 modelled wild stocks. Numbers in brackets show the number of stocks in each LHAZ.

```{r AltCorrMat,  out.width= 400, fig.cap="(ref:AltCorrMat)"}
include_graphics("data/CovarProd/AltCorrMat_Plot.PNG")
```



\clearpage
### Model Implementation of Covariation in Productivity


We incorporated covariation in productivity into the simulaton model by generating correlated time series of standardized residuals, which are then scaled up based on each stock’s observed magnitude of variability. 

We created four alternative covariation scenarios for sensitivity testing:

-	*No covariation*: productivity for each stock is independent of the other stocks.
-	*Simplified correlations – 1984 to 2013 Brood Years*: Using the values from Panel A of Figure 1 for each stock in a group.
-	*Simplified correlations – 1999 to 2013 Brood Years*: Using the values from Panel B of Figure 1 for each stock in a group.
-	*Detailed pairwise correlations – 1984 to 2013 Brood Years*: Using the observed correlations between individual stocks (i.e., the numbers that were averaged to generate Figure \@ref(fig:AltCorrMat)). For example, in this version the productivity correlation between Babine Late Wild and Johnston is a bit less than the correlation between Babine Mid Wild and Johnston. In the simplified versions above, these two correlations are the same.


<!--chapter:end:06_11_ModelExt2_CovarInProd.Rmd-->

# BIAS-CORRECTED BENCHMARK ESTIMATES {#BiasCorrectedBM}


## Nass Summary Tables - Bias Corrected

### Nass Smsy

(ref:SmsyLtAvgNassBC) Comparison of bias-corrected aggregate and stock-level Smsy estimates: Nass / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyLtAvgNassBC, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Smsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyLtAvgNassBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SmsyRecentNassBC) Comparison of bias-corrected aggregate and stock-level Smsy estimates: Nass / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyRecentNassBC, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Smsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyRecentNassBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
### Nass Sgen


(ref:SgenLtAvgNassBC) Comparison of bias-corrected aggregate and stock-level Sgen estimates: Nass / Long-term average productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SgenLtAvgNassBC, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Sgen.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenLtAvgNassBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```



(ref:SgenRecentNassBC) Comparison of bias-corrected aggregate and stock-level Sgen estimates: Nass / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SgenRecentNassBC, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Sgen.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenRecentNassBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage

### Nass Umsy

(ref:UmsyLtAvgNassBC) Comparison of bias-corrected aggregate and stock-level Umsy estimates: Nass / Long-term average productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyLtAvgNassBC, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "LTAvg",BM == "Umsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyLtAvgNassBC)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```



(ref:UmsyRecentNassBC) Comparison of bias-corrected aggregate and stock-level Umsy estimates: Nass / Recent productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyRecentNassBC, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="Nass",Scenario == "Now",BM == "Umsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyRecentNassBC)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```





\clearpage
## Skeena Wild Summary Tables - Bias Corrected

### Skeena Wild Smsy

(ref:SmsyLtAvgSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Smsy estimates: Skeena Wild / Long-term average productivity. Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyLtAvgSkeenaWildBC, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Smsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyLtAvgSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
(ref:SmsyRecentSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Smsy estimates: Skeena Wild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SmsyRecentSkeenaWildBC, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Smsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SmsyRecentSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
### Skeena Wild Sgen


(ref:SgenLtAvgSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Sgen estimates: Skeena Wild / Long-term average productivity. Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added. 

```{r SgenLtAvgSkeenaWildBC, echo = FALSE, results = "asis"}

table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Sgen.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenLtAvgSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```


\clearpage
(ref:SgenRecentSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Sgen estimates: Skeena Wild / Recent productivity.  Stocks are sorted based on median estimate. Mean and median estimates were summed across stocks as a comparison to the aggregate fit, but percentiles can not be simply added.  

```{r SgenRecentSkeenaWildBC, echo = FALSE, results = "asis"}


table.in <-  abd.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Sgen.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:SgenRecentSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,3), hline_after = TRUE) 

```




\clearpage

### Skeena Wild Umsy

(ref:UmsyLtAvgSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Umsy estimates: Skeena Wild / Long-term average productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyLtAvgSkeenaWildBC, echo = FALSE, results = "asis"}

umsy.bm.tab.src <- read_csv("data/SummaryTables_UMSY.csv")

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "LTAvg",BM == "Umsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyLtAvgSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```


\clearpage
(ref:UmsyRecentSkeenaWildBC) Comparison of bias-corrected aggregate and stock-level Umsy estimates: Skeena Wild / Recent productivity.  Table also lists the range and median across stock-level estimates.

```{r UmsyRecentSkeenaWildBC, echo = FALSE, results = "asis"}

table.in <-  umsy.bm.tab.src %>% dplyr::filter(Aggregate=="SkeenaWild",Scenario == "Now",BM == "Umsy.c") %>% select(-Aggregate,-Scenario,-BM) %>%
							mutate_if(is.numeric,function(x){prettyNum(round(x), big.mark=",")})
	
table.in[table.in == "NA"] <- "-"

table.in %>%
   mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l",rep("r",6)),
                  caption = "(ref:UmsyRecentSkeenaWildBC)") %>%
    kableExtra::row_spec(c(1,4), hline_after = TRUE) 

```



<!--chapter:end:06_07-appendix_BiasCorrectedBenchmarks.Rmd-->

# REVIEW OF WILD AND BLDP-ENHANCED BABINE SOCKEYE PRODUCTION {#ChannelReview}

## Context

Considerations for developing management reference points for wild and enhanced Skeena Sockeye include potential interactions between the enhanced and wild Babine stocks, which have distinctive run timing and geographic separation between spawning areas. We reviewed production data for wild and enhanced Babine Sockeye to assess general trends in adult returns, escapement quality (size, sex ratio and fecundity), egg production, and fry and smolt outputs. This was not intended to be a comprehensive assessment of Babine Sockeye production, or a detailed analysis of the effects of the BLDP enhancement program on wild Babine and other Skeena Sockeye stocks. Rather, we provide a high-level overview of observed trends in freshwater production based on available information. An integrated review of BLDP production and updated recommendations for loading targets and operational procedures is a major undertaking that will require input and advice from the facility operator (Fisheries and Oceans Canada - Salmonid Enhancement Program) and is outside the scope of the current review of Skeena and Nass Sockeye escapement goals.



## Babine Sockeye Stocks

Babine Lake is the largest natural freshwater lake in British Columbia, encompassing an area of nearly 500 km^2^ which drains a watershed of approximately 10,000 km^2^. Morrison Lake and Tahlo Lake, which drain through Morrison River into Morrison Arm upstream of Babine Lake. The North Arm, upstream of Harrison Narrows on the northwest side of Babine Lake, flows through a short section of the Upper Babine River into Nilkitkwa Lake, then into the Lower Babine River, a 5th order tributary of the middle Skeena. 

Babine Sockeye have been counted at the Babine weir downstream of Nilkitkwa Lake annually since 1949. The Babine weir which is currently operated by Lake Babine Nation, under contract to Fisheries and Oceans Canada, provides daily counts for all salmon species from the middle of July until the end of September and encompasses most of the Sockeye return. The weir operation has been extended to the end of November in some years. The weir program is assumed to provide a complete count for most years, but adjusted in some years for estimated passage during times when the fence was not operational.

Sockeye salmon escapements to Babine Lake have ranged from 71,000 to 2.1 million past the Babine weir. Very low returns were observed after a catastrophic landslide in Babine River in 1951 that restricted fish passage in 1951 and 1952, and until repairs were completed in 1953 [@Godfry1954BabineSlideEffects]. The lowest Sockeye return of just over 71,000 was recorded in 1955 following the 1950 brood year of 141,000 (Figure \@ref(fig:BabineCounts)).  

Wild Babine Sockeye are assigned to three groups based on adult run timing: an early timed group which primarily spawn in tributaries that drain into the main basin of Babine Lake; a mid-timed group which spawn in Morrison Creek, Morrison Lake, and Tahlo Creek, and a late-timed group of Babine Sockeye includes Sockeye that spawn in sections of the Upper Babine River between Babine Lake and Nilkitkwa Lake, and downstream of Nilkitkwa Lake. The progeny of early and mid-timed wild Babine spawners rear in the main basin of Babine Lake with the exception of Sockeye returning to spawn in Morrison River, Morrison Lake and Tahlo Creek, which rear in Morrison Lake [@WoodLifeHist1995]. The late-timed group exhibits an upstream migration pattern for fry which migrate upstream following emergence and rear in Nilkitkwa Lake and the North Arm of Babine Lake. 

Visual escapement estimates of up to 30 wild Babine Sockeye spawning tributaries are conducted annually by foot or aerial surveys led by DFO and Lake Babine Nation. Estimates from visual escapement surveys for wild Babine systems are adjusted to account for underestimation bias using methods described in @WoodLifeHist1995. Annual stream counts for individual Babine systems are maintained in the Fisheries and Oceans Canada [NUSEDS database](https://open.canada.ca/data/en/dataset/c48669a3-045b-400d-b730-48aafe8c5ee6).  The raw spawner estimates for the different wild Babine systems are expanded and combined into adjusted estimates for the early, mid and wild run timing components using a run-reconstruction procedure described by @WoodLifeHist1995.

Fulton River and Pinkut Creek were, along with Babine River, the most abundant Babine Sockeye stocks and largest contributor to Babine Lake Sockeye before the start of the  BLDP. In the post-BLDP period, Sockeye salmon returns to the enhanced systems increased following while returns wild Babine systems have declined. The pattern of declines has varied between stocks over time. Early and late-timed Babine wild stocks have seen steady declines in spawner abundances. The numbers of recruits produced per spawner (recruits per spawner) since the late 1990s, while mid-timed wild stocks appear to have recovered from low returns in the late 1990s but have been in a state of decline since the mid-2000s.  

The asynchrounous population dynamics between wild and enhanced Babine Sockeye, and among the different wild stocks, suggest that straying of enhanced surplus spawners into wild systems is not likely given that there have been large surpluses and low observed spawner escapement to wild Babine tributaries have been observed in some years, and the reverse in others (Figures \@ref(fig:BabineProdRpS) and \@ref(fig:BabineProdResid)).

(ref:BabineCounts) Babine weir counts 1950 – 2021. Figure shows estimated wild (light grey) and enhanced (dark grey) components of the run.

```{r BabineCounts,  fig.cap="(ref:BabineCounts)" }
include_graphics("data/ChannelReview/BabineWeirCounts.png")
```


\clearpage
(ref:BabineProdRpS) Observed productivity of Babine Sockeye stocks. Panels show productivity in terms of recruits/spawner (R/S), log-transformed to adjust for the commonly observed skewed distribution and smoothed as a 4-yr running average to highlight the underlying pattern. Spawners exclude the channel surplus. Red horizontal lines mark the corresponding raw numbers that can be more directly interpreted: At 1 R/S (*Repl*), the stock replaces itself *in the absence of any harvest*. At  2 R/S, the stock could sustain  50% exploitation rate while maintaining the same spawner abundance (under theoretical stable long-term conditions, i.e., *equilibrium*). For each stock, the largest observed productivity, Max(R/S), and the stock's contribution to the total Skeena spawner abundance since 2000 (%Spn) are listed. Figure \@ref(fig:BabineProdResid) shows changes in productivity after accounting for density dependence.

```{r BabineProdRpS,  fig.cap="(ref:BabineProdRpS)" }
include_graphics("data/ChannelReview/ProdPatterns_BabineStocks_LogRpS.png")
```



(ref:BabineProdResid) Productivity residuals for Babine Sockeye stocks. Panels show productivity patterns as deviations from the expected log(R/S) based on a simple deterministic Ricker fit, smoothed as 4-yr running average to highlight the underlying pattern. The Ricker residuals residuals, in units of ln(R/S), account for within-stock density effects, so that the pattern is a better reflection of fundamental, underlying productivity changes as spawner abundance naturally varies from year to year. With these residuals, the pattern can be directly interpreted, but the specific values are not as biologically meaningful as the observed productivity series in Figure \@ref(fig:BabineProdRpS).

```{r BabineProdResid,  fig.cap="(ref:BabineProdResid)" }
include_graphics("data/ChannelReview/ProdPatterns_BabineStocks_RickerResids.png")
```





\clearpage
## Babine Lake Development Project 

### History

The BLDP spawning channels, adult control weirs, and flow control structures were built in stages starting with construction of Fulton Channel 1 in 1965, and the Fulton weir and Pinkut flow-control structures in 1966. Pinkut Channel and weir, and the Fulton flow-control structures were installed in 1968, followed by Fulton Channel #2, which was completed in two phases, in 1969 and  1971. For the first two years of operation, only the top half of Fulton Channel #2, representing 55% of its eventual capacity, was loaded. Pinkut Channel, which was initially built in 1968, experienced high egg mortality in the first two years of operation as a result of anchor ice formation in the channel bed. In 1970, an auxillary water system was installed to supply warm lake water to the channel. In subsequent years, spawning habitat quality in Pinkut Channel was affected by heavy siltation caused by erosion of the unarmoured banks, and the channel was entirely rebuilt in 1976-77 [@West1987]. Starting in 1973, an airlift operation was used to transport spawners to an inaccessible section of the creek above Pinkut Falls in some years of high returns.

The BLDP spawning channels increased available spawning habitat by 116,000 m^2^ to accommodate approximately 190,000 additional spawners, and flow control provides stable spawning and incubation habitat in Pinkut Creek and Fulton River (West 1987, Table 1). Sockeye returning to Pinkut Creek and Fulton River also spawn in natural stream sections downstream of the respective weirs, which have an estimated capacity for 5,000 and 45,000 effective spawners. The current combined spawning capacity for Pinkut and Fulton Sockeye, including spawning channels, flow-controlled river sections, inaccessible spawning habitat serviced by the Pinkut Airlift program, (which has not operated since 2007), and downstream areas, is 509,000 spawners. The area of available spawning habitat, year of implementation, and current loading targets for BLDP enhanced channel, river and creek sections are provided in Table
\@ref(tab:ChannelTargets).


(ref:ChannelTargets) Area, loading capacity and date of construction for BLDP components. The original target density of one female per 1.25 m^2^ was increased  in the early 1990s for components marked by **\***.

```{r ChannelTargets, echo = FALSE, results = "asis"}

table.in <- read_csv("data/ChannelReview/ChannelTargets.csv") %>% mutate(Target = prettyNum(Target,big.mark = ",",scientific = FALSE)) %>% 			mutate_all(as.character)

table.in[is.na(table.in)] <- "-"

col.names.use <- c("Component",
									 "Area\n(1000$m^2$)",
									 "1st year\noperated",
									 "Target\nspawners\n1000")


table.in %>%
   #mutate_at(1,function(x){gsub("&", "\\\\&", x)}) %>% 
   #mutate_at(1,function(x){gsub("%", "\\\\%", x)}) %>%
csas_table(format = "latex", escape = FALSE, font_size = 9, align = c("l","r","r","r"),
                  caption = "(ref:ChannelTargets)" ,col.names = linebreak(col.names.use ,
                  																												align = "c")) %>%
    kableExtra::row_spec(c(6,7,12,13), hline_after = TRUE)# %>%
	#add_header_above(c(" " = 2, "Scenarios" = 4)) 

```


### Channel Loading

Sockeye escapements, or loading, of spawning channels and Pinkut Creek and Fulton Rivers are managed to maintain target densities of spawners to maximize fry production and reduce risks of over-escapement including redd superimposition following wave spawning events, and disease outbreaks. The channels are loaded by female counts, and the target loading density of about 1.25 females/m^2^ of available spawning gravel is designed to achieve an optimal egg density of 2000-2500 eggs/m^2^. Loading targets for Fulton River were adjusted upwards in the 2000s to mitigate for pre-spawn mortality (PSM) caused by parasites and warmer temperatures. The actual number of eggs deposited in a given year depends on a number of factors including fecundity, egg retention, PSM and the ability to reload spawners, if available, in the event of high PSM. Current loading targets are provided in Table \@ref(tab:ChannelTargets).

Loading events for the BLDP  spawning channels ideally occur in a single event for each channel to avoid wave spawning. Sockeye spawners are enumerated as they pass through weirs located near the mouths of Pinkut Creek and Fulton Rivers. Once the spawning channels and river sections upstream of the weir have reached capacity, any Sockeye remaining holding below the fence are locked out. Visual estimates of the number of Sockeye holding below the BLDP facilities are conducted regularly during the spawning season in most years. If significant pre-spawn mortality is detected in the spawning channels, they may be reloaded with the Sockeye that remain holding below the fences.

The loading target for  Fulton River upstream of the  counting fence was increased from 100,000 to 200,000 in the early 2000s to offset potential PSM related to parasite infection, and because it was thought that additional fry could be produced, albeit at a lower egg-to-fry survival rate, from the lower quality, more marginal spawning habitats that are not included in the estimated area of  good quality spawning habitat for Fulton River above the counting fence (67,000 m^2^).

A disease outbreak (the parasite *Ichthyophthirius multifilis*) caused high pre-spawn mortality in the Fulton and Pinkut spawning channels in 1994 and 1995 [@Traxler1998BabineDisease] and resulted in low spawner escapements to the enhanced facilities in subsequent return years starting in 1998.  Although escapements for both systems rebounded somewhat in the early 2000s, it has remained lower than the pre-outbreak period), with further declines observed for Pinkut Sockeye since 2010.

Loading targets for the Pinkut and Fulton spawning channels and managed river sections have been maintained at full capacity  in most years except for years of exceptionally poor returns, including 1998 and 1999 (following the disease outbreak that affected the 1994 and 1995 brood years), and more recently in 2013 and 2019 when spawning targets were not attained for Fulton Channel #2, and for Pinkut Channel in 2013. Fulton River did not attain its loading targets in 1969, 1991 and 2013.  


## Available biological information for wild and enhanced Babine Sockeye



### Age Sampling

Babine Sockeye primarily rear for 1 year in freshwater following emergence as fry, migrate to sea in their second year of life and return to spawn after 1-3 years at sea, for a total ages ranging from 3 (“jack” Sockeye which spend 1 winter at sea) or 4-5 (“adult” or “large” Sockeye, which spend 2-3 winters at sea). Age sampling has not been regularly conducted at the Babine weir or BLDP facilities since the mid-1990s. Age composition estimates for Babine Sockeye are derived from the aggregate Skeena Sockeye return, which is sampled at the Tyee Test Fishery, of which Babine Sockeye typically account for about 90% of the total return. 

Age-3 Sockeye, which are not effectively sampled at the Tyee Test Fishery are counted at the Babine weir. The estimated returns of “large”, or 4- and 5-year old Sockeye arriving at the Babine fence, which are based on the sampled proportions of Sockeye with a single freshwater year at the Tyee Test Fishery (age 4~2~ and 5~2~ in Gilbert-Rich notation), are added to the counts of age-3 Sockeye from the Babine fence to calculate the proportions for all age classes. 

The proportions of age 3-, 4-, and 5-year old Skeena and thus Babine Sockeye varies across years. Since 1970, the annual proportion of age-3 Sockeye returns to Babine Lake has ranged from 0 – 40%. The proportions of age 4 and age 5 age classes have both ranged between 3 – 92%. Exceptionally low returns of one age class can signal a brood year failure related to poor marine survival for the siblings of a cohort that went to sea in a common year. For example, a poor return of age 3 Sockeye may signal a poor return of age 4 the next year, followed by age 5 in the following year. Because the dominant age class (4~2~ or 5~2~) of spawning females varies by year, there is no clear trend in declining total age at return for Babine Sockeye.


### Body Size

Length-at-age and overall body length have decreased over time for Skeena Sockeye, which are sampled at the Tyee Test Fishery. For Sockeye sampled at the Tyee Test Fishery, length at age decreased by 2-3% for 5-, 6- and 7-year old fish and remained constant for 4 year old fish between the 1980s and 2010s (decadal averages). The pattern of observed changes in overall length and length-at-age for Skeena and Nass Sockeye, which are consistent with decreases observed for Sockeye salmon populations in Southeast Alaska [e.g., @Oke2020RecentDeclinesBodySize], are not linear, with less pronounced declines in older age classes and steeper declines observed since 2010.  

The magnitude of the observed declines in body length for Skeena and Nass Sockeye are consistent with those observed for other Sockeye populations in the North Pacific, and are related to decreases in fecundity [@Oke2020RecentDeclinesBodySize; @Ohlbergeretal2020CkEscQual]. 

### Fecundity

Fisheries and Oceans Canada, Salmon Enhancement Program collects and maintains production datasets for Pinkut and Fulton Sockeye. BLDP Production data to 1985 are reported in @West1987, and are currently being updated by Pacific Salmon Foundation. The data presented here are preliminary, and availabilty by year varies by project and channel. 

DFO-Salmon Enhancement Program (DFO-SEP) personnel collect biological data at the Pinkut and Fulton facilities, including sex ratio and estimated percentage of prespawn mortality (PSM). Biosampling is conducted at both spawning channels to assess body size, fecundity, and egg retention for spawners, which are incorporated into estimates of total egg deposition and density for each BLDP component. Potential fecundity, or the average number of eggs carried by spawning females, is measured from sacrificed samples collected across the observed size spectrum of spawning females at the Pinkut and Fulton spawning channels. Average potential fecundity is estimated by regressing egg counts to body length of sampled fish, and applying the regression equation to the average length of female spawners for each channel, river or creek.

Apparent fecundity, or the actual average number of eggs deposited, which accounts for egg retention (estimated from sampling spawned-out carcasses), is combined the number of effective females after accounting for  prespawn mortality to estimate the actual deposition of eggs deposited in each river, creek or section of channel. While potential fecundity is an indicator of the condition of female spawners entering the channels, apparent fecundity is required to estimate total egg deposition and egg to fry survival for a given year.

Estimates of potential fecundity, which are available for the spawning channels, Pinkut Creek and Fulton Rivers from 1998 onward show a decreasing trend, likely related to a trend in decreasing body size during that time period (Figure \@ref(fig:PotentialFecundity)). Estimates of apparent fecundity (potential fecundity minus egg retention), which are available for longer time series, show a decreasing and nonlinear trend since the 1970s which are likely related to overall declines in average body length that have been observed during the same time period (Figures \@ref(fig:PotentialFecundity) and \@ref(fig:ReducedFecundity)). 


(ref:PotentialFecundity) Calculated potential fecundity for Sockeye sampled at Pinkut and Fulton spawning channels, 1960-2020. Estimates cover 1998-2020 for Fulton systems and 2000-2020 for Pinkut systems.

```{r PotentialFecundity,  fig.cap="(ref:PotentialFecundity)" }
include_graphics("data/ChannelReview/PotentialFecundity.png")
```

(ref:ReducedFecundity) Trends in average apparent fecundity for Babine Sockeye sampled at Fulton Channel 2, 1960-2020. 

```{r ReducedFecundity,  fig.cap="(ref:ReducedFecundity)",out.width = "80%" }
include_graphics("data/ChannelReview/ReducedAverageFecundity.png")
```

### Egg Deposition and Fry Production

From 1973 to 1984, hydraulic sampling was used to assess egg survival in the channels, river and creek.  Hydraulic sampling was then discontinued because it does not assess all mortality prior to hatch and was not considered a replacement for the downstream fry enumeration program [@West1987].

Fry production for Pinkut Creek and Fulton River is assessed annually during the spring outmigration period after emergence, using fan or converging throat traps operated during the spring migration period to generate an estimate of the total abundance of fry entering the lake from both projects. In recent years (2015-2019), Fulton Channel 1 has been operated as part of the river and the fry counts have been combined.

Fry production is not assessed directly for wild Babine systems. A biostandard of 233 fry/spawner, derived from average egg to fry survival in the natural sections of Pinkut Creek and Fulton River is applied to estimate fry production for the early, mid and late-timed wild Babine Sockeye groups (MacDonald and Hume 1984), which are added to reported BLDP fry production to estimate total combined fry abundances for Babine Lake [e.g., @Woodetal1998Babine; @CoxRogersSpilsted2012Babine].

Egg deposition and fry production have remained relatively constant over time for Fulton River and Pinkut Creek. The three channels have seen decreasing trends in  egg deposition since the 1970s which may be related to lower fecundity for spawning females. Fry production has decreased somewhat for Fulton Channel two during this time period, but there is no clear pattern of reduced fry production for Fulton Channel #2 or Pinkut Channel. 

Egg to fry survival rates are higher in the spawning channels than in the natural river sections with regulated flow. This is not surprising, because the channels have been designed and managed for ideal spawning conditions, including water depth, flow, and substrate. For the post-BLDP period between 1970-2020, the average egg to fry survival rates for Pinkut Channel, Fulton Channel 1 and Fulton Channel 2 were 49%, 35%, and 48% respectively compared with 25% for Pinkut Creek and 18% for Fulton River. Egg to fry survival has not changed from pre-BLDP conditions since flow control structures were installed in Fulton River. There is no clear relationship between egg density and egg to fry survival for the spawning channels. Although there is evidence of decreasing egg to fry survival rates with increasing egg densities in Pinkut Creek and Fulton River, fry production increases with increasing egg densities in both systems, but the rates of increase are slower at higher densities than 2000-2500 eggs/m^2^, which are considered ideal for maximum fry production without a decrease in egg-to-fry survival (C. West, unpublished data).

(ref:EggDeposition) Total estimated egg deposition (x 10 million) for BLDP enhanced river, stream and channels, 1960-2020. Here, the upper and lower sections of Pinkut Creek (including the Pinkut Airlift) are combined, as are Fulton Channel 1 and Fulton River, which are assessed together in spring fry enumeration programs. 

```{r EggDeposition,  fig.cap="(ref:EggDeposition)" }
include_graphics("data/ChannelReview/TotalEggDepo.png")
```

(ref:FryProd) Estimated egg to fry survival for BLDP enhanced river, stream and channel, 1960-2020. Here, the upper and lower sections of Pinkut Creek (including the Pinkut Airlift) are combined, as are Fulton Channel 1 and Fulton River, which are assessed together in the spring fry enumeration programs.

```{r FryProd,  fig.cap="(ref:FryProd)" }
include_graphics("data/ChannelReview/FryProd.png")
```


(ref:EggToFry) Estimated fry production (x 10 million) for BLDP facilities, 1960-2020. Here, the upper and lower sections of Pinkut Creek (including the Pinkut Airlift) are combined, as are Fulton Channel 1 and Fulton River, which are assessed together in the spring fry enumeration programs.

```{r EggToFry,  fig.cap="(ref:EggToFry)" }

include_graphics("data/ChannelReview/EggToFry.png")
```


### Smolts

Babine Sockeye smolts are assessed annually at the outlet of Nilkitkwa Lake during the spring migration. Annual smolt abundance estimates are produced by mark and recapture estimation using a parsimonious model reported in MacDonald and Smith (1980). Smolts are sampled for length, weight, age, and prevalence of the parasite Eubothrim salvelini.  The smolt migration is bimodal, which allows for separate for a smaller first peak, consisting of smolts leaving Nilkitwka lake and the North Arm of Babine Lake, which are likely the progeny of late-timed Babine River spawners, and a second larger peak consisting of main-basin populations, including smolts originating from BLDP facilities. The smolt program, which did not operate from 2002-2012 due to budgetary restrictions, resumed in 2013 and is currently operated by Lake Babine Nation.

Smolt production from the BLDP has increased linearly with increasing fry production since the start of the BLDP. The average weight of sampled smolts in the pre and post BLDP periods were 5.4 g (SD 0.5 g) prior to 1975 and 4.8 g (SD 0.4 g) since 1976. The significant decrease in mean weight occurred in the pre BLDP period, and it has remained relatively constant since the production of BLDP smolts began.

Although there is a clear positive relationship between fry production and seaward migrating smolts from Babine Lake, the benefits of increased smolt production to adult returns are less clear, with high variability in smolt to adult survival.  From 1960-2000 (the years prior to the closure of the Babine smolt fence), smolt-to-adult survival ranged from 0.71 – 13.8 adult returns per smolt, with the highest survival rate observed for the 1995 brood year, following disease and associated prespawn mortality in 1994 and 1995 and associated prespawn mortality, and low fry production from BLDP facilities. There are no clear trends in smolt to adult survival, which is highly variable, however there is a weak positive relationship between smolt to adult survival (SAS) and smolt weight and a negative relationship between SAS and smolt abundance. 

(ref:SmoltPlots) Exploratory data analysis (EDA) plots of smolt abundance and weight. (A) Annual abundance of smolts in the main Babine basin, with construction start of enhancement facilities marked by the red vertical line; (B) Changes in smolt weight over time, with simple linear regressions fitted to two time periods: 1950-1968, 1969-2013; (C) Relationship between fry abundance and smolt abundance in the main basin; (D) Relationship between smolt weight and abundance in the main basin;  (E) Relationship between smolt survival and smolt weight; (F) Relationship between adult returns and smolt abundance, with simple linear trendline as a visual reference. More recent observations are shaded darker red in all six panels. Panels C-F include a simple linear trend line as a visual reference.

```{r SmoltPlots,  fig.cap="(ref:SmoltPlots)" }

include_graphics("data/ChannelReview/SmoltPlots.png")
```

### Limnology of Babine Lake

Limnological assessments conducted in the 1950s and 1960s found that the Sockeye rearing capacity of Babine Lake was underutilized [i.e., @Brett1951; @Johnston1956] and estimated that Babine Lake could support up to 300 million fry. The initial target for increased fry production for the BLDP of an additional 100 million fry was exceeded, with BLDP facilities estimated to have contributed an average of 125 million fry (range: 37 – 212 million) to Babine Lake since 1971, which together with the estimated fry production from wild spawning populations (1950-2021 average 68.0 million, range 9.4 – 209.2 million) is less than the estimated capacity of 300 million.

A more recent limnological assessment in 2000 used the PR (phtotosynthetic rate) capacity model estimated the rearing capacity of the main basin of Babine Lake  to be 219 million (Hume and Maclellan 2000), which combined with unsampled habitats in North Arm, Morrison Arm and Hagen Arm would likely approach 300 million fry, with additional rearing capacity in Nilkitkwa Lake. Hydroacoustic fall fry estimates carried out in Nilkitkwa Lake in 2013 and 2016 observed 0.99 and 0.67 million fry, respectively (Carr-Harris and Doire 2017).

Updated limnological assessments are needed to identify any large-scale changes that have occurred during the last two decades, during which Babine Lake has experienced higher temperatures and lower Sockeye returns than in the previous decades, which potentially affect nutrient loading. The results from relatively recent limnological surveys that were carried out at Babine Lake in 2013 and 2015, are not available at this time (D. Selbie, pers. comm., DFO Cultus Lake Salmon Research Laboratory, 2021).







<!--chapter:end:06_08-appendix_ChannelReview.Rmd-->

# SR MODEL FITTING RESULTS AND BIOLOGICAL BENCHMARK ESTIMATES FOR ENHANCED PINKUT AND FULTON {#PinkutFultonResults}


```{r , echo = FALSE, results = "asis"}


source("data/PinkutFultonApp/FUNCTIONS_prepTable.R")

sampled.posteriors.df <- read.csv("data/PinkutFultonApp/SampledPosteriors_Summary.csv",stringsAsFactors = FALSE)


pinkut.sampled.pars <- sampled.posteriors.df %>% dplyr::filter(Stock == "Pinkut")
fulton.sampled.pars <- sampled.posteriors.df %>% dplyr::filter(Stock == "Fulton")


```

This Research Document focuses on SR modelling for wild Sockeye stocks (16 Skeena, 4 Nass), but corresponding SR data, parameter estimates and benchmark estimates for Pinkut and Fulton are included here as a reference. Note, however, that these should not be used given SR model fitting issues and management differences discussed in Section \@ref(AltApproachEnhanced).

Observed productivity, in terms of ln(R/S), does not show a clear density-dependent pattern (Figures \@ref(fig:RpSPlotPinkut) and \@ref(fig:RpSPlotFulton)). This is due to a combination of spawning channel development, annual channel management, natural variation in productivity, density-dependence, and uncertain estimates of spawners and recruits, especially linked to estimating the non-spawning surplus (Section \@ref(SurplusEst)). Given this noisy data, Bayesian estimates of Ricker model parameters are highly sensitive to alternative data treatment assumptions (as illustrated for the Skeena aggregate in Figure \@ref(fig:AltFitPlotSkeena)) as well as alternative priors on productivity and capacity (i.e., the y-intercept and slope of a line fitted through the scatterplot in Figures \@ref(fig:RpSPlotPinkut) and \@ref(fig:RpSPlotFulton) is strongly affected by what we set as a plausible starting point). 

Biological benchmarks for Pinkut are substantially lower under the recent productivity scenario (Table \@ref(tab:BMTableLTAvgPinkut)) than  under the long-term average productivity scenario (Table \@ref(tab:BMTableNowPinkut)). For Fulton, the benchmarks are quite similar, with Smax, Smsy, and Seq a bit lower under recent productivity than under long-term average productivity, and Sgen a bit higher (Table \@ref(tab:BMTableLTAvgFulton)  vs. Table \@ref(tab:BMTableNowFulton)).







\clearpage
(ref:RpSPlotPinkut) Ln(R/S) Plot - Pinkut. Scatter plot of log productivity ln(R/S) vs. spawner abundance. Observations are colour-coded, with earlier data in fainter shading. The secondary axis illustrates the corresponding raw R/S values. Variations of the Ricker model attempt to fit a straight line through this scatter of points. The y axis intercept of the fitted line captures intrinsic productivity (i.e., R/S at very low spawner abundance) and the slope reflects the capacity (i.e., a steeper slope means more of a density-dependent reduction for each additional spawner, indicating lower capacity). 

```{r RpSPlotPinkut,   fig.cap="(ref:RpSPlotPinkut)"}
include_graphics("data/PinkutFultonApp/Pinkut_RpS_ScatterPlot.png")
```



\clearpage
(ref:RpSPlotFulton) Ln(R/S) Plot - Fulton. Scatter plot of log productivity ln(R/S) vs. spawner abundance. Observations are colour-coded, with earlier data in fainter shading. The secondary axis illustrates the corresponding raw R/S values. Variations of the Ricker model attempt to fit a straight line through this scatter of points. The y axis intercept of the fitted line captures intrinsic productivity (i.e., R/S at very low spawner abundance) and the slope reflects the capacity (i.e., a steeper slope means more of a density-dependent reduction for each additional spawner, indicating lower capacity). 

```{r RpSPlotFulton,   fig.cap="(ref:RpSPlotFulton)"}
include_graphics("data/PinkutFultonApp/Fulton_RpS_ScatterPlot.png")
```




\clearpage
(ref:BMTableLTAvgPinkut) Posterior distributions for selected SR parameters and resulting biological benchmarks - Pinkut with long-term average productivity scenario. This table shows estimates using parameters sampled from the AR1 model fit with capped uniform capacity prior. Variables with the ".c" suffix are the bias corrected version (e.g., Smsy vs. Smsy.c).

```{r BMTableLTAvgPinkut, echo = FALSE, results = "asis"}

prepTableBM(bm.df = pinkut.sampled.pars %>% dplyr::filter(Scenario == "LTAvg"),
            caption = "(ref:BMTableLTAvgPinkut)")
```

 
  
(ref:BMTableNowPinkut) Posterior distributions for selected SR parameters and resulting biological benchmarks - Pinkut with recent productivity. This table shows estimates using parameters sampled from the most recent generation (i.e., last 4 brood years) of the time-varying productivity (TVP) model fit with capped uniform capacity prior. Variables with the ".c" suffix are the bias corrected version (e.g., Smsy vs. Smsy.c).

```{r BMTableNowPinkut, echo = FALSE, results = "asis"}

prepTableBM(bm.df = pinkut.sampled.pars %>% dplyr::filter(Scenario == "Now"),
            caption = "(ref:BMTableNowPinkut)")
```




\clearpage
(ref:BMTableLTAvgFulton) Posterior distributions for selected SR parameters and resulting biological benchmarks - Fulton with long-term average productivity.  This table shows estimates using the AR1 model fit with capped uniform capacity prior.  Variables with the ".c" suffix are the bias corrected version (e.g., Smsy vs. Smsy.c).

```{r BMTableLTAvgFulton, echo = FALSE, results = "asis"}

prepTableBM(bm.df = fulton.sampled.pars %>% dplyr::filter(Scenario == "LTAvg"),
            caption = "(ref:BMTableLTAvgFulton)")
```

 
  
(ref:BMTableNowFulton) Posterior distributions for selected SR parameters and resulting biological benchmarks - Fulton with recent productivity. This table shows estimates using parameters sampled from the most recent generation (i.e., last 4 brood years) of the time-varying productivity (TVP) model fit with capped uniform capacity prior. Variables with the ".c" suffix are the bias corrected version (e.g., Smsy vs. Smsy.c).

```{r BMTableNowFulton, echo = FALSE, results = "asis"}

prepTableBM(bm.df = fulton.sampled.pars %>% dplyr::filter(Scenario == "Now"),
            caption = "(ref:BMTableNowFulton)")
```



<!--chapter:end:06_09-appendix_PinkutFultonResults.Rmd-->

